{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44353ae1-41a5-4e8e-bdfb-fec1f445f239",
   "metadata": {},
   "source": [
    "# Semantic Search & RAG with LlamaIndex\n",
    "## ABB #6 - Session 3\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22ee10-b6c8-4e59-babd-366b41f4a357",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c3df73-038b-44b9-9bf2-526dc485c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer, Settings\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1596f6cd-47b2-41d9-905e-a5b8862cf998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# import sk from .env file\n",
    "load_dotenv()\n",
    "my_sk = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d779f30-2fb3-4292-a627-fdec718d6767",
   "metadata": {},
   "source": [
    "### 1) chunk articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53d05bc-7ace-455a-a6bd-bdf45116d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all HTML files from raw directory\n",
    "filename_list = [\"articles/\"+f for f in os.listdir('articles')]\n",
    "\n",
    "chunk_list = []\n",
    "for filename in filename_list:\n",
    "    # only process .html files\n",
    "    if filename.lower().endswith(('.html')):\n",
    "        # read html file\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "    \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Get article title\n",
    "        article_title = soup.find('title').get_text().strip() if soup.find('title') else \"Untitled\"\n",
    "        \n",
    "        # Initialize variables\n",
    "        article_content = []\n",
    "        current_section = \"Main\"  # Default section if no headers found\n",
    "        \n",
    "        # Find all headers and text content\n",
    "        content_elements = soup.find_all(['h1', 'h2', 'h3', 'p', 'ul', 'ol'])\n",
    "    \n",
    "        # iterate through elements and extract text with metadata\n",
    "        for element in content_elements:\n",
    "            if element.name in ['h1', 'h2', 'h3']:\n",
    "                current_section = element.get_text().strip()\n",
    "            elif element.name in ['p', 'ul', 'ol']:\n",
    "                text = element.get_text().strip()\n",
    "                # Only add non-empty content that's at least 30 characters long\n",
    "                if text and len(text) >= 30:\n",
    "                    article_content.append({\n",
    "                        'article_title': article_title,\n",
    "                        'section': current_section,\n",
    "                        'text': text\n",
    "                    })\n",
    "    \n",
    "        # add article content to list\n",
    "        chunk_list.extend(article_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca3f75f3-7f71-4c5f-afb5-c9f077cfe523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778\n"
     ]
    }
   ],
   "source": [
    "# create nodes with Llama Index (i.e. nodes)\n",
    "node_list = []\n",
    "for i, chunk in enumerate(chunk_list):\n",
    "    node_list.append(\n",
    "        TextNode(\n",
    "            id_=str(i), \n",
    "            text=chunk[\"text\"], \n",
    "            metadata = {\n",
    "                \"article\":chunk[\"article_title\"],\n",
    "                \"section\":chunk[\"section\"]\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(len(node_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08452dc6-24ee-4cd7-a0ce-fbb7998761fe",
   "metadata": {},
   "source": [
    "### 2) create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0239d39b-3953-46d3-b2f1-840fc6df076e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Model: text-embedding-ada-002\n",
      "Index Size: 778\n",
      "Embedding Size: 1536\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex(node_list)\n",
    "\n",
    "print(f\"Embedding Model: {index._embed_model.model_name}\")\n",
    "print(f\"Index Size: {len(index.vector_store.data.embedding_dict)}\")\n",
    "print(f\"Embedding Size: {len(index.vector_store.data.embedding_dict[\"0\"])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cd2e5f9-ce1b-4b50-931c-367c2f961948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c5bc4fa1db4ed3846320ef7182bdea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ce77d5644e4a3fb30ad18f58c90969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6484e78eefdc4eeda3e8036989861a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e1092aae6a4031a8884d495480310b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10df5854bd7f415fbdc3a5fe73af5220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16ee6c5b3db41c29bcf628e2f1e2bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7be774bcac4831ad46e06f10d1621c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd49b1b6c904a9ab429246e5b75ce2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1bfb243d5e49dcb531a82a3371c764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b7a468be914c288bfb0ac0ebeca5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eacbad711de41ff8ff683229aec75c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# changing embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "774fad49-b003-4a96-9e3e-e89e33854a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Model: BAAI/bge-small-en-v1.5\n",
      "Index Size: 778\n",
      "Embedding Size: 384\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex(node_list)\n",
    "\n",
    "print(f\"Embedding Model: {index._embed_model.model_name}\")\n",
    "print(f\"Index Size: {len(index.vector_store.data.embedding_dict)}\")\n",
    "print(f\"Embedding Size: {len(index.vector_store.data.embedding_dict[\"0\"])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac9ac6-134e-421c-904e-ec6bf81834db",
   "metadata": {},
   "source": [
    "### 3) semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fc70eb0-019e-424c-9d84-7522b83647fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fd93b0f-a618-43cb-a43a-2841697724a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retriever.retrieve(\"When do I perform fine-tuning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1936aff-9604-4bd1-a6e2-a4d42bc84d15",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeWithScore(node=TextNode(id_='155', embedding=None, metadata={'article': 'LLM Fine-tuning\\u200a—\\u200aFAQs', 'section': 'When do I Fine-tune?'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='This is not to say that fine-tuning is useless. A central benefit of fine-tuning an AI assistant is lowering inference costs [3].', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8114657060166933)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c42d5934-c0d8-4995-bdf2-4741c68443a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format results in markdown\n",
    "results_markdown = \"\"\n",
    "for i, result in enumerate(results, start=1):\n",
    "    results_markdown += f\"{i}. **Article title:** {result.metadata[\"article\"]}  \\n\"\n",
    "    results_markdown += f\"   **Section:** {result.metadata[\"section\"]}  \\n\"\n",
    "    results_markdown += f\"   **Snippet:** {result.text} \\n\\n\"\n",
    "    results_markdown += f\"   **Score:** {result.score} \\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9281d63-1cd9-4f20-b18c-c6d556eba289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** When do I Fine-tune?  \n",
       "   **Snippet:** This is not to say that fine-tuning is useless. A central benefit of fine-tuning an AI assistant is lowering inference costs [3]. \n",
       "\n",
       "   **Score:** 0.8114657060166933 \n",
       "\n",
       "2. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** When NOT to Fine-tune  \n",
       "   **Snippet:** The effectiveness of any approach will depend on the details of the use case. For example, fine-tuning is less effective than retrieval augmented generation (RAG) to provide LLMs with specialized knowledge [1]. \n",
       "\n",
       "   **Score:** 0.800293870277152 \n",
       "\n",
       "3. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** How to Prepare Data for Fine-tuning?  \n",
       "   **Snippet:** For example, if I wanted to fine-tune an LLM to respond to viewer questions on YouTube, I would need to gather a set of comments with questions and my associated responses. For a concrete example of this, check out the code walk-through on YouTube. \n",
       "\n",
       "   **Score:** 0.7996616635141707 \n",
       "\n",
       "4. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** When do I Fine-tune?  \n",
       "   **Snippet:** Fine-tuning, on the other hand, can compress prompt sizes by directly training the model on examples. Shorter prompts mean fewer tokens at inference, leading to lower compute costs and faster model responses [3]. For instance, after fine-tuning, the above prompt could be compressed to the following. \n",
       "\n",
       "   **Score:** 0.7995040458001792 \n",
       "\n",
       "5. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** We’ve already mentioned situations where RAG and fine-tuning perform well. However, since this is such a common question, it’s worth reemphasizing when each approach works best. \n",
       "\n",
       "   **Score:** 0.7930144142584221 \n",
       "\n",
       "6. **Article title:** Fine-Tuning Large Language Models (LLMs)  \n",
       "   **Section:** 3 Ways to Fine-tune  \n",
       "   **Snippet:** The next, and perhaps most popular, way to fine-tune a model is via supervised learning. This involves training a model on input-output pairs for a particular task. An example is instruction tuning, which aims to improve model performance in answering questions or responding to user prompts [1,3]. \n",
       "\n",
       "   **Score:** 0.7919754233525915 \n",
       "\n",
       "7. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Why we care  \n",
       "   **Snippet:** Previous articles in this series discussed fine-tuning, which adapts an existing model for a particular use case. While this is an alternative way to endow an LLM with specialized knowledge, empirically, fine-tuning seems to be less effective than RAG at doing this [1]. \n",
       "\n",
       "   **Score:** 0.7899395386438688 \n",
       "\n",
       "8. **Article title:** Fine-Tuning Large Language Models (LLMs)  \n",
       "   **Section:** What is Fine-tuning?  \n",
       "   **Snippet:** Fine-tuning is taking a pre-trained model and training at least one internal model parameter (i.e. weights). In the context of LLMs, what this typically accomplishes is transforming a general-purpose base model (e.g. GPT-3) into a specialized model for a particular use case (e.g. ChatGPT) [1]. \n",
       "\n",
       "   **Score:** 0.7895567465793972 \n",
       "\n",
       "9. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** What’s Next?  \n",
       "   **Snippet:** Here, I summarized the most common fine-tuning questions I’ve received over the past 12 months. While fine-tuning is not a panacea for all LLM use cases, it has key benefits. \n",
       "\n",
       "   **Score:** 0.7862102243041814 \n",
       "\n",
       "10. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** What is Fine-tuning?  \n",
       "   **Snippet:** I like to define fine-tuning as taking an existing (pre-trained) model and training at least 1 model parameter to adapt it to a particular use case. \n",
       "\n",
       "   **Score:** 0.7854351862786609 \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(results_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62772048-c152-4e5b-9afd-6e7fa4bc9e73",
   "metadata": {},
   "source": [
    "### 4) RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57b5730c-028e-424a-a0e4-b7ee5f445061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5630ecca-ebe8-4363-b0a7-511f7c0e1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9bec48c-41c6-4539-8cae-572c83827362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform fine-tuning when you want to lower inference costs by compressing prompt sizes through direct training on examples, resulting in fewer tokens at inference, lower compute costs, and faster model responses.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"When do I perform fine-tuning?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b77ffd35-8859-4215-815b-23f4cf6d5f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM: gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "print(f\"LLM: {Settings.llm.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "801d6739-d6e3-4480-a09a-91117c3cf5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# changing the global LLM\n",
    "Settings.llm = OpenAI(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a49a2e8-67e0-4adf-8cf8-cda579333312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning is performed when you want to lower inference costs for an AI assistant.\n"
     ]
    }
   ],
   "source": [
    "# simpler way to make query engine\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"When do I perform fine-tuning?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9485a01-a32a-47fa-8e12-e4be9fef595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "print(f\"LLM: {Settings.llm.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b8f37-9270-47e0-aeaa-59d0b9f49997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
