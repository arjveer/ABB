<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Cracking Open the OpenAI (Python) API</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Cracking Open the OpenAI (Python) API</h1>
</header>
<section data-field="subtitle" class="p-summary">
A complete beginner-friendly introduction with example code
</section>
<section data-field="body" class="e-content">
<section name="15e6" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="8e91" id="8e91" class="graf graf--h3 graf--leading graf--title"><strong class="markup--strong markup--h3-strong">Cracking Open the OpenAI (Python) API</strong></h3><h4 name="5446" id="5446" class="graf graf--h4 graf-after--h3 graf--subtitle"><strong class="markup--strong markup--h4-strong">A complete beginner-friendly introduction with example code</strong></h4><figure name="d449" id="d449" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="0*QzoGN7Zhi3m21yU0" data-width="6000" data-height="4000" data-unsplash-photo-id="G78c3DPmD_A" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*QzoGN7Zhi3m21yU0"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@martinsanchez?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@martinsanchez?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener noopener" target="_blank">Martin Sanchez</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener noopener" target="_blank">Unsplash</a></figcaption></figure><p name="c956" id="c956" class="graf graf--p graf-after--figure">This is the 2nd article in a <a href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" data-href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">series</a> on using Large Language Models (LLMs) in practice. Here I present a beginner-friendly introduction to the OpenAI API. This allows you to go beyond restrictive chat interfaces like ChatGPT and to get more out of LLMs for your unique use cases. Python example code is provided below and at the <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/openai-api" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/openai-api" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub repository</a>.</p><h4 name="0c89" id="0c89" class="graf graf--h4 graf-after--p">Table of Contents:</h4><ol class="postList"><li name="2a5d" id="2a5d" class="graf graf--li graf-after--h4">What’s an API?</li><li name="89e5" id="89e5" class="graf graf--li graf-after--li">OpenAI’s (Python) API</li><li name="a146" id="a146" class="graf graf--li graf-after--li">Getting Started (4 Steps)</li><li name="6ce5" id="6ce5" class="graf graf--li graf-after--li graf--trailing">Example Code</li></ol></div></div></section><section name="7da3" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="9789" id="9789" class="graf graf--p graf--leading">In the <a href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" data-href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">first article</a> of this series, I described <a href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971" data-href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Prompt Engineering</strong></a> as the <strong class="markup--strong markup--p-strong">most accessible way to use LLMs</strong> in practice. The easiest (and most popular) way to do this is via tools like ChatGPT, which provide an intuitive, no-cost, and no-code way to interact with an LLM.</p><div name="221d" id="221d" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" data-href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148"><strong class="markup--strong markup--mixtapeEmbed-strong">A Practical Introduction to LLMs</strong><br><em class="markup--em markup--mixtapeEmbed-em">3 levels of using LLMs in practice</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="4575f163eb3b52d853feaadd43ac9d97" data-thumbnail-img-id="0*Nl-5C1WBW4XdGkNI" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*Nl-5C1WBW4XdGkNI);"></a></div><p name="463f" id="463f" class="graf graf--p graf-after--mixtapeEmbed">However, this <strong class="markup--strong markup--p-strong">ease of use comes at a cost</strong>. Namely, the chat UI is restrictive and does not translate well to many practical use cases e.g. building your own customer support bot, real-time sentiment analysis of customer reviews, etc.</p><p name="11a7" id="11a7" class="graf graf--p graf-after--p">In these cases, we can take Prompt Engineering one step further and interact with LLMs <em class="markup--em markup--p-em">programmatically</em>. One way we can do this is via an API.</p><figure name="48af" id="48af" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/czvVibB2lRA?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><h3 name="e151" id="e151" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">1) What’s an API?</strong></h3><p name="3a71" id="3a71" class="graf graf--p graf-after--h3">An <strong class="markup--strong markup--p-strong">application programming interface (API)</strong> allows you to interact with a remote application programmatically. While this might sound technical and scary, the idea is super simple. Consider the following analogy.</p><p name="d935" id="d935" class="graf graf--p graf-after--p">Imagine you have an intense craving for the <a href="https://en.wikipedia.org/wiki/Pupusa" data-href="https://en.wikipedia.org/wiki/Pupusa" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">pupusas</a> you ate during that summer in El Salvador. Unfortunately, you’re back at home and don’t know where to find good Salvadoran food. Lucky for you, however, you have a super-foodie friend that knows every restaurant in town.</p><p name="58cb" id="58cb" class="graf graf--p graf-after--p">So, you send your friend the text.</p><blockquote name="7b93" id="7b93" class="graf graf--blockquote graf--startsWithDoubleQuote graf-after--p">“Any good pupusa spots in town?”</blockquote><p name="afa6" id="afa6" class="graf graf--p graf-after--blockquote">Then, a couple of minutes later, you get the response.</p><blockquote name="c647" id="c647" class="graf graf--blockquote graf--startsWithDoubleQuote graf-after--p">“Yes! Flavors of El Salvador has the best pupusas!”</blockquote><p name="2411" id="2411" class="graf graf--p graf-after--blockquote">While this may seem irrelevant to APIs, this is essentially how they work. You send a <strong class="markup--strong markup--p-strong">request</strong> to a remote application i.e. text your super-foodie friend. Then, the remote application sends back a <strong class="markup--strong markup--p-strong">response</strong> i.e. the text back from your friend.</p><figure name="8a99" id="8a99" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Z3V8d6PFcPv5DPaX9zXm-A.png" data-width="1500" data-height="750" src="https://cdn-images-1.medium.com/max/800/1*Z3V8d6PFcPv5DPaX9zXm-A.png"><figcaption class="imageCaption">A visual analogy of how APIs work. Image by author.</figcaption></figure><p name="db67" id="db67" class="graf graf--p graf-after--figure">The difference between an API and the above analogy is instead of sending the request with your phone’s texting app, you use your favorite programming language e.g. Python, JavaScript, Ruby, Java, etc. This is great if you are developing software where some external information is required because the information retrieval can be automated.</p><h3 name="c096" id="c096" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">2) OpenAI’s (Python) API</strong></h3><p name="3cf7" id="3cf7" class="graf graf--p graf-after--h3">We can use APIs to interact with Large Language Models. A popular one is OpenAI’s API, where instead of typing prompts into the ChatGPT web interface, you can send them to and from OpenAI using Python.</p><figure name="0a76" id="0a76" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*lo-PFSaZ9Z9BqtYBlo78cA.png" data-width="1500" data-height="750" src="https://cdn-images-1.medium.com/max/800/1*lo-PFSaZ9Z9BqtYBlo78cA.png"><figcaption class="imageCaption">Visualization of how API calls to OpenAI works. Image by author.</figcaption></figure><p name="cbe9" id="cbe9" class="graf graf--p graf-after--figure">This gives virtually anyone access to state-of-the-art LLMs (and other ML models) without having to provision the computational resources needed to run them. The downside, of course, is OpenAI doesn’t do this as a charity. Each API call costs money, but more on that in a bit.</p><p name="5f81" id="5f81" class="graf graf--p graf-after--p">Some <strong class="markup--strong markup--p-strong">notable features</strong> of the API (not available with ChatGPT) are listed below.</p><ul class="postList"><li name="b960" id="b960" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Customizable system message</strong> (this is set to something like “<em class="markup--em markup--li-em">I am ChatGPT, a large language model trained by OpenAI, based on the GPT-3.5 architecture. My knowledge is based on information available up until September 2021. Today’s date is July 13, 2023.</em>” for ChatGPT)</li><li name="81bc" id="81bc" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Adjust input parameters</strong> such as maximum response length, number of responses, and temperature (i.e. the “randomness” of the response).</li><li name="c700" id="c700" class="graf graf--li graf-after--li">Include <strong class="markup--strong markup--li-strong">images</strong> and <strong class="markup--strong markup--li-strong">other file types</strong> in prompts</li><li name="1829" id="1829" class="graf graf--li graf-after--li">Extract helpful word <strong class="markup--strong markup--li-strong">embeddings</strong> for downstream tasks</li><li name="3df1" id="3df1" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Input audio</strong> for transcription or translation</li><li name="6b3d" id="6b3d" class="graf graf--li graf-after--li">Model <strong class="markup--strong markup--li-strong">fine-tuning</strong> functionality</li></ul><p name="d597" id="d597" class="graf graf--p graf-after--li">The OpenAI API has <a href="https://platform.openai.com/docs/models" data-href="https://platform.openai.com/docs/models" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">several models</a> from which to choose. The <em class="markup--em markup--p-em">best</em> model to pick will depend on your particular use case. Below is a list of the current models available [<a href="https://platform.openai.com/docs/models" data-href="https://platform.openai.com/docs/models" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">1</a>].</p><figure name="c2e5" id="c2e5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*DDldra_REDf4A_bBdW0McA.png" data-width="1500" data-height="750" src="https://cdn-images-1.medium.com/max/800/1*DDldra_REDf4A_bBdW0McA.png"><figcaption class="imageCaption">List of available models via the OpenAI API as of Jul 2023. Image by author. [<a href="https://platform.openai.com/docs/models" data-href="https://platform.openai.com/docs/models" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">1</a>]</figcaption></figure><p name="254f" id="254f" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Note</em></strong><em class="markup--em markup--p-em">: Each item listed above is accompanied by a set of models which vary in size and cost. Check </em><a href="https://platform.openai.com/docs/models" data-href="https://platform.openai.com/docs/models" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">documentation</em></a><em class="markup--em markup--p-em"> for the most recent information.</em></p><h4 name="c72b" id="c72b" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Pricing &amp; Tokens</strong></h4><p name="2680" id="2680" class="graf graf--p graf-after--h4">While the OpenAI API gives developers easy access to SOTA ML models, one obvious downside is that it <strong class="markup--strong markup--p-strong">costs money</strong>. Pricing is done on a per-token basis (no, I don’t mean NFTs or something you use at the arcade).</p><p name="945b" id="945b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Tokens</strong>, in the context of LLMs, are essentially <strong class="markup--strong markup--p-strong">a set of numbers representing a set of words and characters</strong>. For example, “The” could be a token, “ end” (with the space) could be another, and “.” another.</p><p name="4eed" id="4eed" class="graf graf--p graf-after--p">Thus, the text “The End.” would consist of 3 tokens say (73, 102, 6).</p><figure name="a716" id="a716" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*HJtGJuyQFbbzKKd3TzFFmw.png" data-width="1500" data-height="750" src="https://cdn-images-1.medium.com/max/800/1*HJtGJuyQFbbzKKd3TzFFmw.png"><figcaption class="imageCaption">Toy example showing one possible token mapping between text and integers. Image by author.</figcaption></figure><p name="7be2" id="7be2" class="graf graf--p graf-after--figure">This is a critical step because <strong class="markup--strong markup--p-strong">LLMs (i.e. neural networks) do not “understand” text directly</strong>. The text must be converted into a numerical representation so that the model can perform mathematical operations on the input. Hence, the tokenization step.</p><p name="311f" id="311f" class="graf graf--p graf-after--p">The price of an API call depends on the number of tokens used in the prompt and the model being prompted. The price per model is available on <a href="https://openai.com/pricing" data-href="https://openai.com/pricing" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenAI’s website</a>.</p><h3 name="cb7d" id="cb7d" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">3) Getting Started (4 Steps)</strong></h3><p name="32e9" id="32e9" class="graf graf--p graf-after--h3">Now that we have a basic understanding of the OpenAI API let’s see how to use it. Before we can start coding, we need to set up four things.</p><h4 name="86d4" id="86d4" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">3.1) Make an Account (you get a $5 API credit for 1st three months)</strong></h4><ol class="postList"><li name="0c08" id="0c08" class="graf graf--li graf-after--h4">To make an account go to the <a href="https://platform.openai.com/overview" data-href="https://platform.openai.com/overview" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">OpenAI API Overview page</a>, and click “Sign Up” in the top right corner</li><li name="18ea" id="18ea" class="graf graf--li graf-after--li"><em class="markup--em markup--li-em">Note</em> — If you’ve used ChatGPT before, then you probably already have an OpenAI account. If so, click “Log in”</li></ol><h4 name="4909" id="4909" class="graf graf--h4 graf-after--li"><strong class="markup--strong markup--h4-strong">3.2) Add Payment Method</strong></h4><ol class="postList"><li name="46ea" id="46ea" class="graf graf--li graf-after--h4">If your account is more than 3 months old or the free $5 API credit is not enough for you, you will need to add a payment method before making API calls.</li><li name="dd0e" id="dd0e" class="graf graf--li graf-after--li">Click your profile image and select the manage account option.</li><li name="5acd" id="5acd" class="graf graf--li graf-after--li">Then add a payment method by clicking the “Billing” tab and then “Payment methods”.</li></ol><h4 name="800c" id="800c" class="graf graf--h4 graf-after--li"><strong class="markup--strong markup--h4-strong">3.3) Set Usage Limits</strong></h4><ol class="postList"><li name="525d" id="525d" class="graf graf--li graf-after--h4">Next, I recommend setting usage limits so that you <strong class="markup--strong markup--li-strong">avoid being billed more than you budget for</strong>.</li><li name="ef54" id="ef54" class="graf graf--li graf-after--li">To do this, go to the “Usage limits” under the “Billing” tab. Here you can set a “Soft” and “Hard” limit.</li><li name="cc0d" id="cc0d" class="graf graf--li graf-after--li">If you hit your monthly <strong class="markup--strong markup--li-strong">soft limit,</strong> OpenAI will send you an <strong class="markup--strong markup--li-strong">email notification</strong>.</li><li name="22d2" id="22d2" class="graf graf--li graf-after--li">If you hit your <strong class="markup--strong markup--li-strong">hard limit,</strong> any additional API <strong class="markup--strong markup--li-strong">requests will be denied</strong> (thus, you won’t be charged more than this).</li></ol><h4 name="4146" id="4146" class="graf graf--h4 graf-after--li"><strong class="markup--strong markup--h4-strong">3.4) Get API Secret Key</strong></h4><ol class="postList"><li name="aa27" id="aa27" class="graf graf--li graf-after--h4">Click on “View API keys”</li><li name="6164" id="6164" class="graf graf--li graf-after--li">If this is your first time, you will need to make a new secret key. To do this, click “Create new secret key”</li><li name="40cd" id="40cd" class="graf graf--li graf-after--li">Next, you can give your key a custom name. Here I used “my-first-key”.</li><li name="52a8" id="52a8" class="graf graf--li graf-after--li">Then, click “Create secret key”</li></ol><h3 name="bd71" id="bd71" class="graf graf--h3 graf-after--li"><strong class="markup--strong markup--h3-strong">4) Example Code: Chat Completion API</strong></h3><p name="b338" id="b338" class="graf graf--p graf-after--h3 graf--trailing">With all the setup done, we are (finally) ready to make our first API call. Here we will use the <a href="https://github.com/openai/openai-python" data-href="https://github.com/openai/openai-python" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">openai Python library</a>, which makes integrating OpenAI’s models into your Python code super easy. You can download the package via <a href="https://pypi.org/project/openai/" data-href="https://pypi.org/project/openai/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">pip</a><em class="markup--em markup--p-em">.</em> The below example code (and bonus code) is available on the <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/openai-api" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/openai-api" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub repo</a> for this article.</p></div></div></section><section name="a8cf" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="2c93" id="2c93" class="graf graf--p graf--leading"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">A quick note on Completions API deprecations </em>— </strong>OpenAI is moving away from the freeform prompt paradigm and toward chat-based API calls. According to a blog from OpenAI, the chat-based paradigm provides better responses, given its structured prompt interface, compared to the previous paradigm [<a href="https://openai.com/blog/gpt-4-api-general-availability" data-href="https://openai.com/blog/gpt-4-api-general-availability" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">2</a>].</p><p name="cb92" id="cb92" class="graf graf--p graf-after--p graf--trailing">While older OpenAI (GPT-3) models are still available via the “freeform” paradigm, the more recent (and powerful) models (i.e. GPT-3.5-turbo and GPT-4) are only available via chat-based calls.</p></div></div></section><section name="f3d9" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="c98d" id="c98d" class="graf graf--p graf--leading">Let’s start with a super simple API call. Here we will pass <strong class="markup--strong markup--p-strong">two inputs</strong> into the <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">openai.ChatCompletions.create()</em></strong> method i.e. <strong class="markup--strong markup--p-strong">model</strong> and <strong class="markup--strong markup--p-strong">messages</strong>.</p><ol class="postList"><li name="9038" id="9038" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">model</strong> — defines the name of the language model we want to use (we can choose from the models listed earlier in the article.)</li><li name="4a03" id="4a03" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">messages</strong> — sets the “preceding” chat dialogue as a list of dictionaries. The dictionaries have two key-value pairs (e.g. {“role”: “user”, “content”: “Listen to your”}.) <strong class="markup--strong markup--li-strong">First</strong>, “role” defines <em class="markup--em markup--li-em">who is talking</em> (e.g. “role”:”user”). This can either be the “user”, “assistant”, or “system”. <strong class="markup--strong markup--li-strong">Second</strong>, “content” defines <em class="markup--em markup--li-em">what the role is saying</em> (e.g. “content”: “Listen to your”). While this may feel more restrictive than a freeform prompt interface, we can get creative with input messages to optimize responses for a particular use case (more on this later).</li></ol><p name="3eb0" id="3eb0" class="graf graf--p graf-after--li">This is what our first API call looks like in Python.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="3f9d" id="3f9d" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> openai<br /><span class="hljs-keyword">from</span> sk <span class="hljs-keyword">import</span> my_sk <span class="hljs-comment"># importing secret key from external file</span><br /><span class="hljs-keyword">import</span> time<br /><br /><span class="hljs-comment"># imported secret key (or just copy-paste it here)</span><br />openai.api_key = my_sk <br /><br /><span class="hljs-comment"># create a chat completion</span><br />chat_completion = openai.ChatCompletion.create(model=<span class="hljs-string">&quot;gpt-3.5-turbo&quot;</span>, <br />                    messages=[{<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Listen to your&quot;</span>}])</span></pre><p name="b0b4" id="b0b4" class="graf graf--p graf-after--pre">The API response is stored in the <em class="markup--em markup--p-em">chat_completion</em> variable. Printing <em class="markup--em markup--p-em">chat_completion</em>, we see that it is like a dictionary consisting of 6 key-value pairs.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="json" name="ae8a" id="ae8a" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-punctuation">{</span>&#x27;id&#x27;<span class="hljs-punctuation">:</span> &#x27;chatcmpl<span class="hljs-number">-7</span>dk1Jkf5SDm2422nYRPL9x0QrlhI4&#x27;<span class="hljs-punctuation">,</span><br /> &#x27;object&#x27;<span class="hljs-punctuation">:</span> &#x27;chat.completion&#x27;<span class="hljs-punctuation">,</span><br /> &#x27;created&#x27;<span class="hljs-punctuation">:</span> <span class="hljs-number">1689706049</span><span class="hljs-punctuation">,</span><br /> &#x27;model&#x27;<span class="hljs-punctuation">:</span> &#x27;gpt<span class="hljs-number">-3.5</span>-turbo<span class="hljs-number">-0613</span>&#x27;<span class="hljs-punctuation">,</span><br /> &#x27;choices&#x27;<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>&lt;OpenAIObject at <span class="hljs-number">0x7f9d1a862b80</span>&gt; JSON<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span><br />    <span class="hljs-attr">&quot;index&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span><br />    <span class="hljs-attr">&quot;message&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span><br />      <span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span><br />      <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;heart.&quot;</span><br />    <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br />    <span class="hljs-attr">&quot;finish_reason&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;stop&quot;</span><br />  <span class="hljs-punctuation">}</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br /> &#x27;usage&#x27;<span class="hljs-punctuation">:</span> &lt;OpenAIObject at <span class="hljs-number">0x7f9d1a862c70</span>&gt; JSON<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span><br />   <span class="hljs-attr">&quot;prompt_tokens&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">10</span><span class="hljs-punctuation">,</span><br />   <span class="hljs-attr">&quot;completion_tokens&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span><br />   <span class="hljs-attr">&quot;total_tokens&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">12</span><br /> <span class="hljs-punctuation">}</span><span class="hljs-punctuation">}</span></span></pre><p name="8045" id="8045" class="graf graf--p graf-after--pre">The meaning of each field is listed below.</p><ul class="postList"><li name="844e" id="844e" class="graf graf--li graf--startsWithSingleQuote graf-after--p"><strong class="markup--strong markup--li-strong">‘Id’</strong> = unique ID for the API response</li><li name="bdc9" id="bdc9" class="graf graf--li graf--startsWithSingleQuote graf-after--li"><strong class="markup--strong markup--li-strong">‘Object’</strong> = name of API object that sent the response</li><li name="8bd7" id="8bd7" class="graf graf--li graf--startsWithSingleQuote graf-after--li"><strong class="markup--strong markup--li-strong">‘Created’</strong> = unix timestamp of when the API request was processed</li><li name="4bb0" id="4bb0" class="graf graf--li graf--startsWithSingleQuote graf-after--li"><strong class="markup--strong markup--li-strong">‘Model’</strong> = name of the model used</li><li name="34a3" id="34a3" class="graf graf--li graf--startsWithSingleQuote graf-after--li"><strong class="markup--strong markup--li-strong">‘Choices’</strong> = model response formatted in JSON (i.e. dictionary-like)</li><li name="f365" id="f365" class="graf graf--li graf--startsWithSingleQuote graf-after--li"><strong class="markup--strong markup--li-strong">‘Usage’</strong> = token count meta-data formatted in JSON (i.e. dictionary-like)</li></ul><p name="9d92" id="9d92" class="graf graf--p graf-after--li">However, the main thing we care about here is the ‘<strong class="markup--strong markup--p-strong">Choices</strong>’ field since this is <strong class="markup--strong markup--p-strong">where the model response is stored</strong>. In this case, we see the “assistant” role responds with the message <em class="markup--em markup--p-em">“</em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">heart.”</em></strong></p><p name="43f3" id="43f3" class="graf graf--p graf-after--p">Yay! We made our 1st API call. Now let’s start playing with the model input parameters.</p><h4 name="126e" id="126e" class="graf graf--h4 graf-after--p">max_tokens</h4><p name="30a0" id="30a0" class="graf graf--p graf-after--h4">First, we can set the <strong class="markup--strong markup--p-strong">maximum number of tokens</strong> allowed in the model response using the <em class="markup--em markup--p-em">max_tokens</em> input parameter. This can be helpful for many reasons depending on the use case. In this case, I just want a one-word response, so I’ll set it to 1 token.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="48da" id="48da" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># setting max number of tokens</span><br /><br /><span class="hljs-comment"># create a chat completion</span><br />chat_completion = openai.ChatCompletion.create(model=<span class="hljs-string">&quot;gpt-3.5-turbo&quot;</span>, <br />                    messages=[{<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Listen to your&quot;</span>}],<br />                    max_tokens = <span class="hljs-number">1</span>)<br /><br /><span class="hljs-comment"># print the chat completion</span><br /><span class="hljs-built_in">print</span>(chat_completion.choices[<span class="hljs-number">0</span>].message.content)<br /><br /><span class="hljs-string">&quot;&quot;&quot;<br />Output:<br /><span class="hljs-meta">&gt;&gt;&gt; </span>heart <br />&quot;&quot;&quot;</span></span></pre><h4 name="39ee" id="39ee" class="graf graf--h4 graf-after--pre">n</h4><p name="212c" id="212c" class="graf graf--p graf-after--h4">Next, we can set the <strong class="markup--strong markup--p-strong">number of responses</strong> we would like to receive from the model. Again, this can be helpful for many reasons depending on the use case. For example, if we want to generate a set of responses from which we can select the one we like best.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="f26b" id="f26b" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># setting number of completions</span><br /><br /><span class="hljs-comment"># create a chat completion</span><br />chat_completion = openai.ChatCompletion.create(model=<span class="hljs-string">&quot;gpt-3.5-turbo&quot;</span>, <br />                                messages=[{<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Listen to your&quot;</span>}],<br />                                max_tokens = <span class="hljs-number">2</span>,<br />                                n=<span class="hljs-number">5</span>)<br /><br /><span class="hljs-comment"># print the chat completion</span><br /><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(chat_completion.choices)):<br />    <span class="hljs-built_in">print</span>(chat_completion.choices[i].message.content)<br /><br /><span class="hljs-string">&quot;&quot;&quot;<br />Ouput:<br /><span class="hljs-meta">&gt;&gt;&gt; </span>heart.<br /><span class="hljs-meta">&gt;&gt;&gt; </span>heart and<br /><span class="hljs-meta">&gt;&gt;&gt; </span>heart.<br />&gt;&gt;&gt;<br /><span class="hljs-meta">&gt;&gt;&gt; </span>heart,<br />&gt;&gt;&gt;<br /><span class="hljs-meta">&gt;&gt;&gt; </span>heart,<br />&quot;&quot;&quot;</span></span></pre><p name="b04f" id="b04f" class="graf graf--p graf-after--pre">Notice that <strong class="markup--strong markup--p-strong">not all the completions are identical</strong>. This may be a good thing or a bad thing based on the use case (e.g. creative use cases vs. process automation use cases). Therefore, it can be advantageous to adjust the <em class="markup--em markup--p-em">diversity</em> of chat completions for a given prompt.</p><h4 name="82cf" id="82cf" class="graf graf--h4 graf-after--p">temperature</h4><p name="4d96" id="4d96" class="graf graf--p graf-after--h4">It turns out we can do this by tuning the <strong class="markup--strong markup--p-strong">temperature</strong> parameter. Put simply, this <strong class="markup--strong markup--p-strong">adjusts the “randomness” of chat completions</strong>.<em class="markup--em markup--p-em"> </em>Values for this parameter <strong class="markup--strong markup--p-strong">range from 0 to 2</strong>, where 0 makes completions more predictable, and 2 makes them less predictable [<a href="https://platform.openai.com/docs/api-reference/chat/create" data-href="https://platform.openai.com/docs/api-reference/chat/create" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">3</a>].</p><p name="58c6" id="58c6" class="graf graf--p graf-after--p">Conceptually, we can think of temp=0 will default to the most likely next word while temp=2 will enable completions that are relatively unlikely. Let’s see what this looks like.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="fde7" id="fde7" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># temperature=0</span><br /><br /><span class="hljs-comment"># create a chat completion</span><br />chat_completion = openai.ChatCompletion.create(model=<span class="hljs-string">&quot;gpt-3.5-turbo&quot;</span>, <br />                                messages=[{<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Listen to your&quot;</span>}],<br />                                max_tokens = <span class="hljs-number">2</span>,<br />                                n=<span class="hljs-number">5</span>,<br />                                temperature=<span class="hljs-number">0</span>)<br /><br /><span class="hljs-comment"># print the chat completion</span><br /><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(chat_completion.choices)):<br />    <span class="hljs-built_in">print</span>(chat_completion.choices[i].message.content)<br /><br /><span class="hljs-string">&quot;&quot;&quot;<br />Output:<br /><span class="hljs-meta">&gt;&gt;&gt; </span>heart.<br /><span class="hljs-meta">&gt;&gt;&gt; </span>heart.<br /><span class="hljs-meta">&gt;&gt;&gt; </span>heart.<br /><span class="hljs-meta">&gt;&gt;&gt; </span>heart.<br /><span class="hljs-meta">&gt;&gt;&gt; </span>heart.<br />&quot;&quot;&quot;</span></span></pre><p name="9c91" id="9c91" class="graf graf--p graf-after--pre">As expected, when temp=0, all 5 completions are identical and produce something “very likely.” Now let’s see what happens when we <strong class="markup--strong markup--p-strong">turn up the temperature</strong>.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="fdde" id="fdde" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># temperature=2</span><br /><br /><span class="hljs-comment"># create a chat completion</span><br />chat_completion = openai.ChatCompletion.create(model=<span class="hljs-string">&quot;gpt-3.5-turbo&quot;</span>, <br />                                messages=[{<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Listen to your&quot;</span>}],<br />                                max_tokens = <span class="hljs-number">2</span>,<br />                                n=<span class="hljs-number">5</span>,<br />                                temperature=<span class="hljs-number">2</span>)<br /><br /><span class="hljs-comment"># print the chat completion</span><br /><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(chat_completion.choices)):<br />    <span class="hljs-built_in">print</span>(chat_completion.choices[i].message.content)<br /><br /><span class="hljs-string">&quot;&quot;&quot;<br />Output:<br /><span class="hljs-meta">&gt;&gt;&gt; </span>judgment<br /><span class="hljs-meta">&gt;&gt;&gt; </span>Advice<br /><span class="hljs-meta">&gt;&gt;&gt; </span>.inner awareness<br /><span class="hljs-meta">&gt;&gt;&gt; </span>heart.<br />&gt;&gt;&gt;<br /><span class="hljs-meta">&gt;&gt;&gt; </span>ging ist<br />&quot;&quot;&quot;</span></span></pre><p name="da6a" id="da6a" class="graf graf--p graf-after--pre">Again, as expected, the chat completions with temp=2 were much more diverse and “out of pocket.”</p><h4 name="cd10" id="cd10" class="graf graf--h4 graf-after--p">messages roles: Lyric Completion Assistant</h4><p name="4dea" id="4dea" class="graf graf--p graf-after--h4">Finally, we can leverage the different roles in this chat-based prompting paradigm to adjust the language model responses even further.</p><p name="f5e4" id="f5e4" class="graf graf--p graf-after--p">Recall from earlier that we can include content from 3 different roles in our prompts: <strong class="markup--strong markup--p-strong">system</strong>, <strong class="markup--strong markup--p-strong">user</strong>, and <strong class="markup--strong markup--p-strong">assistant</strong>. The <strong class="markup--strong markup--p-strong">system </strong>message <strong class="markup--strong markup--p-strong">sets the context (or task) for model completions</strong> <em class="markup--em markup--p-em">e.g. “You are a friendly chatbot that does not want to destroy all humans” or “Summarize user prompts in max 10 words”.</em></p><p name="ad7f" id="ad7f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">User</strong> and <strong class="markup--strong markup--p-strong">assistant</strong> messages can be used in at least two ways. <strong class="markup--strong markup--p-strong">One</strong>, to generate examples for <strong class="markup--strong markup--p-strong">in-context learning</strong>, and <strong class="markup--strong markup--p-strong">two</strong>, to store and update <strong class="markup--strong markup--p-strong">conversation history</strong> for a real-time chatbot. Here we will use both ways to create a lyric completion assistant.</p><p name="94b3" id="94b3" class="graf graf--p graf-after--p">We start by making the <strong class="markup--strong markup--p-strong">system message</strong> <em class="markup--em markup--p-em">“I am Roxette lyric completion assistant. When given a line from a song, I will provide the next line in the song.” </em>Then, provide <strong class="markup--strong markup--p-strong">two examples of</strong> <strong class="markup--strong markup--p-strong">user and assistant messages</strong>. Followed by the same <strong class="markup--strong markup--p-strong">user prompt</strong> used in the preceding examples i.e.<em class="markup--em markup--p-em">“Listen to your”.</em></p><p name="6b30" id="6b30" class="graf graf--p graf-after--p">Here’s what that looks like in code.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="d945" id="d945" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># initial prompt with system message and 2 task examples</span><br />messages_list = [{<span class="hljs-string">&quot;role&quot;</span>:<span class="hljs-string">&quot;system&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;I am Roxette lyric completion assistant. When given a line from a song, I will provide the next line in the song.&quot;</span>},<br />                 {<span class="hljs-string">&quot;role&quot;</span>:<span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;I know there&#x27;s something in the wake of your smile&quot;</span>},<br />                 {<span class="hljs-string">&quot;role&quot;</span>:<span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;I get a notion from the look in your eyes, yeah&quot;</span>},<br />                 {<span class="hljs-string">&quot;role&quot;</span>:<span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;You&#x27;ve built a love but that love falls apart&quot;</span>},<br />                 {<span class="hljs-string">&quot;role&quot;</span>:<span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Your little piece of Heaven turns too dark&quot;</span>},<br />                 {<span class="hljs-string">&quot;role&quot;</span>:<span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Listen to your&quot;</span>}]<br /><br /><span class="hljs-comment"># sequentially generate 4 chat completions</span><br /><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br />    <span class="hljs-comment"># create a chat completion</span><br />    chat_completion = openai.ChatCompletion.create(model=<span class="hljs-string">&quot;gpt-3.5-turbo&quot;</span>, <br />                                    messages=messages_list,<br />                                    max_tokens = <span class="hljs-number">15</span>,<br />                                    n=<span class="hljs-number">1</span>,<br />                                    temperature=<span class="hljs-number">0</span>)<br /><br />    <span class="hljs-comment"># print the chat completion</span><br />    <span class="hljs-built_in">print</span>(chat_completion.choices[<span class="hljs-number">0</span>].message.content)<br /><br />    new_message = {<span class="hljs-string">&quot;role&quot;</span>:<span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>:chat_completion.choices[<span class="hljs-number">0</span>].message.content} <span class="hljs-comment"># append new message to message list</span><br />    messages_list.append(new_message)<br />    time.sleep(<span class="hljs-number">0.1</span>)<br /><br /><span class="hljs-string">&quot;&quot;&quot;<br />Output:<br /><span class="hljs-meta">&gt;&gt;&gt; </span>Heart when he&#x27;s calling for you<br /><span class="hljs-meta">&gt;&gt;&gt; </span>Listen to your heart, there&#x27;s nothing else you can do<br /><span class="hljs-meta">&gt;&gt;&gt; </span>I don&#x27;t know where you&#x27;re going and I don&#x27;t know why<br /><span class="hljs-meta">&gt;&gt;&gt; </span>But listen to your heart before you tell him goodbye<br />&quot;&quot;&quot;</span></span></pre><p name="0bdc" id="0bdc" class="graf graf--p graf-after--pre">Comparing the output to the <a href="https://www.azlyrics.com/lyrics/roxette/listentoyourheart.html" data-href="https://www.azlyrics.com/lyrics/roxette/listentoyourheart.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">actual lyrics</a> to the hit Roxette song, we see they are an exact match. This is due to the combination of all the different inputs we provided to the model.</p><p name="57a5" id="57a5" class="graf graf--p graf-after--p">To see what this looks like when we “<em class="markup--em markup--p-em">crank the temperature</em>,” check out the bonus code on <a href="https://github.com/ShawhinT/YouTube-Blog/blob/main/LLMs/openai-api/openai-api-demo.ipynb" data-href="https://github.com/ShawhinT/YouTube-Blog/blob/main/LLMs/openai-api/openai-api-demo.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>. (Warning: it gets weird)</p><h3 name="2877" id="2877" class="graf graf--h3 graf-after--p">Conclusion</h3><p name="4a7a" id="4a7a" class="graf graf--p graf-after--h3">Here I gave a beginner-friendly guide to the OpenAI Python API with example code. The biggest upside of using OpenAI’s API is you can work with powerful LLMs without worrying about provisioning computational resources. The <strong class="markup--strong markup--p-strong">downsides</strong>, however, are <strong class="markup--strong markup--p-strong">API calls cost money</strong> and potential <strong class="markup--strong markup--p-strong">security concerns</strong> of sharing some types of data with a 3rd party (OpenAI).</p><p name="069c" id="069c" class="graf graf--p graf-after--p">To avoid these downsides, we can turn to open-source LLM solutions. This will be the focus of the <a href="https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" data-href="https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" class="markup--anchor markup--p-anchor" target="_blank">next article</a> in this series, where we’ll explore the Hugging Face Transformers library.</p><p name="bea8" id="bea8" class="graf graf--p graf-after--p">👉 <strong class="markup--strong markup--p-strong">More on LLMs</strong>: <a href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" data-href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Introduction</a> | <a href="https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" data-href="https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" class="markup--anchor markup--p-anchor" target="_blank">Hugging Face Transformers</a> | <a href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" data-href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" class="markup--anchor markup--p-anchor" target="_blank">Prompt Engineering</a> | <a href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--p-anchor" target="_blank">Fine-tuning</a> | <a href="https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9" data-href="https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Build an LLM</a> | <a href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" data-href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">QLoRA</a> | <a href="https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac" data-href="https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac" class="markup--anchor markup--p-anchor" target="_blank">RAG</a> | <a href="https://towardsdatascience.com/text-embeddings-classification-and-semantic-search-8291746220be" data-href="https://towardsdatascience.com/text-embeddings-classification-and-semantic-search-8291746220be" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Text Embeddings</a></p><div name="b7e1" id="b7e1" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/list/8e009ae3054c" data-href="https://shawhin.medium.com/list/8e009ae3054c" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/list/8e009ae3054c"><strong class="markup--strong markup--mixtapeEmbed-strong">Large Language Models (LLMs)</strong><br><em class="markup--em markup--mixtapeEmbed-em">A series on how to use LLMs in practice.</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/list/8e009ae3054c" class="js-mixtapeImage mixtapeImage mixtapeImage--mediumCatalog  u-ignoreBlock" data-media-id="e38ca2d0de6aa61d7c09af603d8f43e1" data-thumbnail-img-id="0*ae40ceaca4c0bdcbdf431a66c777e68bffc1e356.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/304/160/0*ae40ceaca4c0bdcbdf431a66c777e68bffc1e356.jpeg);"></a></div></div></div></section><section name="2806" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="efb8" id="efb8" class="graf graf--h3 graf--leading">Resources</h3><p name="9954" id="9954" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Connect</strong>: <a href="https://shawhintalebi.com/" data-href="https://shawhintalebi.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">My website</a> | <a href="https://calendly.com/shawhintalebi" data-href="https://calendly.com/shawhintalebi" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Book a call</a> | <a href="https://shawhintalebi.com/contact/" data-href="https://shawhintalebi.com/contact/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Ask me anything</a></p><p name="90da" id="90da" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Socials</strong>: <a href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" data-href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">YouTube 🎥</a> | <a href="https://www.linkedin.com/in/shawhintalebi/" data-href="https://www.linkedin.com/in/shawhintalebi/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">LinkedIn</a> | <a href="https://twitter.com/ShawhinT" data-href="https://twitter.com/ShawhinT" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Twitter</a></p><p name="aac6" id="aac6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Support</strong>: <a href="https://www.buymeacoffee.com/shawhint" data-href="https://www.buymeacoffee.com/shawhint" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener noopener" target="_blank">Buy me a coffee</a> ☕️</p><div name="4f3a" id="4f3a" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/subscribe" data-href="https://shawhin.medium.com/subscribe" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/subscribe"><strong class="markup--strong markup--mixtapeEmbed-strong">Get FREE access to every new story I write</strong><br><em class="markup--em markup--mixtapeEmbed-em">Get FREE access to every new story I write P.S. I do not share your email with anyone By signing up, you will create a…</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/subscribe" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8d6d6c2d301f2c367b618a850fa468b3" data-thumbnail-img-id="0*O69FxQ5FttNEA_r4" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*O69FxQ5FttNEA_r4);"></a></div></div></div></section><section name="5d22" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="9c2c" id="9c2c" class="graf graf--p graf--leading">[1] <a href="https://platform.openai.com/docs/models" data-href="https://platform.openai.com/docs/models" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenAI Models documentation</a></p><p name="b480" id="b480" class="graf graf--p graf-after--p">[2] <a href="https://openai.com/blog/gpt-4-api-general-availability" data-href="https://openai.com/blog/gpt-4-api-general-availability" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GPT-4 Availability &amp; Completions API Deprecation</a></p><p name="7b48" id="7b48" class="graf graf--p graf-after--p graf--trailing">[3] Temperature definition from <a href="https://platform.openai.com/docs/api-reference/chat/create" data-href="https://platform.openai.com/docs/api-reference/chat/create" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">API reference</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@shawhin" class="p-author h-card">Shaw Talebi</a> on <a href="https://medium.com/p/230e4cae7971"><time class="dt-published" datetime="2023-07-21T15:44:21.249Z">July 21, 2023</time></a>.</p><p><a href="https://medium.com/@shawhin/cracking-open-the-openai-python-api-230e4cae7971" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 2, 2024.</p></footer></article></body></html>