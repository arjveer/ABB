<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Compressing Large Language Models (LLMs)</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Compressing Large Language Models (LLMs)</h1>
</header>
<section data-field="subtitle" class="p-summary">
Make LLMs 10X smaller without sacrificing performance
</section>
<section data-field="body" class="e-content">
<section name="613f" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="51bb" id="51bb" class="graf graf--h3 graf--leading graf--title"><strong class="markup--strong markup--h3-strong">Compressing Large Language Models (LLMs)</strong></h3><h4 name="d053" id="d053" class="graf graf--h4 graf-after--h3 graf--subtitle"><strong class="markup--strong markup--h4-strong">Make LLMs 10X smaller without sacrificing performance</strong></h4><figure name="7731" id="7731" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="1*VDHOHFG-PA8VddO-tuTFbA.png" data-width="1280" data-height="720" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*VDHOHFG-PA8VddO-tuTFbA.png"><figcaption class="imageCaption">Image from Canva.</figcaption></figure><p name="cb26" id="cb26" class="graf graf--p graf-after--figure graf--trailing">This article is part of a <a href="https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c" data-href="https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">larger series</a> on using large language models (LLMs) in practice. While the immense scale of LLMs is responsible for their impressive performance across a wide range of use cases, this presents <strong class="markup--strong markup--p-strong">challenges in their application to real-world problems</strong>. In this article, I discuss how we can overcome these challenges by compressing LLMs. I start with a high-level overview of key concepts and then walk through a concrete example with Python code.</p></div></div></section><section name="4cf0" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><figure name="f41b" id="f41b" class="graf graf--figure graf--iframe graf--leading"><iframe src="https://www.youtube.com/embed/FLkUOkeMd5M?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="de0b" id="de0b" class="graf graf--p graf-after--figure">The AI mantra of 2023 was <em class="markup--em markup--p-em">&quot;Bigger is Better</em>,&quot; where the equation for improving language models was pretty simple: <strong class="markup--strong markup--p-strong">more data + more parameters + more compute = better performance</strong> [1].</p><p name="6e32" id="6e32" class="graf graf--p graf-after--p">While this is likely still the case (GPT-5 coming soon?), there are obvious challenges with working with 100B+ parameter models. For example, a 100B parameter model using FP16 requires 200GB <em class="markup--em markup--p-em">just</em> for storage!</p><p name="5780" id="5780" class="graf graf--p graf-after--p">Needless to say, most consumer devices (e.g. phones, tablets, laptops) can’t handle models this big. <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">But.. what if we could make them smaller?</em></strong></p><h3 name="7f5d" id="7f5d" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Model Compression</strong></h3><p name="ee9e" id="ee9e" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Model compression</strong> aims to <strong class="markup--strong markup--p-strong">reduce the size of machine learning models without sacrificing performance</strong> [2]. This works for (big) neural networks because they are often <em class="markup--em markup--p-em">over-parameterized</em> (i.e. consist of redundant computational units) [3].</p><p name="d146" id="d146" class="graf graf--p graf-after--p">The key benefit of model compression is <strong class="markup--strong markup--p-strong">lower inference costs</strong>. This means wider accessibility of powerful ML models (i.e. running LLMs locally on your laptop), lower-cost integration of AI into consumer products, and on-device inference, which supports user privacy and security [3].</p><h3 name="fb7f" id="fb7f" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">3 Ways to Compress Models</strong></h3><p name="6a23" id="6a23" class="graf graf--p graf-after--h3">There is a wide range of techniques for model compression. Here, I will focus on 3 broad categories.</p><ul class="postList"><li name="3884" id="3884" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Quantization</strong> — Representing models with lower precision data types</li><li name="3fb7" id="3fb7" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Pruning</strong> — Removing unnecessary components from a model</li><li name="e2bc" id="e2bc" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Knowledge Distillation</strong> — Training a smaller model using a bigger one</li></ul><p name="30a5" id="30a5" class="graf graf--p graf-after--li"><em class="markup--em markup--p-em">Note</em>: these approaches are independent of one another. Thus, <strong class="markup--strong markup--p-strong">techniques from multiple categories can be combined</strong> for maximum compression!</p><h3 name="1eea" id="1eea" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">1) Quantization</strong></h3><p name="cc9e" id="cc9e" class="graf graf--p graf-after--h3">While quantization might sound like a scary and sophisticated word, it is a simple idea. It consists of <strong class="markup--strong markup--p-strong">lowering the precision of model parameters</strong>. You can think of this as converting a high-resolution image to a lower-resolution one while still maintaining the picture’s core properties.</p><figure name="ca70" id="ca70" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*M_0ZN9vERSCayZID56g1UA.png" data-width="1016" data-height="386" src="https://cdn-images-1.medium.com/max/800/1*M_0ZN9vERSCayZID56g1UA.png"><figcaption class="imageCaption">Quantization analogy. Image by author.</figcaption></figure><p name="758e" id="758e" class="graf graf--p graf-after--figure">Two common classes of quantization techniques are <strong class="markup--strong markup--p-strong">Post-training Quantization (PTQ)</strong> and <strong class="markup--strong markup--p-strong">Quantization-Aware Training (QAT)</strong>.</p><h4 name="c5ed" id="c5ed" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">1.1) Post-training Quantization (PTQ)</strong></h4><p name="7792" id="7792" class="graf graf--p graf-after--h4">Given a neural network, <strong class="markup--strong markup--p-strong">Post-training Quantization (PTQ)</strong> compresses the model by <strong class="markup--strong markup--p-strong">replacing parameters with a lower-precision data type</strong> (e.g. FP16 to INT-8). This is one of the fastest and simplest ways to reduce a model’s computational requirements because it <strong class="markup--strong markup--p-strong">requires no additional training or data labeling</strong> [4].</p><p name="2a22" id="2a22" class="graf graf--p graf-after--p">While this is a relatively easy way to cut model costs, excessive quantization in this way (e.g., FP16 to INT4) often leads to performance degradation, which limits the potential gains of PTQ. [3]</p><h4 name="300b" id="300b" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">1.2) Quantization-Aware Training (QAT)</strong></h4><p name="3f96" id="3f96" class="graf graf--p graf-after--h4">For situations where greater compression is needed, PTQ&#39;s limitations can be overcome by <strong class="markup--strong markup--p-strong">training models (from scratch) with lower-precision data types</strong>. This is the idea behind Quantization-Aware Training (QAT) [5].</p><p name="bab1" id="bab1" class="graf graf--p graf-after--p">While this approach is more technically demanding, it can lead to a significantly smaller, well-performing model. For instance, the BitNet architecture used a <strong class="markup--strong markup--p-strong">ternary data type (i.e. 1.58-bit)</strong> to match the performance of the original Llama LLM [6]!</p><p name="4934" id="4934" class="graf graf--p graf-after--p">Of course, a large technical gap exists between PTQ and from-scratch QAT. An approach between the two is <strong class="markup--strong markup--p-strong">Quantization-aware Fine-tuning</strong>, which consists of additional training of a pre-trained model after quantization [3].</p><div name="332b" id="332b" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" data-href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32"><strong class="markup--strong markup--mixtapeEmbed-strong">QLoRA — How to Fine-Tune an LLM on a Single GPU</strong><br><em class="markup--em markup--mixtapeEmbed-em">An introduction with Python example code (ft. Mistral-7b)</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="c0900ed2426afb6d07227db72e8115ed" data-thumbnail-img-id="0*C783ZfFIphu8eFdm" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*C783ZfFIphu8eFdm);"></a></div><h3 name="0933" id="0933" class="graf graf--h3 graf-after--mixtapeEmbed"><strong class="markup--strong markup--h3-strong">2) Pruning</strong></h3><p name="7540" id="7540" class="graf graf--p graf-after--h3">The goal of <strong class="markup--strong markup--p-strong">pruning</strong> is to <strong class="markup--strong markup--p-strong">remove model components that have little impact on performance</strong> [7]. This is effective because ML models (especially large ones) tend to learn redundant and noisy structures [3].</p><p name="7a90" id="7a90" class="graf graf--p graf-after--p">An analogy here is like clipping dead branches from a tree. Removing them reduces the size of the tree without harming it.</p><figure name="9a14" id="9a14" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*l9mhfjOIRnohnycafFXLIw.png" data-width="880" data-height="580" src="https://cdn-images-1.medium.com/max/800/1*l9mhfjOIRnohnycafFXLIw.png"><figcaption class="imageCaption">Pruning analogy. Image by author.</figcaption></figure><p name="b882" id="b882" class="graf graf--p graf-after--figure">Pruning approaches can be categorized into two buckets: <strong class="markup--strong markup--p-strong">Unstructured</strong> and <strong class="markup--strong markup--p-strong">Structured Pruning</strong>.</p><h4 name="07fa" id="07fa" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">2.1) Unstructured Pruning</strong></h4><p name="4137" id="4137" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">Unstructured pruning</strong> <strong class="markup--strong markup--p-strong">removes unimportant weights from a neural network</strong> (i.e. setting them to zero). For example, early works such as Optimal Brain Damage and Optimal Brain Surgeon computed a saliency score for each parameter in the network by estimating the impact pruning on the loss function [7].</p><p name="b21a" id="b21a" class="graf graf--p graf-after--p">More recently, magnitude-based approaches (i.e. removing weights with the smallest absolute value) have become more popular due to their simplicity and scalability [7].</p><p name="a12c" id="a12c" class="graf graf--p graf-after--p">While the granularity of unstructured pruning can significantly decrease parameter count, these <strong class="markup--strong markup--p-strong">gains require specialized hardware to be realized</strong> [7]. Unstructured pruning results in sparse matrix operations (i.e. multiplying matrixes with lots of zeros), which standard hardware cannot do more efficiently than non-sparse operations.</p><h4 name="329b" id="329b" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">2.2) Structured Pruning</strong></h4><p name="ed72" id="ed72" class="graf graf--p graf-after--h4">Alternatively, <strong class="markup--strong markup--p-strong">structured pruning</strong> <strong class="markup--strong markup--p-strong">removes entire structures from a neural network</strong> (e.g. attention heads, neurons, and layers) [5]. This avoids the spare matrix operation problem because entire matrices can be dropped from the model rather than individual parameters.</p><p name="7941" id="7941" class="graf graf--p graf-after--p">While there are various ways to identify structures for pruning, in principle, they all seek to remove structures with the smallest impact on performance. A survey of structured pruning approaches is available in reference [5].</p><h3 name="c79c" id="c79c" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">3) Knowledge Distillation</strong></h3><p name="d77f" id="d77f" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Knowledge Distillation</strong> <strong class="markup--strong markup--p-strong">transfers knowledge from a (larger) teacher model to a (smaller) student model</strong> [5]. One way to do this is to generate predictions with a teacher model and use them to train a student model. Learning from the teacher model’s output logits (i.e., probabilities for all possible next tokens) provides richer information than the original training data, which improves student model performance [8].</p><p name="b63e" id="b63e" class="graf graf--p graf-after--p">More recent distillation applications <strong class="markup--strong markup--p-strong">discard the need for logits</strong> altogether and<strong class="markup--strong markup--p-strong"> learn from synthetic data generated from the teacher model</strong>. A popular example is Stanford’s Alpaca model, which fine-tuned the LLaMa 7B (foundation) model using synthetic data from OpenAI’s text-davinci-003 (i.e. the original ChatGPT model), enabling it to follow user instructions [9].</p><h3 name="2943" id="2943" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization</strong></h3><p name="a62e" id="a62e" class="graf graf--p graf-after--h3">With a basic understanding of various compression techniques, let’s see a hands-on example of how to do this in Python. Here, we will compress a 100M parameter model that classifies URLs as safe or unsafe (i.e. phishing).</p><p name="0002" id="0002" class="graf graf--p graf-after--p">We first use knowledge distillation to compress the 100M parameter model into a 50M parameter one. Then, using 4-bit quantization, we further reduced the memory footprint by 3X, resulting in a <strong class="markup--strong markup--p-strong">final model that is 7X smaller than our original one</strong>.</p><p name="ff19" id="ff19" class="graf graf--p graf-after--p graf--trailing">The example code is available on <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/model-compression" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/model-compression" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>. The models (<a href="https://huggingface.co/shawhin/bert-phishing-classifier_teacher" data-href="https://huggingface.co/shawhin/bert-phishing-classifier_teacher" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Teacher</a>, <a href="https://huggingface.co/shawhin/bert-phishing-classifier_student" data-href="https://huggingface.co/shawhin/bert-phishing-classifier_student" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Student</a>, <a href="https://huggingface.co/shawhin/bert-phishing-classifier_student_4bit" data-href="https://huggingface.co/shawhin/bert-phishing-classifier_student_4bit" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Student-4bit</a>) and <a href="https://huggingface.co/datasets/shawhin/phishing-site-classification" data-href="https://huggingface.co/datasets/shawhin/phishing-site-classification" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">dataset</a> are freely available on the Hugging Face Hub.</p></div></div></section><section name="791b" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="d147" id="d147" class="graf graf--p graf--leading">We start by importing a few helpful libraries.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="52c9" id="52c9" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br /><br /><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification<br /><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertForSequenceClassification, DistilBertConfig<br /><br /><span class="hljs-keyword">import</span> torch<br /><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br /><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br /><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br /><br /><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score, precision_recall_fscore_support</span></pre><p name="b37f" id="b37f" class="graf graf--p graf-after--pre">Then, we load our dataset from the Hugging Face Hub. This includes training (2100 rows), testing (450 rows), and validation (450 rows) sets.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="1513" id="1513" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">data = load_dataset(<span class="hljs-string">&quot;shawhin/phishing-site-classification&quot;</span>)</span></pre><p name="4d5f" id="4d5f" class="graf graf--p graf-after--pre">Next, we load in our teacher model. To help speed up training, I loaded the model onto a T4 GPU that was freely available on Google Colab.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="2da7" id="2da7" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># use Nvidia GPU</span><br />device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span>)<br /><br /><span class="hljs-comment"># Load teacher model and tokenizer</span><br />model_path = <span class="hljs-string">&quot;shawhin/bert-phishing-classifier_teacher&quot;</span><br /><br />tokenizer = AutoTokenizer.from_pretrained(model_path)<br />teacher_model = AutoModelForSequenceClassification.from_pretrained(model_path)<br />                                                  .to(device)</span></pre><p name="f18d" id="f18d" class="graf graf--p graf-after--pre">The teacher model is a fine-tuned version of Goolge’s <a href="https://huggingface.co/google-bert/bert-base-uncased" data-href="https://huggingface.co/google-bert/bert-base-uncased" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">bert-base-uncased</a> that performs binary classification on phishing website URLs. The code to train the teacher model is available on <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/model-compression" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/model-compression" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p><p name="0e4b" id="0e4b" class="graf graf--p graf-after--p">For the student model, we initialize a new model from scratch based on <a href="https://huggingface.co/distilbert/distilbert-base-uncased" data-href="https://huggingface.co/distilbert/distilbert-base-uncased" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">distilbert-base-uncased</a>. We modify the architecture by removing two layers and four attention heads from the remaining layers.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="a8b5" id="a8b5" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># Load student model</span><br />my_config = DistilBertConfig(n_heads=<span class="hljs-number">8</span>, n_layers=<span class="hljs-number">4</span>) <span class="hljs-comment"># drop 4 heads per layer and 2 layers</span><br /><br />student_model = DistilBertForSequenceClassification<br />                                    .from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>,<br />                                    config=my_config,)<br />                                    .to(device)</span></pre><p name="7fa8" id="7fa8" class="graf graf--p graf-after--pre">Before we can train our student model, we will need to <strong class="markup--strong markup--p-strong">tokenize the dataset</strong>. This is important because the models expect input text to be represented in a particular way.</p><p name="f622" id="f622" class="graf graf--p graf-after--p">Here, I pad examples based on each batch&#39;s longest example. This allows the batches to be represented as a PyTorch tensor.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="79ab" id="79ab" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># define text preprocessing</span><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):<br />    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], padding=<span class="hljs-string">&#x27;max_length&#x27;</span>, truncation=<span class="hljs-literal">True</span>)<br /><br /><span class="hljs-comment"># tokenize all datasetse</span><br />tokenized_data = data.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)<br />tokenized_data.set_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;torch&#x27;</span>, <br />                          columns=[<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>])</span></pre><p name="8da0" id="8da0" class="graf graf--p graf-after--pre">Another important step before training is defining an evaluation strategy for our models during training. Below, I define a function that computes the <strong class="markup--strong markup--p-strong">accuracy, precision, recall, and F1 score</strong> given a model and dataset.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="789b" id="789b" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># Function to evaluate model performance</span><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_model</span>(<span class="hljs-params">model, dataloader, device</span>):<br />    model.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># Set model to evaluation mode</span><br />    all_preds = []<br />    all_labels = []<br /><br />    <span class="hljs-comment"># Disable gradient calculations</span><br />    <span class="hljs-keyword">with</span> torch.no_grad():<br />        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataloader:<br />            input_ids = batch[<span class="hljs-string">&#x27;input_ids&#x27;</span>].to(device)<br />            attention_mask = batch[<span class="hljs-string">&#x27;attention_mask&#x27;</span>].to(device)<br />            labels = batch[<span class="hljs-string">&#x27;labels&#x27;</span>].to(device)<br /><br />            <span class="hljs-comment"># Forward pass to get logits</span><br />            outputs = model(input_ids, attention_mask=attention_mask)<br />            logits = outputs.logits<br /><br />            <span class="hljs-comment"># Get predictions</span><br />            preds = torch.argmax(logits, dim=<span class="hljs-number">1</span>).cpu().numpy()<br />            all_preds.extend(preds)<br />            all_labels.extend(labels.cpu().numpy())<br /><br />    <span class="hljs-comment"># Calculate evaluation metrics</span><br />    accuracy = accuracy_score(all_labels, all_preds)<br />    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, <br />                                                              all_preds, <br />                                                              average=<span class="hljs-string">&#x27;binary&#x27;</span>)<br /><br />    <span class="hljs-keyword">return</span> accuracy, precision, recall, f1</span></pre><p name="89c4" id="89c4" class="graf graf--p graf-after--pre">Now, we are ready to begin the training process. To allow our student model to learn from both the ground truth labels in the training set (i.e., hard targets) and the teacher model’s logits (i.e., soft targets), we must construct a special loss function that considers both targets.</p><p name="e890" id="e890" class="graf graf--p graf-after--p">This is done by <strong class="markup--strong markup--p-strong">combining the KL divergence</strong> of the student and teacher’s output probability distribution <strong class="markup--strong markup--p-strong">with the cross entropy loss</strong> of the student’s logits with the ground truth.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="4afb" id="4afb" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># Function to compute distillation and hard-label loss</span><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">distillation_loss</span>(<span class="hljs-params">student_logits, teacher_logits, <br />                      true_labels, temperature, alpha</span>):<br />    <span class="hljs-comment"># Compute soft targets from teacher logits</span><br />    soft_targets = nn.functional.softmax(teacher_logits / temperature, dim=<span class="hljs-number">1</span>)<br />    student_soft = nn.functional.log_softmax(student_logits / temperature, dim=<span class="hljs-number">1</span>)<br /><br />    <span class="hljs-comment"># KL Divergence loss for distillation</span><br />    distill_loss = nn.functional.kl_div(student_soft, <br />                                    soft_targets, <br />                                    reduction=<span class="hljs-string">&#x27;batchmean&#x27;</span>) * (temperature ** <span class="hljs-number">2</span>)<br /><br />    <span class="hljs-comment"># Cross-entropy loss for hard labels</span><br />    hard_loss = nn.CrossEntropyLoss()(student_logits, true_labels)<br /><br />    <span class="hljs-comment"># Combine losses</span><br />    loss = alpha * distill_loss + (<span class="hljs-number">1.0</span> - alpha) * hard_loss<br /><br />    <span class="hljs-keyword">return</span> loss</span></pre><p name="037b" id="037b" class="graf graf--p graf-after--pre">Next, we define our hyperparameters, optimizer, and train/test datasets.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="8e84" id="8e84" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># hyperparameters</span><br />batch_size = <span class="hljs-number">32</span><br />lr = <span class="hljs-number">1e-4</span><br />num_epochs = <span class="hljs-number">5</span><br />temperature = <span class="hljs-number">2.0</span><br />alpha = <span class="hljs-number">0.5</span><br /><br /><span class="hljs-comment"># define optimizer</span><br />optimizer = optim.Adam(student_model.parameters(), lr=lr)<br /><br /><span class="hljs-comment"># create training data loader</span><br />dataloader = DataLoader(tokenized_data[<span class="hljs-string">&#x27;train&#x27;</span>], batch_size=batch_size)<br /><span class="hljs-comment"># create testing data loader</span><br />test_dataloader = DataLoader(tokenized_data[<span class="hljs-string">&#x27;test&#x27;</span>], batch_size=batch_size)</span></pre><p name="4d00" id="4d00" class="graf graf--p graf-after--pre">Finally, we train our student model using PyTorch.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="e34e" id="e34e" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># put student model in train mode</span><br />student_model.train()<br /><br /><span class="hljs-comment"># train model</span><br /><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br />    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataloader:<br />        <span class="hljs-comment"># Prepare inputs</span><br />        input_ids = batch[<span class="hljs-string">&#x27;input_ids&#x27;</span>].to(device)<br />        attention_mask = batch[<span class="hljs-string">&#x27;attention_mask&#x27;</span>].to(device)<br />        labels = batch[<span class="hljs-string">&#x27;labels&#x27;</span>].to(device)<br /><br />        <span class="hljs-comment"># Disable gradient calculation for teacher model</span><br />        <span class="hljs-keyword">with</span> torch.no_grad():<br />            teacher_outputs = teacher_model(input_ids, <br />                                            attention_mask=attention_mask)<br />            teacher_logits = teacher_outputs.logits<br /><br />        <span class="hljs-comment"># Forward pass through the student model</span><br />        student_outputs = student_model(input_ids, <br />                                        attention_mask=attention_mask)<br />        student_logits = student_outputs.logits<br /><br />        <span class="hljs-comment"># Compute the distillation loss</span><br />        loss = distillation_loss(student_logits, teacher_logits, labels, <br />                                  temperature, alpha)<br /><br />        <span class="hljs-comment"># Backpropagation</span><br />        optimizer.zero_grad()<br />        loss.backward()<br />        optimizer.step()<br /><br />    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">{epoch + <span class="hljs-number">1</span>}</span> completed with loss: <span class="hljs-subst">{loss.item()}</span>&quot;</span>)<br /><br />    <span class="hljs-comment"># Evaluate the teacher model</span><br />    teacher_accuracy, teacher_precision, teacher_recall, teacher_f1 = <br />                         evaluate_model(teacher_model, test_dataloader, device)<br /><br />    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Teacher (test) - Accuracy: <span class="hljs-subst">{teacher_accuracy:<span class="hljs-number">.4</span>f}</span>, <br />                              Precision: <span class="hljs-subst">{teacher_precision:<span class="hljs-number">.4</span>f}</span>, <br />                              Recall: <span class="hljs-subst">{teacher_recall:<span class="hljs-number">.4</span>f}</span>, <br />                              F1 Score: <span class="hljs-subst">{teacher_f1:<span class="hljs-number">.4</span>f}</span>&quot;</span>)<br /><br />    <span class="hljs-comment"># Evaluate the student model</span><br />    student_accuracy, student_precision, student_recall, student_f1 = <br />                         evaluate_model(student_model, test_dataloader, device)<br />    <br />    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Student (test) - Accuracy: <span class="hljs-subst">{student_accuracy:<span class="hljs-number">.4</span>f}</span>, <br />                              Precision: <span class="hljs-subst">{student_precision:<span class="hljs-number">.4</span>f}</span>, <br />                              Recall: <span class="hljs-subst">{student_recall:<span class="hljs-number">.4</span>f}</span>, <br />                              F1 Score: <span class="hljs-subst">{student_f1:<span class="hljs-number">.4</span>f}</span>&quot;</span>)<br />    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n&quot;</span>)<br /><br />    <span class="hljs-comment"># put student model back into train mode</span><br />    student_model.train()</span></pre><p name="b105" id="b105" class="graf graf--p graf-after--pre">The training results are shown in the screenshot below. Remarkably, by the end of training, the<strong class="markup--strong markup--p-strong"> student model outperformed the teacher</strong> across all evaluation metrics!</p><figure name="b503" id="b503" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-5iHN2Q3ZnrEBmucP1y8TQ.png" data-width="930" data-height="516" src="https://cdn-images-1.medium.com/max/800/1*-5iHN2Q3ZnrEBmucP1y8TQ.png"><figcaption class="imageCaption">Knowledge distillation training results. Image by author.</figcaption></figure><p name="b97e" id="b97e" class="graf graf--p graf-after--figure">As a final step, we can evaluate the models on the independent validation set, i.e., data not used in training model parameters or tuning hyperparameters.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="efdd" id="efdd" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># create testing data loader</span><br />validation_dataloader = DataLoader(tokenized_data[<span class="hljs-string">&#x27;validation&#x27;</span>], batch_size=<span class="hljs-number">8</span>)<br /><br /><span class="hljs-comment"># Evaluate the teacher model</span><br />teacher_accuracy, teacher_precision, teacher_recall, teacher_f1 = <br />                   evaluate_model(teacher_model, validation_dataloader, device)<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Teacher (validation) - Accuracy: <span class="hljs-subst">{teacher_accuracy:<span class="hljs-number">.4</span>f}</span>, <br />                               Precision: <span class="hljs-subst">{teacher_precision:<span class="hljs-number">.4</span>f}</span>, <br />                               Recall: <span class="hljs-subst">{teacher_recall:<span class="hljs-number">.4</span>f}</span>, <br />                               F1 Score: <span class="hljs-subst">{teacher_f1:<span class="hljs-number">.4</span>f}</span>&quot;</span>)<br /><br /><span class="hljs-comment"># Evaluate the student model</span><br />student_accuracy, student_precision, student_recall, student_f1 = <br />                   evaluate_model(student_model, validation_dataloader, device)<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Student (validation) - Accuracy: <span class="hljs-subst">{student_accuracy:<span class="hljs-number">.4</span>f}</span>, <br />                               Precision: <span class="hljs-subst">{student_precision:<span class="hljs-number">.4</span>f}</span>, <br />                               Recall: <span class="hljs-subst">{student_recall:<span class="hljs-number">.4</span>f}</span>, <br />                               F1 Score: <span class="hljs-subst">{student_f1:<span class="hljs-number">.4</span>f}</span>&quot;</span>)</span></pre><p name="79d1" id="79d1" class="graf graf--p graf-after--pre">Here, again, we see the student outperform the teacher.</p><figure name="3e6d" id="3e6d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kajMTfpJTJYcFMu-8UWvfA.png" data-width="1188" data-height="65" src="https://cdn-images-1.medium.com/max/800/1*kajMTfpJTJYcFMu-8UWvfA.png"><figcaption class="imageCaption">Model performances on the validation set. Image by author.</figcaption></figure><p name="3703" id="3703" class="graf graf--p graf-after--figure">So far, we’ve <strong class="markup--strong markup--p-strong">reduced our model from 109M parameters (438 MB) to 52.8M parameters (211 MB)</strong>. However, we can go one step further and quantize the student model.</p><p name="6d38" id="6d38" class="graf graf--p graf-after--p">First, we push the model of the <a href="https://huggingface.co/shawhin/bert-phishing-classifier_student" data-href="https://huggingface.co/shawhin/bert-phishing-classifier_student" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Hugging Face Hub</a>.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="6925" id="6925" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">student_model.push_to_hub(<span class="hljs-string">&quot;shawhin/bert-phishing-classifier_student&quot;</span>)</span></pre><p name="6f7c" id="6f7c" class="graf graf--p graf-after--pre">Then, we can load it back in using 4-bit quantization. For that, we can use the BitsAndBytes integration in the transformers library.</p><p name="658f" id="658f" class="graf graf--p graf-after--p">We set up the config to store model parameters using the <strong class="markup--strong markup--p-strong">4-bit NormalFloat</strong> <strong class="markup--strong markup--p-strong">data type </strong>described in the <a href="https://medium.com/towards-data-science/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" data-href="https://medium.com/towards-data-science/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" class="markup--anchor markup--p-anchor" target="_blank">QLoRA</a> paper and the <strong class="markup--strong markup--p-strong">bfloat16</strong> <strong class="markup--strong markup--p-strong">for computation</strong> [10].</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="01a4" id="01a4" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BitsAndBytesConfig<br /><br /><span class="hljs-comment"># load model in model as 4-bit</span><br />nf4_config = BitsAndBytesConfig(<br />    load_in_4bit=<span class="hljs-literal">True</span>,<br />    bnb_4bit_quant_type=<span class="hljs-string">&quot;nf4&quot;</span>,<br />    bnb_4bit_compute_dtype = torch.bfloat16,<br />    bnb_4bit_use_double_quant=<span class="hljs-literal">True</span><br />)<br /><br />model_nf4 = AutoModelForSequenceClassification.from_pretrained(model_id, <br />                                                device_map=device, <br />                                                quantization_config=nf4_config)</span></pre><p name="e322" id="e322" class="graf graf--p graf-after--pre">We can then evaluate our quantized model on the validation set.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="5782" id="5782" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># Evaluate the student model</span><br />quantized_accuracy, quantized_precision, quantized_recall, quantized_f1 = <br />                       evaluate_model(model_nf4, validation_dataloader, device)<br /><br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Post-quantization Performance&quot;</span>)<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Accuracy: <span class="hljs-subst">{quantized_accuracy:<span class="hljs-number">.4</span>f}</span>, <br />        Precision: <span class="hljs-subst">{quantized_precision:<span class="hljs-number">.4</span>f}</span>, <br />        Recall: <span class="hljs-subst">{quantized_recall:<span class="hljs-number">.4</span>f}</span>, <br />        F1 Score: <span class="hljs-subst">{quantized_f1:<span class="hljs-number">.4</span>f}</span>&quot;</span>)</span></pre><figure name="b255" id="b255" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*NtsbvduBzWt0V5puF2Eqfg.png" data-width="894" data-height="70" src="https://cdn-images-1.medium.com/max/800/1*NtsbvduBzWt0V5puF2Eqfg.png"><figcaption class="imageCaption">Student model performance on the validation set after quantization. Image by author.</figcaption></figure><p name="1c24" id="1c24" class="graf graf--p graf-after--figure">Once again, we see a small performance improvement after compression (down to 62.7MB). An intuitive explanation for this is <strong class="markup--strong markup--p-strong">Occam’s Razor principle</strong>, which states that <strong class="markup--strong markup--p-strong">simpler models are better</strong>.</p><p name="bf62" id="bf62" class="graf graf--p graf-after--p">In this case, the model may be overparameterized for this binary classification task. Thus, simplifying the model results in better performance.</p><h3 name="cb04" id="cb04" class="graf graf--h3 graf-after--p">Recap</h3><p name="86b7" id="86b7" class="graf graf--p graf-after--h3">While modern large language models (LLMs) demonstrate impressive performance on various tasks, their scale presents challenges in deploying them in real-world settings.</p><p name="44ca" id="44ca" class="graf graf--p graf-after--p">Recent innovations in model compression techniques help mitigate these challenges by reducing the computational cost of LLM solutions. Here, we discussed three broad categories of compression techniques (Quantization, Pruning, and Knowledge Distillation) and walked through an example implementation in Python.</p><p name="d21a" id="d21a" class="graf graf--p graf-after--p">More on LLMs 👇</p><div name="dc29" id="dc29" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://shawhin.medium.com/list/8e009ae3054c" data-href="https://shawhin.medium.com/list/8e009ae3054c" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/list/8e009ae3054c"><strong class="markup--strong markup--mixtapeEmbed-strong">Large Language Models (LLMs)</strong><br><em class="markup--em markup--mixtapeEmbed-em">A series on how to use LLMs in practice.</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/list/8e009ae3054c" class="js-mixtapeImage mixtapeImage mixtapeImage--mediumCatalog  u-ignoreBlock" data-media-id="e38ca2d0de6aa61d7c09af603d8f43e1" data-thumbnail-img-id="0*25468db0247fa9b94faff917a2fa024394c0272f.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/304/160/0*25468db0247fa9b94faff917a2fa024394c0272f.jpeg);"></a></div><p name="1bdc" id="1bdc" class="graf graf--p graf-after--mixtapeEmbed graf--trailing"><strong class="markup--strong markup--p-strong">My website</strong>: <a href="https://www.shawhintalebi.com/" data-href="https://www.shawhintalebi.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">https://www.shawhintalebi.com/</a></p></div></div></section><section name="3ad2" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="7e5e" id="7e5e" class="graf graf--p graf--leading">[1] <a href="https://arxiv.org/abs/2001.08361" data-href="https://arxiv.org/abs/2001.08361" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Scaling Laws for Neural Language Models</a></p><p name="7bf1" id="7bf1" class="graf graf--p graf-after--p">[2] <a href="https://arxiv.org/abs/1710.09282" data-href="https://arxiv.org/abs/1710.09282" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">A Survey of Model Compression and Acceleration for Deep Neural Networks</a></p><p name="42e2" id="42e2" class="graf graf--p graf-after--p">[3] <a href="https://machinelearning.apple.com/research/model-compression-in-practice" data-href="https://machinelearning.apple.com/research/model-compression-in-practice" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Model Compression in Practice: Lessons Learned from Practitioners Creating On-device Machine Learning Experiences</a></p><p name="28fc" id="28fc" class="graf graf--p graf-after--p">[4] <a href="https://arxiv.org/abs/1710.09282" data-href="https://arxiv.org/abs/1710.09282" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Model Compression for Deep Neural Networks: A Survey</a></p><p name="0e41" id="0e41" class="graf graf--p graf-after--p">[5] <a href="https://arxiv.org/abs/2308.07633" data-href="https://arxiv.org/abs/2308.07633" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">A Survey on Model Compression for Large Language Models</a></p><p name="3664" id="3664" class="graf graf--p graf-after--p">[6] <a href="https://arxiv.org/abs/2402.17764" data-href="https://arxiv.org/abs/2402.17764" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a></p><p name="f5cb" id="f5cb" class="graf graf--p graf-after--p">[7] <a href="https://arxiv.org/abs/1710.01878" data-href="https://arxiv.org/abs/1710.01878" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">To prune, or not to prune: exploring the efficacy of pruning for model compression</a></p><p name="8c75" id="8c75" class="graf graf--p graf-after--p">[8] <a href="https://arxiv.org/abs/1503.02531" data-href="https://arxiv.org/abs/1503.02531" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Distilling the Knowledge in a Neural Network</a></p><p name="4c3d" id="4c3d" class="graf graf--p graf-after--p">[9] <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" data-href="https://crfm.stanford.edu/2023/03/13/alpaca.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Alpaca: A Strong, Replicable Instruction-Following Model</a></p><p name="33fb" id="33fb" class="graf graf--p graf-after--p graf--trailing">[10] <a href="https://arxiv.org/abs/2305.14314" data-href="https://arxiv.org/abs/2305.14314" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">QLoRA: Efficient Finetuning of Quantized LLMs</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@shawhin" class="p-author h-card">Shaw Talebi</a> on <a href="https://medium.com/p/9f406eea5b5e"><time class="dt-published" datetime="2024-08-30T05:09:49.243Z">August 30, 2024</time></a>.</p><p><a href="https://medium.com/@shawhin/compressing-large-language-models-llms-9f406eea5b5e" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 2, 2024.</p></footer></article></body></html>