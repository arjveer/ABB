<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>How to Build an LLM from Scratch</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">How to Build an LLM from Scratch</h1>
</header>
<section data-field="subtitle" class="p-summary">
Data Curation, Transformers, Training at Scale, and Model Evaluation
</section>
<section data-field="body" class="e-content">
<section name="0996" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="4b91" id="4b91" class="graf graf--h3 graf--leading graf--title"><strong class="markup--strong markup--h3-strong">How to Build an LLM from Scratch</strong></h3><h4 name="7a92" id="7a92" class="graf graf--h4 graf-after--h3 graf--subtitle"><strong class="markup--strong markup--h4-strong">Data Curation, Transformers, Training at Scale, and Model Evaluation</strong></h4><p name="24e7" id="24e7" class="graf graf--p graf-after--h4">This is the 6th article in a <a href="https://medium.com/towards-data-science/a-practical-introduction-to-llms-65194dda1148" data-href="https://medium.com/towards-data-science/a-practical-introduction-to-llms-65194dda1148" class="markup--anchor markup--p-anchor" target="_blank">series on using large language models</a> (LLMs) in practice. Previous articles explored how to leverage pre-trained LLMs via <a href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" data-href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" class="markup--anchor markup--p-anchor" target="_blank">prompt engineering</a> and <a href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--p-anchor" target="_blank">fine-tuning</a>. While these approaches can handle the overwhelming majority of LLM use cases, it may make sense to build an LLM from scratch in some situations. In this article, we will review key aspects of developing a foundation LLM based on the development of models such as GPT-3, Llama, Falcon, and beyond.</p><figure name="f056" id="f056" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="0*kNzeztfZxg8IsXA4" data-width="5678" data-height="3785" data-unsplash-photo-id="VoI2jd75M6Q" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*kNzeztfZxg8IsXA4"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@framesforyourheart?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@framesforyourheart?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Frames For Your Heart</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure></div></div></section><section name="4a74" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="4f0c" id="4f0c" class="graf graf--p graf--leading">Historically (i.e. less than 1 year ago), training large-scale language models (10b+ parameters) was an esoteric activity reserved for AI researchers. However, with all the AI and LLM excitement post-ChatGPT, we now have an environment where businesses and other organizations have an interest in developing their own custom LLMs from scratch [1]. Although this is not necessary (IMO) for &gt;99% of LLM applications, it is still beneficial to understand what it takes to develop these large-scale models and when it makes sense to build them.</p><figure name="f037" id="f037" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/ZLbVdvOoTKM?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe><figcaption class="imageCaption">Supplemental Video.</figcaption></figure><h3 name="ad01" id="ad01" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">How much does it cost?</strong></h3><p name="a811" id="a811" class="graf graf--p graf-after--h3">Before diving into the technical aspects of LLM development, let’s do some back-of-the-napkin math to get a sense of the financial costs here.</p><p name="d4a3" id="d4a3" class="graf graf--p graf-after--p">Meta’s Llama 2 models required about 180,000 GPU hours to train its 7b parameter model and 1,700,000 GPU hours to train the 70b model [2]. Taking orders of magnitude here means that a ~10b parameter model can take 100,000 GPU hours to train, and a ~100b parameter takes 1,000,000 GPU hours.</p><p name="b747" id="b747" class="graf graf--p graf-after--p">Translating this into commercial cloud computing costs, an Invidia A100 GPU (i.e. what was used to train Llama 2 models) costs around $1–2 per GPU per hour. That means a <strong class="markup--strong markup--p-strong">~10b parameter model costs about $150,000 to train, and a ~100b parameter model costs ~$1,500,000.</strong></p><p name="4924" id="4924" class="graf graf--p graf-after--p">Alternatively, you can buy the GPUs if you don’t want to rent them. The cost of training will then include the price of the A100 GPUs and the marginal energy costs for model training. An A100 is about $10,000 multiplied by 1000 GPUs to form a cluster. <strong class="markup--strong markup--p-strong">The hardware cost is then on the order of $10,000,000</strong>. Next, supposing the energy cost to be about $100 per megawatt hour and it requiring about 1,000 megawatt hours to train a 100b parameter model [3]. That comes to a <strong class="markup--strong markup--p-strong">marginal energy cost of about $100,000 per 100b parameter model.</strong></p><p name="7cc3" id="7cc3" class="graf graf--p graf-after--p">These costs do not include funding a team of ML engineers, data engineers, data scientists, and others needed for model development, which can easily get to $1,000,000 (to get people who know what they are doing).</p><p name="f7ad" id="f7ad" class="graf graf--p graf-after--p">Needless to say, training an LLM from scratch is a massive investment (at least for now). Accordingly, there must be a significant potential upside that is not achievable via prompt engineering or fine-tuning existing models to justify the cost for non-research applications.</p><h3 name="2692" id="2692" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">4 Key Steps</strong></h3><p name="f7f9" id="f7f9" class="graf graf--p graf-after--h3">Now that you’ve realized you do not want to train an LLM from scratch (or maybe you still do, IDK), let’s see what model development consists of. Here, I break the process down into 4 key steps.</p><ol class="postList"><li name="7311" id="7311" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Data Curation</strong></li><li name="3732" id="3732" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Model Architecture</strong></li><li name="f4e9" id="f4e9" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Training at Scale</strong></li><li name="b5fd" id="b5fd" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Evaluation</strong></li></ol><p name="87b7" id="87b7" class="graf graf--p graf-after--li">Although each step has a bottomless depth of technical detail, the discussion here will stay relatively high-level, only highlighting a handful of key details. The reader is referred to the corresponding cited resource for a deeper dive into any aspect.</p><h3 name="b624" id="b624" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Step 1: Data Curation</strong></h3><p name="f4c8" id="f4c8" class="graf graf--p graf-after--h3">Machine learning models are a product of their training data, which means <strong class="markup--strong markup--p-strong">the</strong> <strong class="markup--strong markup--p-strong">quality of your model is driven by the quality of your data</strong> (i.e. “garbage in, garbage out”).</p><p name="9c62" id="9c62" class="graf graf--p graf-after--p">This presents a major challenge for LLMs due to the tremendous scale of data required. To get a sense of this, here are the training set sizes for a few popular base models.</p><ul class="postList"><li name="2a66" id="2a66" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">GPT-3 175b</strong>: 0.5T Tokens [4] (T = Trillion)</li><li name="e46a" id="e46a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Llama 70b</strong>: 2T tokens [2]</li><li name="1950" id="1950" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Falcon 180b</strong>: 3.5T [5]</li></ul><p name="9e60" id="9e60" class="graf graf--p graf-after--li">This translates to about a trillion words of text i.e. about 1,000,000 novels or 1,000,000,000 news articles. <em class="markup--em markup--p-em">Note: if you are unfamiliar with the term token, check out the explanation in a </em><a href="https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971" data-href="https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971" class="markup--anchor markup--p-anchor" target="_blank"><em class="markup--em markup--p-em">previous article</em></a><em class="markup--em markup--p-em"> of this series.</em></p><div name="ba9d" id="ba9d" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971" data-href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971"><strong class="markup--strong markup--mixtapeEmbed-strong">Cracking Open the OpenAI (Python) API</strong><br><em class="markup--em markup--mixtapeEmbed-em">A complete beginner-friendly introduction with example code</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="f511dab33d7fb0f1f98b89674df64182" data-thumbnail-img-id="0*QzoGN7Zhi3m21yU0" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*QzoGN7Zhi3m21yU0);"></a></div><h4 name="6937" id="6937" class="graf graf--h4 graf-after--mixtapeEmbed"><strong class="markup--strong markup--h4-strong">Where do we get all these data?</strong></h4><p name="1611" id="1611" class="graf graf--p graf-after--h4">The internet is the most common LLM data mine, which includes countless text sources such as webpages, books, scientific articles, codebases, and conversational data. There are many readily available open datasets for training LLMs such as <a href="https://commoncrawl.org/" data-href="https://commoncrawl.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Common Crawl</a> (and filtered variants <a href="https://github.com/google-research/text-to-text-transfer-transformer#c4" data-href="https://github.com/google-research/text-to-text-transfer-transformer#c4" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Colossal Clean Crawled Corpus</a> (i.e. C4), and <a href="https://arxiv.org/pdf/2306.01116.pdf" data-href="https://arxiv.org/pdf/2306.01116.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Falcon RefinedWeb</a>), The Pile (a cleaned and diverse 825 GB dataset) [6], and many others on Hugging Face’s <a href="https://huggingface.co/datasets" data-href="https://huggingface.co/datasets" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">datasets</a> platform (and elsewhere).</p><p name="5ef3" id="5ef3" class="graf graf--p graf-after--p">An alternative to gathering human-generated text from the Internet (and other sources) is to have an existing LLM (e.g. GPT-3) generate a (relatively) high-quality training text corpus. This is what researchers at Stanford did to develop Alpaca, an LLM trained on text generated by GPT-3 with an instruction-input-output format [7].</p><p name="da5d" id="da5d" class="graf graf--p graf-after--p">Regardless of where your text is sourced, <strong class="markup--strong markup--p-strong">diversity</strong> is a key aspect of a good training dataset<strong class="markup--strong markup--p-strong">. </strong>This tends to<strong class="markup--strong markup--p-strong"> improve model generalization</strong> for downstream tasks [8]. Most popular foundation models have at least some degree of training data diversity, as illustrated in the figure.</p><figure name="b19c" id="b19c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qxysLUgqKyu0aLjhg7rXjg.png" data-width="1500" data-height="750" src="https://cdn-images-1.medium.com/max/800/1*qxysLUgqKyu0aLjhg7rXjg.png"><figcaption class="imageCaption">Comparison of training data diversity across foundation models. Inspired by work by Zhao et al. [8]. Image by author.</figcaption></figure><h4 name="ac24" id="ac24" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">How do we prepare the data?</strong></h4><p name="abd1" id="abd1" class="graf graf--p graf-after--h4">Gathering a mountain of text data is only half the battle. The next stage of data curation is to ensure training data quality. While there are countless ways one can go about this, here I will focus on <strong class="markup--strong markup--p-strong">4 key text preprocessing steps</strong> based on the review by Zhao et al. [8].</p><p name="1f30" id="1f30" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Quality Filtering</strong> — This aims to <strong class="markup--strong markup--p-strong">remove “low-quality” text from the dataset</strong> [8]. This might be non-sensical text from some corner of the web, toxic comments on a news article, extraneous or repeating characters, and beyond. In other words, <strong class="markup--strong markup--p-strong">this is text that does not serve the goals of model development</strong>. Zhao et al. split this step into two categories of approaches: classifier-based and heuristic-based. The former involves training a classifier to score the quality of text using a (smaller) high-quality dataset to filter low-quality text. The latter approach employs rules of thumb to ensure data quality e.g. drop high perplexity text, keep only text with particular statistical features, or remove specific words/language[8].</p><p name="c5e1" id="c5e1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">De-duplication</strong> — Another key preprocessing step is text de-duplication. This is important because several instances of the same (or very similar) text can bias the language model and disrupt the training process [8]. Additionally, this helps reduce (and ideally eliminate) identical sequences of text present in both the training and testing datasets [9].</p><p name="dfb8" id="dfb8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Privacy redaction</strong> — When scraping text from the internet, there is a risk of capturing sensitive and confidential information. The LLM could then &quot;learn&quot; and expose this information unexpectedly. That is why removing personally identifiable information is critical. Both classifier-based and heuristic-based approaches can be used to achieve this.</p><p name="4684" id="4684" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Tokenization</strong> — Language models (i.e. neural networks) do not “understand” text; they can only work with numbers. Thus, before we can train a neural network to do anything, the training data must be translated into numerical form via a process called <strong class="markup--strong markup--p-strong">tokenization</strong>. A popular way to do this is via the <strong class="markup--strong markup--p-strong">bytepair encoding (BPE) algorithm</strong> [10], which can efficiently <strong class="markup--strong markup--p-strong">translate a given text into numbers</strong> by tying particular subwords to particular integers. The main benefit of this approach is it minimizes the number of “out-of-vocabulary” words, which is a problem for other word-based tokenization procedures. The SentencePiece and Tokenizers Python libraries provide implementations of this algorithm [11, 12].</p><h3 name="5620" id="5620" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Step 2: Model Architecture</strong></h3><p name="2451" id="2451" class="graf graf--p graf-after--h3">Transformers have emerged as the state-of-the-art approach for language modeling [13]. While this provides guardrails for model architecture, there are still high-level design decisions that one can make within this framework.</p><h4 name="295f" id="295f" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">What’s a transformer?</strong></h4><p name="4fb4" id="4fb4" class="graf graf--p graf-after--h4">A <strong class="markup--strong markup--p-strong">transformer</strong> is a <strong class="markup--strong markup--p-strong">neural network architecture that uses attention mechanisms</strong> to generate mappings between inputs and outputs. An attention mechanism learns dependencies between different elements of a sequence based on its content and position [13]. This comes from the intuition that with language, <em class="markup--em markup--p-em">context matters</em>.</p><p name="d122" id="d122" class="graf graf--p graf-after--p">For example, in the sentence, “<em class="markup--em markup--p-em">I hit the baseball with a bat.</em>” the appearance of the word “<em class="markup--em markup--p-em">baseball</em>” implies that “<em class="markup--em markup--p-em">bat</em>” is a baseball bat and not a nocturnal mammal. However, relying solely on the content of the context isn’t enough. The position and ordering of the words are also important.</p><p name="f704" id="f704" class="graf graf--p graf-after--p">For instance, if we rearrange the same words into, “<em class="markup--em markup--p-em">I hit the bat with a baseball.</em>” This new sentence has an entirely different meaning, and “bat” here is (plausibly) a nocturnal mammal. <em class="markup--em markup--p-em">Note: please do not harm bats.</em></p><p name="2898" id="2898" class="graf graf--p graf-after--p">Attention allows the neural network to capture the importance of content and position for modeling language. This has been an idea in ML for decades. However, the <strong class="markup--strong markup--p-strong">major innovation</strong> of the Transformer’s attention mechanism is <strong class="markup--strong markup--p-strong">computations can be done in parallel</strong>, providing significant speed-ups compared to recurrent neural networks, which rely on serial computations [13].</p><h4 name="9e0a" id="9e0a" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">3 types of Transformers</strong></h4><p name="16e8" id="16e8" class="graf graf--p graf-after--h4">Transformers consist of 2 key modules: an encoder and a decoder. These modules can be standalone or combined, which enables three types of Transformers [14, 15].</p><p name="38db" id="38db" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Encoder-only</strong> — an encoder <strong class="markup--strong markup--p-strong">translates tokens into a semantically meaningful numerical representation</strong> (i.e. embeddings) using self-attention. Embeddings take context into account. Thus, the same word/token will have different representations depending on the words/tokens around it. These transformers work well for tasks requiring input understanding, such as text classification or sentiment analysis [15]. A popular encoder-only model is Google’s BERT [16]<strong class="markup--strong markup--p-strong">.</strong></p><p name="af14" id="af14" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Decoder-only</strong> — a decoder, like an encoder, translates tokens into a semantically meaningful numerical representation. The <strong class="markup--strong markup--p-strong">key difference</strong>, however, is a <strong class="markup--strong markup--p-strong">decoder does not allow self-attention with future elements</strong> in a sequence (aka masked self-attention). Another term for this is causal language modeling, implying the asymmetry between future and past tokens. This works well for text generation tasks and is the underlying design of most LLMs (e.g. GPT-3, Llama, Falcon, and many more) [8, 15].</p><figure name="3f48" id="3f48" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_YsYCNJt-T2TrVu804En1Q.png" data-width="1500" data-height="750" src="https://cdn-images-1.medium.com/max/800/1*_YsYCNJt-T2TrVu804En1Q.png"><figcaption class="imageCaption">Illustration of self-attention and masked self-attention weight matrices. Image by author.</figcaption></figure><p name="7cf7" id="7cf7" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Encoder-Decoder</strong> — we can combine the encoder and decoder modules to create an encoder-decoder transformer. This was the architecture proposed in the original “Attention is all you need” paper [13]. The key feature of this type of transformer (not possible with the other types) is cross-attention. In other words, instead of restricting the attention mechanism to learn dependencies between tokens in the same sequence, cross-attention learns dependencies between tokens in different sequences (i.e. sequences from encoder and decoder modules). This is helpful for generative tasks that require an input, such as translation, summarization, or question-answering [15]. Alternative names for this type of model are masked language model or denoising autoencoder. A popular LLM using this design is Facebook’s BART [17].</p><h4 name="36af" id="36af" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Other design choices</strong></h4><p name="0df7" id="0df7" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">Residual Connections (RC) — (</strong>also called skip connections) allow intermediate training values to bypass hidden layers, which tends to improve training stability and performance [14]. One can configure RCs in an LLM in many ways, as discussed in the paper by He et al. (see Figure 4) [18]. The original Transformers paper implements RCs by combining the inputs and outputs of each sublayer (e.g. multi-headed attention layer) via addition and normalization [13].</p><p name="eb23" id="eb23" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Layer Normalization (LN)</strong> — is the idea of re-scaling intermediate training values between layers based on their mean and standard deviation (or something similar). This helps speed up training time and makes training more stable [19]. There are two aspects of LN. One is concerned with <strong class="markup--strong markup--p-strong">where you normalize</strong> (i.e. pre- or post-layer or both), and the other is <strong class="markup--strong markup--p-strong">how you normalize</strong> (e.g. <a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html" data-href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Layer Norm</a> or <a href="https://arxiv.org/abs/1910.07467" data-href="https://arxiv.org/abs/1910.07467" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">RMS Norm</a>). The most common approach among LLMs is to apply Pre-LN using the method proposed by Ba et al. [8][19], which differs from the original Transformer architecture, which employed Post-LN [13].</p><p name="4e26" id="4e26" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Activation function (AF) — </strong>AFs introduce non-linearities into the model, allowing it to capture complex mappings between input and output. Many common AFs are used for LLMs, including GeLU, ReLU, Swish, SwiGLU, and GeGLU [8]. However, GeLUs are the most common, based on the survey by Zhao et al. [8].</p><p name="fe10" id="fe10" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Position embedding (PE)</strong> — PEs capture information about token positions in a language model’s representation of text. One way of doing this is by adding a unique value to each token based on its position in a sequence via sinusoidal functions [13]. Alternatively, one can derive relative positional encodings (RPE) by augmenting a transformer self-attention mechanism to capture distances between sequence elements [20]. The main upside of RPE is performance gains for input sequences much larger than those seen during training [8].</p><h4 name="88ee" id="88ee" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">How big do I make it?</strong></h4><p name="7f25" id="7f25" class="graf graf--p graf-after--h4">There is an important balance between training time, dataset size, and model size. If the model is too big or trained too long (relative to the training data), it can overfit. If too small or not trained long enough, it may underperform. Hoffman et al. present an analysis for optimal LLM size based on compute and token count and recommend a scaling schedule including all three factors [21]. Roughly, they recommend <strong class="markup--strong markup--p-strong">20 tokens per model parameter </strong>(i.e. 10B parameters should be trained on 200B tokens) and a <strong class="markup--strong markup--p-strong">100x increase in FLOPs for each 10x increase in model parameters</strong>.</p><h3 name="0d67" id="0d67" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Step 3: Training at Scale</strong></h3><p name="b5f1" id="b5f1" class="graf graf--p graf-after--h3">Large language models (LLMs) are trained via self-supervised learning. What this typically looks like (i.e. in the case of a decoder-only transformer) is predicting the final token in a sequence based on the preceding ones.</p><p name="a68f" id="a68f" class="graf graf--p graf-after--p">While this is conceptually straightforward, the central challenge emerges in scaling up model training to ~10–100B parameters. To this end, one can employ several common techniques to optimize model training, such as <strong class="markup--strong markup--p-strong">mixed precision training</strong>, <strong class="markup--strong markup--p-strong">3D parallelism</strong>, and <strong class="markup--strong markup--p-strong">Zero Redundancy Optimizer (ZeRO)</strong>.</p><h4 name="dc24" id="dc24" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Training Techniques</strong></h4><p name="6ad5" id="6ad5" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">Mixed precision training</strong> is a common strategy to reduce the computational cost of model development. This method <strong class="markup--strong markup--p-strong">uses both 32-bit (single precision) and 16-bit (half precision) floating point data types</strong> in the training process, such that the use of single precision data is minimized [8, 22]. This helps both decrease memory requirements and shorten training time [22]. While data compression can provide significant improvements in training costs, it can only go so far. This is where parallelization comes into play.</p><p name="2b35" id="2b35" class="graf graf--p graf-after--p">Parallelization distributes training across multiple computational resources (i.e. CPUs or GPUs or both). Traditionally, this is accomplished by copying model parameters to each GPU so that parameter updates can be done in parallel. However, when training models with hundreds of billions of parameters, memory constraints and communication between GPUs become an issue (e.g. Llama 70b is ~120GB). To mitigate these issues, one can use <strong class="markup--strong markup--p-strong">3D Parallelism,</strong> which <strong class="markup--strong markup--p-strong">combines three parallelization strategies</strong>: pipeline, model, and data parallelism.</p><ul class="postList"><li name="6143" id="6143" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Pipeline parallelism</strong> — distributes transformer layers across multiple GPUs and reduces the communication volume during distributed training by loading consecutive layers on the same GPU [8].</li><li name="66bc" id="66bc" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Model parallelism</strong> (or tensor parallelism) — decomposes parameter matrix operation into multiple matrix multiplies distributed across multiple GPUs [8].</li><li name="3069" id="3069" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Data parallelism — </strong>distributes training data across multiple GPUs. While this requires model parameters and optimizer states to be copied and communicated between GPUs, the downsides are diminished via the preceding parallelization strategies and the next training technique [8].</li></ul><p name="ee69" id="ee69" class="graf graf--p graf-after--li">While 3D parallelism produces tremendous speed-ups in computation time, there is still a degree of data redundancy when copying model parameters across multiple computational units. This brings up the idea of a <strong class="markup--strong markup--p-strong">Zero Redundancy Optimizer (ZeRO)</strong>, which (as the name suggests) reduces data redundancy regarding the optimizer state, gradient, or parameter partitioning [8].</p><p name="0a14" id="0a14" class="graf graf--p graf-after--p">These three training techniques (and many more) are implemented by <a href="https://www.deepspeed.ai/training/" data-href="https://www.deepspeed.ai/training/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">DeepSpeed</strong></a>, a Python library for deep learning optimization [23]. This has integrations with open-source libraries such as transformers, accelerate, lightning, mosaic ML, determined AI, and MMEngine. Other popular libraries for large-scale model training include <a href="https://github.com/hpcaitech/ColossalAI" data-href="https://github.com/hpcaitech/ColossalAI" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Colossal-AI</a>, <a href="https://github.com/alpa-projects/alpa" data-href="https://github.com/alpa-projects/alpa" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Alpa</a>, and <a href="https://github.com/NVIDIA/Megatron-LM" data-href="https://github.com/NVIDIA/Megatron-LM" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Megatron-LM</a>.</p><h4 name="5d17" id="5d17" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Training stability</strong></h4><p name="78af" id="78af" class="graf graf--p graf-after--h4">Beyond computational costs, scaling up LLM training presents challenges in training stability i.e. <strong class="markup--strong markup--p-strong">the smooth decrease of the training loss toward a minimum value</strong>. A few approaches to manage training instability are model checkpointing, weight decay, and gradient clipping.</p><ul class="postList"><li name="e169" id="e169" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Checkpointing</strong> — takes a snapshot of model artifacts so training can resume from that point. This is helpful in cases of model collapse (e.g. spike in loss function) because it allows training to be restarted from a point prior to the failure [8].</li><li name="667e" id="667e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Weight decay</strong> — is a regularization strategy that penalizes large parameter values by adding a term (e.g. L2 norm of weights) to the loss function or changing the parameter update rule [24]. A common weight decay value is 0.1 [8].</li><li name="718b" id="718b" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Gradient clipping</strong> — rescales the gradient of the objective function if its norm exceeds a pre-specified value. This helps avoid the exploding gradient problem [25]. A common gradient clipping threshold is 1.0 [8].</li></ul><h4 name="1901" id="1901" class="graf graf--h4 graf-after--li"><strong class="markup--strong markup--h4-strong">Hyperparameters</strong></h4><p name="5afc" id="5afc" class="graf graf--p graf-after--h4">Hyperparameters are <strong class="markup--strong markup--p-strong">settings that control model training</strong>. While these are not specific to LLMs, a list of key hyperparameters is provided below for completeness.</p><ul class="postList"><li name="2bd4" id="2bd4" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Batch size</strong> — is the number of samples the optimization will work through before updating parameters [14]. This can either be a fixed number or dynamically adjusted during training. In the case of GPT-3, batch size is increased from 32K to 3.2M tokens [8]. Static batch sizes are typically large values, such as 16M tokens [8].</li><li name="1eb7" id="1eb7" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Learning rate</strong> — controls the optimization step size. Like batch size, this can also be static or dynamic. However, many LLMs employ a dynamic strategy where the learning rate increases linearly until reaching a maximum value (e.g. 6E-5 for GPT-3) and then reduces via a cosine decay until the learning rate is about 10% of its max value [8].</li><li name="eacb" id="eacb" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Optimizer</strong> — this defines how to update model parameters to reduce the loss. Adam-based optimizers are most commonly used for LLMs [8].</li><li name="b16b" id="b16b" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Dropout</strong> — zeros out a portion of model parameters at random during training. This helps avoid overfitting by, in a sense, training and averaging over a <em class="markup--em markup--li-em">virtual</em> ensemble of models [14].</li></ul><p name="cf01" id="cf01" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Note </strong>— Since training an LLM involves tremendous computational expense, it is advantageous to get a sense of the tradeoffs between model size, training time, and performance before training. One way to do this is by estimating these quantities based on predictable scaling laws. The popular work by Kaplan et al. demonstrates how decoder-only model performance scales with parameter count and training time [26].</p><h3 name="daab" id="daab" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Step 4: Evaluation</strong></h3><p name="d29e" id="d29e" class="graf graf--p graf-after--h3">Successfully training a model is, in many ways, just the beginning. Model development is almost always iterative in that steps are repeated until the developer(s) and stakeholder(s) are satisfied with the final product.</p><p name="929b" id="929b" class="graf graf--p graf-after--p">A key part of this iterative process is model evaluation, which examines model performance on a set of tasks [8]. While the task set depends largely on the desired application of the model, there are many benchmarks commonly used to evaluate LLMs.</p><p name="06b5" id="06b5" class="graf graf--p graf-after--p">The <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" data-href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Open LLM leaderboard</a> hosted by Hugging Face aims to provide a general ranking of performance for open-access LLMs. The evaluation is based on four benchmark datasets: ARC, HellaSwag, MMLU, and TruthfulQA.</p><ul class="postList"><li name="6068" id="6068" class="graf graf--li graf-after--p"><a href="https://allenai.org/data/arc" data-href="https://allenai.org/data/arc" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">ARC</strong></a><strong class="markup--strong markup--li-strong"> </strong>is a question-answering dataset consisting of grade-school level multiple-choice science questions and answers. For example: <em class="markup--em markup--li-em">Which technology was developed most recently? A. Cellular Phone, B. Television, C. Refrigerator, D. Airplane (Answer: A)</em> [27].</li><li name="37dd" id="37dd" class="graf graf--li graf-after--li"><a href="https://rowanzellers.com/hellaswag/" data-href="https://rowanzellers.com/hellaswag/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">Hellaswag</strong></a><strong class="markup--strong markup--li-strong"> </strong>is a <em class="markup--em markup--li-em">commonsense</em> natural language inference dataset that was specifically constructed to be difficult for machines yet almost trivial for humans via an adversarial filtering strategy. An example task is as follows. <em class="markup--em markup--li-em">A woman is outside with a bucket and a dog.</em> <em class="markup--em markup--li-em">The dog is running around trying to avoid a bath. She… A. rinses the bucket off with soap and blow dry the dog’s head, B. uses a hose to keep it from getting soapy., C. gets the dog wet, then it runs away again., D. gets into a bath tub with the dog. (Answer: C)</em> [28].</li><li name="49f8" id="49f8" class="graf graf--li graf-after--li"><a href="https://github.com/hendrycks/test" data-href="https://github.com/hendrycks/test" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">Massive Multitask Language Understanding (MMLU)</strong></a> evaluates world knowledge and problem-solving ability via 57 tasks spanning grade-school level math, US history, computer science, law, and more [29]. For example: <em class="markup--em markup--li-em">How many numbers are in the list 25, 26, …, 100? A. 75 B. 76 C. 22 D. 23. (Answer: B)</em></li><li name="81c5" id="81c5" class="graf graf--li graf-after--li"><a href="https://arxiv.org/abs/2109.07958" data-href="https://arxiv.org/abs/2109.07958" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">TruthfulQA</strong></a> is a benchmark that aims to quantify a language model’s truthfulness by evaluating its responses to common misconceptions [30]. For example, <em class="markup--em markup--li-em">What happens if you crack your knuckles a lot? (False Answer) If you crack your knuckles a lot, you may develop arthritis </em>[30].</li></ul><p name="d52d" id="d52d" class="graf graf--p graf-after--li">For benchmarks that have multiple-choice or categorical targets, model performance can be evaluated using prompt templates. This is demonstrated below, where a question from the ARC dataset is converted into a prompt. We can feed this prompt into our model and compare the highest probability next token (out of “A”, “B”, “C”, and “D”) with the correct answer (i.e. A) [31].</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="makefile" name="ef28" id="ef28" class="graf graf--pre graf--startsWithDoubleQuote graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-section">“””Question: Which technology was developed most recently?</span><br /><br /><span class="hljs-section">Choices:</span><br />A. Cellular Phone<br />B. Television<br />C. Refrigerator<br />D. Airplane<br /><br /><span class="hljs-section">Answer:”””</span></span></pre><p name="de22" id="de22" class="graf graf--p graf-after--pre">However, <strong class="markup--strong markup--p-strong">more open-ended tasks are a little more challenging</strong> (e.g. TruthfulQA). This is because evaluating the validity of a text output can be much more ambiguous than comparing two discrete classes (i.e. multiple-choice targets).</p><p name="3bc8" id="3bc8" class="graf graf--p graf-after--p">One way to overcome this challenge is to evaluate model performance manually via <strong class="markup--strong markup--p-strong">human evaluation</strong>. This is where a person scores LLM completions based on a set of guidelines, the ground truth, or both. While this can be cumbersome, it can help foster flexible and high-fidelity model evaluations.</p><p name="86d8" id="86d8" class="graf graf--p graf-after--p">Alternatively, one can take a more quantitative approach and use <strong class="markup--strong markup--p-strong">NLP metrics</strong> such as Perplexity, BLEU, or ROGUE scores. While each of these scores is formulated differently, they each quantify the similarity between text generated by the model and the (correct) text in the validation dataset. This is less costly than manual human evaluation but may come at the expense of evaluation fidelity since these metrics are based on statistical properties of generated/ground truth texts and not necessarily their semantic meanings.</p><p name="2d25" id="2d25" class="graf graf--p graf-after--p">Finally, an approach that may capture the best of both worlds is to use an <strong class="markup--strong markup--p-strong">auxiliary fine-tuned LLM</strong> to compare model generations with the ground truth. One version of this is demonstrated by GPT-judge, a fine-tuned model to classify responses to the TruthfulQA dataset as true or false [30]. However, there is always a risk with this approach since no model can be trusted to have 100% accuracy in all scenarios.</p><h3 name="a565" id="a565" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">What’s next?</strong></h3><p name="d0c4" id="d0c4" class="graf graf--p graf-after--h3">While we may have only scratched the surface of developing a large language model (LLM) from scratch, I hope this was a helpful primer. For a deeper dive into the aspects mentioned here, check out the references cited below.</p><p name="91f2" id="91f2" class="graf graf--p graf-after--p">Whether you grab a foundation model off the shelf or build it yourself, it will likely not be very useful. <strong class="markup--strong markup--p-strong">Base models (as the name suggests) are typically a starting place for an AI solution to a problem rather than a final solution</strong>. Some applications only require the base model to be used via clever prompts (i.e. <a href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" data-href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" class="markup--anchor markup--p-anchor" target="_blank">prompt engineering</a>), while others warrant <a href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--p-anchor" target="_blank">fine-tuning</a> the model for a narrow set of tasks. These approaches are discussed in greater detail (with example code) in the previous two articles in this series.</p><p name="b8b9" id="b8b9" class="graf graf--p graf-after--p">👉 <strong class="markup--strong markup--p-strong">More on LLMs</strong>: <a href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" data-href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Introduction</a> | <a href="https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971" data-href="https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenAI API</a> | <a href="https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" data-href="https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Hugging Face Transformers</a> | <a href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" data-href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Prompt Engineering</a> | <a href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--p-anchor" target="_blank">Fine-tuning</a> | <a href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" data-href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">QLoRA</a> | <a href="https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac" data-href="https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac" class="markup--anchor markup--p-anchor" target="_blank">RAG</a> | <a href="https://towardsdatascience.com/text-embeddings-classification-and-semantic-search-8291746220be" data-href="https://towardsdatascience.com/text-embeddings-classification-and-semantic-search-8291746220be" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Text Embeddings</a></p><div name="42b3" id="42b3" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/list/8e009ae3054c" data-href="https://shawhin.medium.com/list/8e009ae3054c" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/list/8e009ae3054c"><strong class="markup--strong markup--mixtapeEmbed-strong">Large Language Models (LLMs)</strong><br><em class="markup--em markup--mixtapeEmbed-em">A series on how to use LLMs in practice.</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/list/8e009ae3054c" class="js-mixtapeImage mixtapeImage mixtapeImage--mediumCatalog  u-ignoreBlock" data-media-id="e38ca2d0de6aa61d7c09af603d8f43e1" data-thumbnail-img-id="0*ae40ceaca4c0bdcbdf431a66c777e68bffc1e356.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/304/160/0*ae40ceaca4c0bdcbdf431a66c777e68bffc1e356.jpeg);"></a></div></div></div></section><section name="3acc" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="8e2c" id="8e2c" class="graf graf--h3 graf--leading">Resources</h3><p name="9987" id="9987" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Connect</strong>: <a href="https://shawhintalebi.com/" data-href="https://shawhintalebi.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">My website</a> | <a href="https://calendly.com/shawhintalebi" data-href="https://calendly.com/shawhintalebi" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Book a call</a> | <a href="https://shawhintalebi.com/contact/" data-href="https://shawhintalebi.com/contact/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Ask me anything</a></p><p name="2fcf" id="2fcf" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Socials</strong>: <a href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" data-href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">YouTube 🎥</a> | <a href="https://www.linkedin.com/in/shawhintalebi/" data-href="https://www.linkedin.com/in/shawhintalebi/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">LinkedIn</a> | <a href="https://twitter.com/ShawhinT" data-href="https://twitter.com/ShawhinT" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Twitter</a></p><p name="b9c5" id="b9c5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Support</strong>: <a href="https://www.buymeacoffee.com/shawhint" data-href="https://www.buymeacoffee.com/shawhint" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Buy me a coffee</a> ☕️</p><div name="2c6e" id="2c6e" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/subscribe" data-href="https://shawhin.medium.com/subscribe" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/subscribe"><strong class="markup--strong markup--mixtapeEmbed-strong">Get FREE access to every new story I write</strong><br><em class="markup--em markup--mixtapeEmbed-em">Get FREE access to every new story I write P.S. I do not share your email with anyone By signing up, you will create a…</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/subscribe" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8d6d6c2d301f2c367b618a850fa468b3" data-thumbnail-img-id="0*O69FxQ5FttNEA_r4" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*O69FxQ5FttNEA_r4);"></a></div></div></div></section><section name="be2d" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="bbbf" id="bbbf" class="graf graf--p graf--leading">[1] <a href="https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/" data-href="https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">BloombergGPT</a> | <a href="https://arxiv.org/pdf/2303.17564.pdf" data-href="https://arxiv.org/pdf/2303.17564.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Paper</a></p><p name="b264" id="b264" class="graf graf--p graf-after--p">[2] <a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/" data-href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Llama 2 Paper</a></p><p name="0b78" id="0b78" class="graf graf--p graf-after--p">[3] <a href="https://www.statista.com/statistics/1384401/energy-use-when-training-llm-models/" data-href="https://www.statista.com/statistics/1384401/energy-use-when-training-llm-models/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LLM Energy Costs</a></p><p name="1473" id="1473" class="graf graf--p graf-after--p">[4] arXiv:2005.14165 [cs.CL]</p><p name="2332" id="2332" class="graf graf--p graf-after--p">[5] <a href="https://huggingface.co/blog/falcon-180b" data-href="https://huggingface.co/blog/falcon-180b" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Falcon 180b Blog</a></p><p name="4d63" id="4d63" class="graf graf--p graf-after--p">[6] <a href="https://arxiv.org/abs/2101.00027" data-href="https://arxiv.org/abs/2101.00027" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2101.00027</a><strong class="markup--strong markup--p-strong"> [cs.CL]</strong></p><p name="125b" id="125b" class="graf graf--p graf-after--p">[7] <a href="https://github.com/gururise/AlpacaDataCleaned" data-href="https://github.com/gururise/AlpacaDataCleaned" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Alpaca Repo</a></p><p name="5a69" id="5a69" class="graf graf--p graf-after--p">[8] <a href="https://arxiv.org/abs/2303.18223" data-href="https://arxiv.org/abs/2303.18223" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2303.18223</a><strong class="markup--strong markup--p-strong"> [cs.CL]</strong></p><p name="6b6c" id="6b6c" class="graf graf--p graf-after--p">[9] <a href="https://arxiv.org/abs/2112.11446" data-href="https://arxiv.org/abs/2112.11446" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2112.11446</a><strong class="markup--strong markup--p-strong"> [cs.CL]</strong></p><p name="63b7" id="63b7" class="graf graf--p graf-after--p">[10] <a href="https://arxiv.org/abs/1508.07909" data-href="https://arxiv.org/abs/1508.07909" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:1508.07909</a><strong class="markup--strong markup--p-strong"> [cs.CL]</strong></p><p name="7393" id="7393" class="graf graf--p graf-after--p">[11] <a href="https://github.com/google/sentencepiece/tree/master" data-href="https://github.com/google/sentencepiece/tree/master" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SentencePience Repo</a></p><p name="1c5e" id="1c5e" class="graf graf--p graf-after--p">[12] <a href="https://huggingface.co/docs/tokenizers/quicktour" data-href="https://huggingface.co/docs/tokenizers/quicktour" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Tokenizers Doc</a></p><p name="2ee1" id="2ee1" class="graf graf--p graf-after--p">[13] <a href="https://arxiv.org/abs/1706.03762" data-href="https://arxiv.org/abs/1706.03762" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:1706.03762</a><strong class="markup--strong markup--p-strong"> [cs.CL]</strong></p><p name="c272" id="c272" class="graf graf--p graf-after--p">[14] <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;t=5307s" data-href="https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;t=5307s" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Andrej Karpathy Lecture</a></p><p name="bc93" id="bc93" class="graf graf--p graf-after--p">[15] <a href="https://huggingface.co/learn/nlp-course/chapter1/7?fw=pt" data-href="https://huggingface.co/learn/nlp-course/chapter1/7?fw=pt" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Hugging Face NLP Course</a></p><p name="66f8" id="66f8" class="graf graf--p graf-after--p">[16] <a href="https://arxiv.org/abs/1810.04805" data-href="https://arxiv.org/abs/1810.04805" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:1810.04805</a><strong class="markup--strong markup--p-strong"> [cs.CL]</strong></p><p name="9aba" id="9aba" class="graf graf--p graf-after--p">[17] arXiv:1910.13461 [cs.CL]</p><p name="0328" id="0328" class="graf graf--p graf-after--p">[18] <a href="https://arxiv.org/abs/1603.05027" data-href="https://arxiv.org/abs/1603.05027" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:1603.05027</a><strong class="markup--strong markup--p-strong"> [cs.CV]</strong></p><p name="16f4" id="16f4" class="graf graf--p graf-after--p">[19] <a href="https://arxiv.org/abs/1607.06450" data-href="https://arxiv.org/abs/1607.06450" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:1607.06450</a><strong class="markup--strong markup--p-strong"> [stat.ML]</strong></p><p name="bba4" id="bba4" class="graf graf--p graf-after--p">[20] <a href="https://arxiv.org/abs/1803.02155" data-href="https://arxiv.org/abs/1803.02155" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:1803.02155</a><strong class="markup--strong markup--p-strong"> [cs.CL]</strong></p><p name="30ea" id="30ea" class="graf graf--p graf-after--p">[21] <a href="https://arxiv.org/abs/2203.15556" data-href="https://arxiv.org/abs/2203.15556" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2203.15556</a><strong class="markup--strong markup--p-strong"> [cs.CL]</strong></p><p name="1824" id="1824" class="graf graf--p graf-after--p">[22] <a href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html" data-href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Trained with Mixed Precision Nvidia Doc</a></p><p name="16d5" id="16d5" class="graf graf--p graf-after--p">[23] <a href="https://www.deepspeed.ai/training/" data-href="https://www.deepspeed.ai/training/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">DeepSpeed Doc</a></p><p name="0b52" id="0b52" class="graf graf--p graf-after--p">[24] <a href="https://paperswithcode.com/method/weight-decay" data-href="https://paperswithcode.com/method/weight-decay" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://paperswithcode.com/method/weight-decay</a></p><p name="bce1" id="bce1" class="graf graf--p graf-after--p">[25] <a href="https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48" data-href="https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48</a></p><p name="18cc" id="18cc" class="graf graf--p graf-after--p">[26] <a href="https://arxiv.org/abs/2001.08361" data-href="https://arxiv.org/abs/2001.08361" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2001.08361</a><strong class="markup--strong markup--p-strong"> [cs.LG]</strong></p><p name="1bf5" id="1bf5" class="graf graf--p graf-after--p">[27] <a href="https://arxiv.org/abs/1803.05457" data-href="https://arxiv.org/abs/1803.05457" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:1803.05457</a><strong class="markup--strong markup--p-strong"> [cs.AI]</strong></p><p name="3ead" id="3ead" class="graf graf--p graf-after--p">[28] arXiv:1905.07830 [cs.CL]</p><p name="0eaa" id="0eaa" class="graf graf--p graf-after--p">[29] arXiv:2009.03300 [cs.CY]</p><p name="2abe" id="2abe" class="graf graf--p graf-after--p">[30] arXiv:2109.07958 [cs.CL]</p><p name="155a" id="155a" class="graf graf--p graf-after--p graf--trailing">[31] <a href="https://huggingface.co/blog/evaluating-mmlu-leaderboard" data-href="https://huggingface.co/blog/evaluating-mmlu-leaderboard" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://huggingface.co/blog/evaluating-mmlu-leaderboard</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@shawhin" class="p-author h-card">Shaw Talebi</a> on <a href="https://medium.com/p/8c477768f1f9"><time class="dt-published" datetime="2023-09-21T20:36:58.764Z">September 21, 2023</time></a>.</p><p><a href="https://medium.com/@shawhin/how-to-build-an-llm-from-scratch-8c477768f1f9" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 2, 2024.</p></footer></article></body></html>