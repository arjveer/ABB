<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>A Practical Introduction to LLMs</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">A Practical Introduction to LLMs</h1>
</header>
<section data-field="subtitle" class="p-summary">
3 levels of using LLMs in practice
</section>
<section data-field="body" class="e-content">
<section name="89e0" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1842" id="1842" class="graf graf--h3 graf--leading graf--title"><strong class="markup--strong markup--h3-strong">A Practical Introduction to LLMs</strong></h3><h4 name="7338" id="7338" class="graf graf--h4 graf-after--h3 graf--subtitle">3 levels of using LLMs in practice</h4><p name="e915" id="e915" class="graf graf--p graf-after--h4">This is the first article in a <a href="https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c" data-href="https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">series</a> on using Large Language Models (LLMs) in practice. Here I will give an introduction to LLMs and present 3 levels of working with them. Future articles will explore practical aspects of LLMs, such as how to use <a href="https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971" data-href="https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971" class="markup--anchor markup--p-anchor" target="_blank">OpenAI’s public API</a>, the <a href="https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" data-href="https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" class="markup--anchor markup--p-anchor" target="_blank">Hugging Face Transformers</a> Python library, how to <a href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--p-anchor" target="_blank">fine-tune LLMs</a>, and <a href="https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9" data-href="https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9" class="markup--anchor markup--p-anchor" target="_blank">how to build an LLM from scratch</a>.</p><figure name="fadc" id="fadc" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="0*Nl-5C1WBW4XdGkNI" data-width="5472" data-height="3648" data-unsplash-photo-id="Oaqk7qqNh_c" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*Nl-5C1WBW4XdGkNI"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@impatrickt?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@impatrickt?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Patrick Tomasso</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure></div></div></section><section name="e074" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="5ee4" id="5ee4" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">What is an LLM?</strong></h3><p name="b3ef" id="b3ef" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">LLM</strong> is short for <strong class="markup--strong markup--p-strong">Large Language Model</strong>, which is a recent innovation in AI and machine learning. This powerful new type of AI went viral in Dec 2022 with the release of ChatGPT.</p><p name="8c54" id="8c54" class="graf graf--p graf-after--p">For those enlightened enough to live outside the world of AI buzz and tech news cycles, <strong class="markup--strong markup--p-strong">ChatGPT</strong> is a chat interface that ran on an LLM called GPT-3 (now upgraded to either GPT-3.5 or GPT-4 at the time of writing this).</p><p name="6a69" id="6a69" class="graf graf--p graf-after--p">If you’ve used ChatGPT, it’s obvious that this is not your traditional chatbot from <a href="https://en.wikipedia.org/wiki/AIM_%28software%29" data-href="https://en.wikipedia.org/wiki/AIM_(software)" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AOL Instant Messenger</a> or your credit card’s customer care.</p><p name="78fc" id="78fc" class="graf graf--p graf-after--p">This one <em class="markup--em markup--p-em">feels</em> different.</p><figure name="76df" id="76df" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/tFHeUSJAYbE?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><h3 name="2a42" id="2a42" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">What makes an LLM “large”?</strong></h3><p name="ac00" id="ac00" class="graf graf--p graf-after--h3">When I heard the term “Large Language Model,” one of my first questions was, <em class="markup--em markup--p-em">how is this different from a “regular” language model?</em></p><p name="bc0c" id="bc0c" class="graf graf--p graf-after--p">A language model is more generic than a large language model. Just like all squares are rectangles but not all rectangles are squares. <strong class="markup--strong markup--p-strong">All LLMs are language models, but not all language models are LLMs</strong>.</p><figure name="b33f" id="b33f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*aXPMaaRhZ8lgMhHA1HzO1w.png" data-width="1500" data-height="750" src="https://cdn-images-1.medium.com/max/800/1*aXPMaaRhZ8lgMhHA1HzO1w.png"><figcaption class="imageCaption">Large Language Models are a special type of Language Model. Image by author.</figcaption></figure><p name="426f" id="426f" class="graf graf--p graf-after--figure">Okay, so LLMs are a special type of language model, <strong class="markup--strong markup--p-strong">but what makes them special?</strong></p><p name="fcd0" id="fcd0" class="graf graf--p graf-after--p">There are <strong class="markup--strong markup--p-strong">2 key properties</strong> that distinguish LLMs from other language models. One is quantitative, and the other is qualitative.</p><ol class="postList"><li name="34fb" id="34fb" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Quantitatively</strong>, what distinguishes an LLM is the number of parameters used in the model. Current LLMs have on the order of <strong class="markup--strong markup--li-strong">10–100 billion parameters</strong> [1].</li><li name="a568" id="a568" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Qualitatively</strong>, something remarkable happens when a language model becomes “large.” It exhibits so-called <strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">emergent properties</em></strong><em class="markup--em markup--li-em"> </em>e.g. zero-shot learning [1]. These are <strong class="markup--strong markup--li-strong">properties that seem to suddenly appear</strong> when a language model reaches a sufficiently large size.</li></ol><h3 name="4094" id="4094" class="graf graf--h3 graf-after--li"><strong class="markup--strong markup--h3-strong">Zero-shot Learning</strong></h3><p name="a326" id="a326" class="graf graf--p graf-after--h3">The major innovation of GPT-3 (and other LLMs) is that it is capable of <strong class="markup--strong markup--p-strong">zero-shot learning</strong> in a wide variety of contexts [2]. This means ChatGPT can <strong class="markup--strong markup--p-strong">perform a task even if it has not been explicitly trained to do it</strong>.</p><p name="6d7d" id="6d7d" class="graf graf--p graf-after--p">While this might be no big deal to us highly evolved humans, this zero-shot learning ability starkly contrasts the prior machine learning paradigm.</p><p name="3bd2" id="3bd2" class="graf graf--p graf-after--p">Previously, a model needed to be <strong class="markup--strong markup--p-strong">explicitly trained on the task it aimed to do</strong> in order to have good performance. This could require anywhere from 1k-1M pre-labeled training examples.</p><p name="5a00" id="5a00" class="graf graf--p graf-after--p">For instance, if you wanted a computer to do language translation, sentiment analysis, and identify grammatical errors. Each of these tasks would require a specialized model trained on a large set of labeled examples. Now, however, <strong class="markup--strong markup--p-strong">LLMs can do all these things without explicit training</strong>.</p><h3 name="698f" id="698f" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">How do LLMs work?</strong></h3><p name="4c76" id="4c76" class="graf graf--p graf-after--h3">The core task used to train most state-of-the-art LLMs is <strong class="markup--strong markup--p-strong">word prediction</strong>. In other words, given a sequence of words, <strong class="markup--strong markup--p-strong">what is the probability distribution of the next word</strong>?</p><p name="bcfe" id="bcfe" class="graf graf--p graf-after--p">For example, given the sequence “Listen to your ____,” the most likely next words might be: heart, gut, body, parents, grandma, etc. This might look like the probability distribution shown below.</p><figure name="68b3" id="68b3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_yFX4vrio7Io2tnVWbW8xQ.png" data-width="1500" data-height="750" src="https://cdn-images-1.medium.com/max/800/1*_yFX4vrio7Io2tnVWbW8xQ.png"><figcaption class="imageCaption">Toy probability distribution of next word in sequence “Listen to your ___.” Image by author.</figcaption></figure><p name="58e0" id="58e0" class="graf graf--p graf-after--figure">Interestingly, this is the same way many (non-large) language models have been trained in the past (e.g. GPT-1) [3]. However, for some reason, when language models get beyond a certain size (say ~10B parameters), these (emergent) abilities, such as zero-shot learning, can start to pop up [1].</p><p name="da71" id="da71" class="graf graf--p graf-after--p">Although there is no clear answer as to <em class="markup--em markup--p-em">why</em> this occurs (only speculations for now), it is clear that LLMs are a powerful technology with countless potential use cases.</p><h3 name="a624" id="a624" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">3 Levels of Using LLMs</strong></h3><p name="f27a" id="f27a" class="graf graf--p graf-after--h3">Now we turn to how to use this powerful technology in practice. While there are countless potential LLM use cases, here I categorize them into 3 levels <strong class="markup--strong markup--p-strong">ordered by required technical knowledge and computational resources</strong>. We start with the most accessible.</p><h4 name="ca0e" id="ca0e" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Level 1: Prompt Engineering</strong></h4><p name="3f58" id="3f58" class="graf graf--p graf-after--h4">The first level of using LLMs in practice is <strong class="markup--strong markup--p-strong">prompt engineering</strong>, which I define as <strong class="markup--strong markup--p-strong">any use of an LLM out-of-the-box</strong> i.e. not changing any model parameters. While many technically-inclined individuals seem to scoff at the idea of prompt engineering, this is the most accessible way to use LLMs (both technically and economically) in practice.</p><div name="6b5c" id="6b5c" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://towardsdatascience.com/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" data-href="https://towardsdatascience.com/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f"><strong class="markup--strong markup--mixtapeEmbed-strong">Prompt Engineering — How to trick AI into solving your problems</strong><br><em class="markup--em markup--mixtapeEmbed-em">7 prompting tricks, Langchain, and Python example code</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="47d6a520aa635ed82852bc636764e286" data-thumbnail-img-id="0*ZcfH-qxXT4AYAqwr" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*ZcfH-qxXT4AYAqwr);"></a></div><p name="3815" id="3815" class="graf graf--p graf-after--mixtapeEmbed">There are 2 main ways to do prompt engineering: the <strong class="markup--strong markup--p-strong">Easy Way</strong> and the <strong class="markup--strong markup--p-strong">Less Easy Way</strong>.</p><p name="b574" id="b574" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Easy Way: ChatGPT (or another convenient LLM UI) — </strong>The key benefit of this method is convenience. Tools like ChatGPT provide an intuitive, no-cost, and no-code way to use an LLM (it doesn’t get much easier than that).</p><p name="11ee" id="11ee" class="graf graf--p graf-after--p">However, convenience often comes at a cost. In this case, there are <strong class="markup--strong markup--p-strong">2 key drawbacks</strong> to this approach. The <strong class="markup--strong markup--p-strong">first</strong> is a lack of functionality. For example, ChatGPT does not readily enable users to customize model input parameters (e.g. temperature or max response length), which are values that modulate LLM outputs. <strong class="markup--strong markup--p-strong">Second</strong>, interactions with the ChatGPT UI cannot be readily automated and thus applied to large-scale use cases.</p><p name="ed31" id="ed31" class="graf graf--p graf-after--p">While these drawbacks may be dealbreakers for some use cases, both can be ameliorated if we take prompt engineering one step further.</p><p name="541f" id="541f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Less Easy Way: Interact with LLM directly — </strong>We can overcome some of the drawbacks of ChatGPT by interacting directly with an LLM via programmatic interfaces. This could be via public APIs (e.g. OpenAI’s API) or running an LLM locally (using libraries like <a href="https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" data-href="https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" class="markup--anchor markup--p-anchor" target="_blank">Transformers</a>).</p><p name="486b" id="486b" class="graf graf--p graf-after--p">While this way of doing prompt engineering is less convenient (since it requires programming knowledge and potential API costs), it provides a customizable, flexible, and scalable way to use LLMs in practice. Future articles in this series will discuss <a href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971" data-href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">paid</a> and <a href="https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" data-href="https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">cost-free</a> ways to do this type of prompt engineering.</p><p name="0609" id="0609" class="graf graf--p graf-after--p">Although prompt engineering (as defined here) can handle most potential LLM applications, relying on a generic model, out-of-the-box may result in sub-optimal performance for specific use cases. For these situations, we can go to the next level of using LLMs.</p><h4 name="4055" id="4055" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Level 2: Model Fine-tuning</strong></h4><p name="743c" id="743c" class="graf graf--p graf-after--h4">The second level of using an LLM is <strong class="markup--strong markup--p-strong">model fine-tuning</strong>, which I’ll define as taking an existing LLM and tweaking it for a particular use case by <strong class="markup--strong markup--p-strong">training at least 1 (internal) model parameter </strong>i.e. weights and biases. For the aficionados out there, this is an example of <em class="markup--em markup--p-em">transfer learning</em> i.e. using some part of an existing model to develop another model.</p><p name="4a28" id="4a28" class="graf graf--p graf-after--p">Fine-tuning typically consists of 2 steps. <strong class="markup--strong markup--p-strong">Step 1</strong>: Obtain a pre-trained LLM. <strong class="markup--strong markup--p-strong">Step 2</strong>: Update model parameters for a specific task given (typically 1000s of) high-quality labeled examples.</p><p name="94fb" id="94fb" class="graf graf--p graf-after--p">The model parameters define the LLM’s internal representation of the input text. Thus, by tweaking these parameters for a particular task, the internal representations become optimized for the fine-tuning task (or at least that’s the idea).</p><p name="c444" id="c444" class="graf graf--p graf-after--p">This is a powerful approach to model development because a relatively <strong class="markup--strong markup--p-strong">small number of examples</strong> and computational resources <strong class="markup--strong markup--p-strong">can produce exceptional model performance</strong>.</p><p name="82ba" id="82ba" class="graf graf--p graf-after--p">The downside, however, is it requires significantly more technical expertise and computational resources than prompt engineering. In a <a href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--p-anchor" target="_blank">future article</a>, I will attempt to curb this downside by reviewing fine-tuning techniques and sharing example Python code.</p><div name="00a7" id="00a7" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91"><strong class="markup--strong markup--mixtapeEmbed-strong">Fine-Tuning Large Language Models (LLMs)</strong><br><em class="markup--em markup--mixtapeEmbed-em">A conceptual overview with example Python code</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="24d889172e792bfd8cc0d6a6b2908d00" data-thumbnail-img-id="1*YHNrnuaHtS2meh39gGmCCw.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*YHNrnuaHtS2meh39gGmCCw.png);"></a></div><p name="132c" id="132c" class="graf graf--p graf-after--mixtapeEmbed">While prompt engineering and model fine-tuning can likely handle 99% of LLM applications, there are cases where one must go even further.</p><h4 name="7d77" id="7d77" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Level 3: Build your own LLM</strong></h4><p name="c2bc" id="c2bc" class="graf graf--p graf-after--h4">The third and final way to use an LLM in practice is to <a href="https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9" data-href="https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">build your own</strong></a>. In terms of model parameters, this is where you <strong class="markup--strong markup--p-strong">come up with all the model parameters </strong>from scratch.</p><p name="b331" id="b331" class="graf graf--p graf-after--p">An LLM is primarily a product of its training data. Thus, for some applications, it may be necessary to curate custom, high-quality text corpora for model training—for example, a medical research corpus for the development of a clinical application.</p><p name="b1c7" id="b1c7" class="graf graf--p graf-after--p">The biggest upside to this approach is you can <strong class="markup--strong markup--p-strong">fully customize the LLM for your particular use case</strong>. This is the ultimate flexibility. However, as is often the case, flexibility comes at the cost of convenience.</p><p name="94a1" id="94a1" class="graf graf--p graf-after--p">Since the <strong class="markup--strong markup--p-strong">key to LLM performance is scale</strong>, building an LLM from scratch requires tremendous computational resources and technical expertise. In other words, this isn’t going to be a solo weekend project but a full team working for months, if not years, with a 7–8F budget.</p><p name="af15" id="af15" class="graf graf--p graf-after--p">Nevertheless, in a <a href="https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9" data-href="https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9" class="markup--anchor markup--p-anchor" target="_blank">future article</a> in this series, we will explore popular techniques for developing LLMs from scratch.</p><div name="c0e2" id="c0e2" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9" data-href="https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9"><strong class="markup--strong markup--mixtapeEmbed-strong">How to Build an LLM from Scratch</strong><br><em class="markup--em markup--mixtapeEmbed-em">Data Curation, Transformers, Training at Scale, and Model Evaluation</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="1f4c43b3e14643643309a58986d69715" data-thumbnail-img-id="0*kNzeztfZxg8IsXA4" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*kNzeztfZxg8IsXA4);"></a></div><h3 name="09e3" id="09e3" class="graf graf--h3 graf-after--mixtapeEmbed">Conclusion</h3><p name="c6e8" id="c6e8" class="graf graf--p graf-after--h3">While there is more than enough hype about LLMs, they are a powerful innovation in AI. Here, I provided a primer on what LLMs are and framed how they can be used in practice. The <a href="https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971" data-href="https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971" class="markup--anchor markup--p-anchor" target="_blank">next article</a> in this series will give a beginner’s guide to OpenAI’s Python API to help jumpstart your next LLM use case.</p><p name="5e91" id="5e91" class="graf graf--p graf-after--p">👉 <strong class="markup--strong markup--p-strong">More on LLMs</strong>: <a href="https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971" data-href="https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971" class="markup--anchor markup--p-anchor" target="_blank">OpenAI API</a> | <a href="https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" data-href="https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" class="markup--anchor markup--p-anchor" target="_blank">Hugging Face Transformers</a> | <a href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" data-href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" class="markup--anchor markup--p-anchor" target="_blank">Prompt Engineering</a> | <a href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--p-anchor" target="_blank">Fine-tuning</a> | <a href="https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9" data-href="https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Build an LLM</a> | <a href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" data-href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">QLoRA</a> | <a href="https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac" data-href="https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac" class="markup--anchor markup--p-anchor" target="_blank">RAG</a> | <a href="https://towardsdatascience.com/text-embeddings-classification-and-semantic-search-8291746220be" data-href="https://towardsdatascience.com/text-embeddings-classification-and-semantic-search-8291746220be" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Text Embeddings</a></p><div name="0f36" id="0f36" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/list/8e009ae3054c" data-href="https://shawhin.medium.com/list/8e009ae3054c" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/list/8e009ae3054c"><strong class="markup--strong markup--mixtapeEmbed-strong">Large Language Models (LLMs)</strong><br><em class="markup--em markup--mixtapeEmbed-em">A series on how to use LLMs in practice.</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/list/8e009ae3054c" class="js-mixtapeImage mixtapeImage mixtapeImage--mediumCatalog  u-ignoreBlock" data-media-id="e38ca2d0de6aa61d7c09af603d8f43e1" data-thumbnail-img-id="0*ae40ceaca4c0bdcbdf431a66c777e68bffc1e356.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/304/160/0*ae40ceaca4c0bdcbdf431a66c777e68bffc1e356.jpeg);"></a></div></div></div></section><section name="eb72" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="8706" id="8706" class="graf graf--h3 graf--leading">Resources</h3><p name="9bc8" id="9bc8" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Connect</strong>: <a href="https://shawhintalebi.com/" data-href="https://shawhintalebi.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener noopener" target="_blank">My website</a> | <a href="https://calendly.com/shawhintalebi" data-href="https://calendly.com/shawhintalebi" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener noopener" target="_blank">Book a call</a></p><p name="878b" id="878b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Socials</strong>: <a href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" data-href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">YouTube 🎥</a> | <a href="https://www.linkedin.com/in/shawhintalebi/" data-href="https://www.linkedin.com/in/shawhintalebi/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">LinkedIn</a> | <a href="https://twitter.com/ShawhinT" data-href="https://twitter.com/ShawhinT" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Twitter</a></p><p name="567a" id="567a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Support</strong>: <a href="https://www.buymeacoffee.com/shawhint?source=about_page-------------------------------------" data-href="https://www.buymeacoffee.com/shawhint?source=about_page-------------------------------------" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener noopener noopener" target="_blank">Buy me a coffee</a> ☕️</p><div name="bcef" id="bcef" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/subscribe" data-href="https://shawhin.medium.com/subscribe" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/subscribe"><strong class="markup--strong markup--mixtapeEmbed-strong">Get FREE access to every new story I write</strong><br><em class="markup--em markup--mixtapeEmbed-em">Get FREE access to every new story I write P.S. I do not share your email with anyone By signing up, you will create a…</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/subscribe" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8d6d6c2d301f2c367b618a850fa468b3" data-thumbnail-img-id="0*O69FxQ5FttNEA_r4" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*O69FxQ5FttNEA_r4);"></a></div></div></div></section><section name="99bf" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="5117" id="5117" class="graf graf--p graf--leading">[1] Survey of Large Language Models. <a href="https://arxiv.org/abs/2303.18223" data-href="https://arxiv.org/abs/2303.18223" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2303.18223</a><strong class="markup--strong markup--p-strong"> [cs.CL]</strong></p><p name="f54a" id="f54a" class="graf graf--p graf-after--p">[2] GPT-3 Paper. <a href="https://arxiv.org/abs/2005.14165" data-href="https://arxiv.org/abs/2005.14165" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2005.14165</a><strong class="markup--strong markup--p-strong"> [cs.CL]</strong></p><p name="0931" id="0931" class="graf graf--p graf-after--p graf--trailing">[3] Radford, A., &amp; Narasimhan, K. (2018). Improving Language Understanding by Generative Pre-Training. (<a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" data-href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GPT-1 Paper</a>)</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@shawhin" class="p-author h-card">Shaw Talebi</a> on <a href="https://medium.com/p/65194dda1148"><time class="dt-published" datetime="2023-07-13T18:28:26.030Z">July 13, 2023</time></a>.</p><p><a href="https://medium.com/@shawhin/a-practical-introduction-to-llms-65194dda1148" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 2, 2024.</p></footer></article></body></html>