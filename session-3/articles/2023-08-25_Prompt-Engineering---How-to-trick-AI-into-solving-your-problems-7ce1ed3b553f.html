<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Prompt Engineering — How to trick AI into solving your problems</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Prompt Engineering — How to trick AI into solving your problems</h1>
</header>
<section data-field="subtitle" class="p-summary">
7 prompting tricks, Langchain, and Python example code
</section>
<section data-field="body" class="e-content">
<section name="c988" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="4476" id="4476" class="graf graf--h3 graf--leading graf--title"><strong class="markup--strong markup--h3-strong">Prompt Engineering: How to Trick AI into Solving Your Problems</strong></h3><h4 name="a88b" id="a88b" class="graf graf--h4 graf-after--h3 graf--subtitle">7 prompting tricks, LangChain, and Python example code</h4><p name="57ee" id="57ee" class="graf graf--p graf-after--h4">This is the fourth article in a <a href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" data-href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">series on using large language models</a> (LLMs) in practice. Here, I will discuss prompt engineering (PE) and how to use it to build LLM-enabled applications. I start by reviewing key PE techniques and then walk through Python example code of using LangChain to build an LLM-based application.</p><figure name="0721" id="0721" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="0*ZcfH-qxXT4AYAqwr" data-width="6000" data-height="4000" data-unsplash-photo-id="mZNRsYE9Qi4" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*ZcfH-qxXT4AYAqwr"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@ninjason?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@ninjason?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Jason Leung</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure></div></div></section><section name="45d2" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="c70d" id="c70d" class="graf graf--p graf--leading">When first hearing about prompt engineering, many technical people (including myself) tend to scoff at the idea. We might think, “<em class="markup--em markup--p-em">Prompt engineering? Psssh, that’s lame. Tell me how to build an LLM from scratch.</em>”</p><p name="e668" id="e668" class="graf graf--p graf-after--p">However, after diving into it more deeply, I’d caution developers against writing off prompt engineering automatically. I’ll go even further and say that <strong class="markup--strong markup--p-strong">prompt engineering can realize 80% of the value</strong> of most LLM use cases with (relatively) very low effort.</p><p name="73de" id="73de" class="graf graf--p graf-after--p">My goal with this article is to convey this point via a practical review of prompt engineering and illustrative examples. While there are surely gaps in what prompt engineering can do, it opens the door to discovering simple and clever solutions to our problems.</p><figure name="e8a5" id="e8a5" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/0cf7vzM_dZ0?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe><figcaption class="imageCaption">Supplemental Video.</figcaption></figure><h3 name="c2e1" id="c2e1" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">What is Prompt Engineering?</strong></h3><p name="3930" id="3930" class="graf graf--p graf-after--h3">In the <a href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" data-href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">first article of this series</a>, I defined <strong class="markup--strong markup--p-strong">prompt engineering</strong> as <strong class="markup--strong markup--p-strong">any use of an LLM out-of-the-box</strong> (i.e. not training any internal model parameters). However, there is much more that can be said about it.</p><ol class="postList"><li name="b8ca" id="b8ca" class="graf graf--li graf-after--p">Prompt Engineering is “<em class="markup--em markup--li-em">the means by which LLMs are programmed with prompts.</em>” [<a href="https://arxiv.org/abs/2302.11382" data-href="https://arxiv.org/abs/2302.11382" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">1</a>]</li><li name="ad6e" id="ad6e" class="graf graf--li graf-after--li">Prompt Engineering is “a<em class="markup--em markup--li-em">n empirical art of composing and formatting the prompt to maximize a model’s performance on a desired task.</em>” [<a href="https://arxiv.org/abs/2106.09685" data-href="https://arxiv.org/abs/2106.09685" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">2</a>]</li><li name="547f" id="547f" class="graf graf--li graf--startsWithDoubleQuote graf-after--li"><em class="markup--em markup--li-em">“language models… want to complete documents, and so you can trick them into performing tasks just by arranging fake documents</em>.” [<a href="https://www.youtube.com/watch?v=bZQun8Y4L2A" data-href="https://www.youtube.com/watch?v=bZQun8Y4L2A" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">3</a>]</li></ol><p name="637e" id="637e" class="graf graf--p graf-after--li">The first definition conveys the key innovation coming from LLMs, which is that <strong class="markup--strong markup--p-strong">computers can now be programmed using plain English</strong>. The second point frames prompt engineering as a largely empirical endeavor, where practitioners, tinkerers, and builders are the key explorers of this new way of programming.</p><p name="78d7" id="78d7" class="graf graf--p graf-after--p">The third point (from <a href="https://medium.com/u/ac9d9a35533e" data-href="https://medium.com/u/ac9d9a35533e" data-anchor-type="2" data-user-id="ac9d9a35533e" data-action-value="ac9d9a35533e" data-action="show-user-card" data-action-type="hover" class="markup--user markup--p-user" target="_blank">Andrej Karpathy</a>) reminds us that <strong class="markup--strong markup--p-strong">LLMs aren’t explicitly trained to do almost anything we ask them to do</strong>. Thus, in some sense, we are “tricking” these language models to solve problems. I feel this captures the essence of prompt engineering, which relies less on your technical skills and more on your creativity.</p><h3 name="a493" id="a493" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">2 Levels of Prompt Engineering</strong></h3><p name="6214" id="6214" class="graf graf--p graf-after--h3">There are two distinct ways in which one can do prompt engineering, which I called the “<strong class="markup--strong markup--p-strong">easy way</strong>” and the “<strong class="markup--strong markup--p-strong">less easy way</strong>” in the <a href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" data-href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">first article</a> of this series.</p><h4 name="a614" id="a614" class="graf graf--h4 graf-after--p">The Easy Way</h4><p name="d9b2" id="d9b2" class="graf graf--p graf-after--h4">This is how most of the world does prompt engineering, which is via ChatGPT (or something similar). It is an intuitive, no-code, and cost-free way to interact with an LLM.</p><p name="82cf" id="82cf" class="graf graf--p graf-after--p">While this is a great approach for something quick and simple, e.g. summarizing a page of text, rewriting an email, helping you brainstorm birthday party plans, etc., it has its downsides. A big one is that <strong class="markup--strong markup--p-strong">it’s not easy to integrate this approach into a larger automated process or software system</strong>. To do this, we need to go one step further.</p><h4 name="782d" id="782d" class="graf graf--h4 graf-after--p">The Less Easy Way</h4><p name="fb78" id="fb78" class="graf graf--p graf-after--h4">This resolves many of the drawbacks of the “easy way” by interacting with LLMs programmatically i.e. using Python. We got a sense of how we can do this in the previous two articles of this series, where explored <a href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971" data-href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenAI’s Python API</a> and the <a href="https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" data-href="https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Hugging Face Transformers library</a>.</p><p name="4019" id="4019" class="graf graf--p graf-after--p">While this requires more technical knowledge, <strong class="markup--strong markup--p-strong">this is where the real power of prompt engineering lies</strong> because it allows developers to integrate LLM-based modules into larger software systems.</p><p name="daec" id="daec" class="graf graf--p graf-after--p">A good (and perhaps ironic) example of this is ChatGPT. The core of this product is prompting a pre-trained model (i.e. GPT-3.5-turbo) to act like a chatbot and then wrapping it in an easy-to-use web interface.</p><p name="a4e2" id="a4e2" class="graf graf--p graf-after--p">Of course, developing GPT-3.5-turbo is the hard part, <strong class="markup--strong markup--p-strong">but that’s not something we need to worry about here</strong>. With all the pre-trained LLMs we have at our fingertips, almost anyone with basic programming skills can create a powerful AI application like ChatGPT without being an AI researcher or a machine learning Ph.D.</p><h3 name="2463" id="2463" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Building AI Apps with Prompt Engineering</strong></h3><p name="b518" id="b518" class="graf graf--p graf-after--h3">The less easy way unlocks a <strong class="markup--strong markup--p-strong">new paradigm of programming and software development</strong>. No longer are developers required to define every inch of logic in their software systems. They now have the option to offload a non-trivial portion to LLMs. Let’s look at a concrete example of what this might look like.</p><p name="e522" id="e522" class="graf graf--p graf-after--p">Suppose you want to create an <strong class="markup--strong markup--p-strong">automatic grader for a high school history class</strong>. The trouble, however, is that all the questions have written responses, so there often can be multiple versions of a correct answer. For example, the following responses to “<em class="markup--em markup--p-em">Who was the 35th president of the United States of America?</em>” could be correct.</p><ul class="postList"><li name="fc29" id="fc29" class="graf graf--li graf-after--p">John F. Kennedy</li><li name="f203" id="f203" class="graf graf--li graf-after--li">JFK</li><li name="7f90" id="7f90" class="graf graf--li graf-after--li">Jack Kennedy (a common nickname)</li><li name="080d" id="080d" class="graf graf--li graf-after--li">John Fitzgerald Kennedy (probably trying to get extra credit)</li><li name="b5ca" id="b5ca" class="graf graf--li graf-after--li">John F. Kenedy (misspelled last name)</li></ul><p name="6f92" id="6f92" class="graf graf--p graf-after--li">In the <strong class="markup--strong markup--p-strong">traditional programming paradigm</strong>, it was on the developer to figure out how to account for all these variations. To do this, they might list all possible correct answers and use an exact string-matching algorithm or maybe even use fuzzy matching to help with misspelled words.</p><p name="907b" id="907b" class="graf graf--p graf-after--p">However, with this new <strong class="markup--strong markup--p-strong">LLM-enabled paradigm</strong>, <strong class="markup--strong markup--p-strong">the problem can be solved through simple prompt engineering</strong>. For instance, we could use the following prompt to evaluate student answers.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="markdown" name="0e86" id="0e86" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">You are a high school history teacher grading homework assignments. \<br />Based on the homework question indicated by “Q:” and the correct answer \<br />indicated by “A:”, your task is to determine whether the student&#x27;s answer is \<br />correct.<br />Grading is binary; therefore, student answers can be correct or wrong.<br />Simple misspellings are okay.<br /> <br />Q: {question}<br />A: {correct<span class="hljs-emphasis">_answer}<br /> <br />Student Answer: {student_</span>answer}</span></pre><p name="43c5" id="43c5" class="graf graf--p graf-after--pre">We can think of this prompt as a function, where given a <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">question</em></strong>, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">correct_answer</em></strong>, and <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">student_answer</em></strong>, it generates the student&#39;s grade. This can then be integrated into a larger piece of software that implements the automatic grader.</p><p name="59aa" id="59aa" class="graf graf--p graf-after--p">In terms of time-saving, this prompt took me about 2 minutes to write, while if I were to try to develop an algorithm to do the same thing, it would take me hours (if not days) and probably have worse performance. <strong class="markup--strong markup--p-strong">So the time savings for tasks like this are 100–1000x</strong>.</p><p name="3c5a" id="3c5a" class="graf graf--p graf-after--p">Of course, there are many tasks in which LLMs do not provide any substantial benefit, and other existing methods are much better suited (e.g. predicting tomorrow’s weather). In no way are LLMs the solution to every problem, but they do create a new set of solutions to tasks that require processing natural language effectively—something that has been historically difficult for computers to do.</p><h3 name="0644" id="0644" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">7 Tricks for Prompt Engineering</strong></h3><p name="2c53" id="2c53" class="graf graf--p graf-after--h3">While the prompt example from before may seem like a natural and obvious way to frame the automatic grading task, it deliberately employed specific prompt engineering heuristics (or “tricks,” as I’ll call them). These (and other) tricks have emerged as reliable ways to improve the quality of LLM responses.</p><p name="462a" id="462a" class="graf graf--p graf-after--p">Although there are many tips and tricks for writing good prompts, here I restrict the discussion to the ones that seem the most fundamental (IMO) based on a handful of references [1,3–5]. For a deeper dive, I recommend the reader explore the sources cited here.</p><h4 name="7237" id="7237" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Trick 1: Be Descriptive (More is Better)</strong></h4><p name="1f1a" id="1f1a" class="graf graf--p graf-after--h4">A defining feature of LLMs is that they are trained on massive text corpora. This equips them with a vast knowledge of the world and the ability to perform an enormous variety of tasks. However, this impressive generality may hinder performance on a specific task if the proper context is not provided.</p><p name="dc2a" id="dc2a" class="graf graf--p graf-after--p">For example, let’s compare two prompts for generating a birthday message for my dad.</p><p name="cd26" id="cd26" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Without Trick</em></strong></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="markdown" name="fd3f" id="fd3f" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Write me a birthday message for my dad.</span></pre><p name="0add" id="0add" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">With Trick</em></strong></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="markdown" name="284e" id="284e" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Write me a birthday message for my dad no longer than 200 \<br />characters. This is a big birthday because he is turning 50. To celebrate, \<br />I booked us a boys&#x27; trip to Cancun. Be sure to include some cheeky humor, he \<br />loves that.</span></pre><h4 name="3597" id="3597" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Trick 2: Give Examples</strong></h4><p name="1f0f" id="1f0f" class="graf graf--p graf-after--h4">The next trick is to give the LLM example responses to improve its performance on a particular task. The technical term for this is <strong class="markup--strong markup--p-strong">few-shot learning,</strong> and has been shown to improve LLM performance significantly [6].</p><p name="aaea" id="aaea" class="graf graf--p graf-after--p">Let’s look at a specific example. Say we want to write a subtitle for a Towards Data Science article. We can use existing examples to help guide the LLM completion.</p><p name="2dc4" id="2dc4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Without Trick</em></strong></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="markdown" name="bc2c" id="bc2c" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Given the title of a Towards Data Science blog article, write a subtitle for it.<br /><br />Title: Prompt Engineering—How to trick AI into solving your problems<br />Subtitle:</span></pre><p name="7db3" id="7db3" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">With Trick</em></strong></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="markdown" name="ac95" id="ac95" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Given the title of a Towards Data Science blog article, write a subtitle for it.<br /><br />Title: A Practical Introduction to LLMs<br />Subtitle: 3 levels of using LLMs in practice<br /><br />Title: Cracking Open the OpenAI (Python) API<br />Subtitle: A complete beginner-friendly introduction with example code<br /><br />Title: Prompt Engineering-How to trick AI into solving your problems<br />Subtitle:</span></pre><h4 name="cb95" id="cb95" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Trick 3: Use Structured Text</strong></h4><p name="0c53" id="0c53" class="graf graf--p graf-after--h4">Ensuring prompts follow an organized structure not only makes them easier to read and write, but also tends to help the model generate good completions. We employed this technique in the example for <strong class="markup--strong markup--p-strong">Trick 2</strong>, where we explicitly labeled the <em class="markup--em markup--p-em">title</em> and <em class="markup--em markup--p-em">subtitle</em> for each example.</p><p name="235e" id="235e" class="graf graf--p graf-after--p">However, there are countless ways we can give our prompts structure. Here are a handful of examples: use ALL CAPS for emphasis, use delimiters like ``` to highlight a body of text, use markup languages like Markdown or HTML to format text, use JSON to organize information, etc.<br> <br> Now, let’s see this in action.</p><p name="6bb0" id="6bb0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Without Trick</em></strong></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="markdown" name="a5ce" id="a5ce" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Write me a recipe for chocolate chip cookies.</span></pre><p name="2a16" id="2a16" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">With Trick</em></strong></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="markdown" name="a882" id="a882" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Create a well-organized recipe for chocolate chip cookies. Use the following \<br />formatting elements:<br /><br /><span class="hljs-strong">**Title**</span>: Classic Chocolate Chip Cookies<br /><span class="hljs-strong">**Ingredients**</span>: List the ingredients with precise measurements and formatting.<br /><span class="hljs-strong">**Instructions**</span>: Provide step-by-step instructions in numbered format, detailing the baking process.<br /><span class="hljs-strong">**Tips**</span>: Include a separate section with helpful baking tips and possible variations.</span></pre><h4 name="fb98" id="fb98" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Trick 4: Chain of Thought</strong></h4><p name="2b2d" id="2b2d" class="graf graf--p graf-after--h4">This trick was proposed by Wei et al. [7]. The basic idea is to guide an LLM to think “step by step”. This helps break down complex problems into manageable sub-problems, which gives the LLM “time to think” [3,5]. Zhang et al. showed that this could be as simple as including the text “<em class="markup--em markup--p-em">Let’s think step by step</em>” in the prompt [8].</p><p name="73dd" id="73dd" class="graf graf--p graf-after--p">This notion can be extended to any recipe-like process. For example, if I want to create a LinkedIn post based on my latest Medium blog, I can guide the LLM to mirror the step-by-step process I follow.</p><p name="a18f" id="a18f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Without Trick</em></strong></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="markdown" name="ddaf" id="ddaf" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Write me a LinkedIn post based on the following Medium blog.<br /><br />Medium blog: {Medium blog text}</span></pre><p name="42a1" id="42a1" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">With Trick</em></strong></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="markdown" name="db33" id="db33" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Write me a LinkedIn post based on the step-by-step process and Medium blog \<br />given below.  <br /><br />Step 1: Come up with a one line hook relevant to the blog. <br />Step 2: Extract 3 key points from the article <br />Step 3: Compress each point to less than 50 characters. <br />Step 4: Combine the hook, compressed key points from Step 3, and a call to action \<br />to generate the final output.<br /><br />Medium blog: {Medium blog text}</span></pre><h4 name="de6e" id="de6e" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Trick 5: Chatbot Personas</strong></h4><p name="c31f" id="c31f" class="graf graf--p graf-after--h4">A somewhat surprising technique that tends to improve LLM performance is to prompt it to take on a particular persona e.g. “<em class="markup--em markup--p-em">you are an expert</em>”. This is helpful because you may not know the best way to describe your problem to the LLM, but you may know who would help you solve that problem [<a href="https://arxiv.org/abs/2302.11382" data-href="https://arxiv.org/abs/2302.11382" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">1</a>]. Here’s what this might look like in practice.</p><p name="907d" id="907d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Without Trick</em></strong></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="markdown" name="8aac" id="8aac" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Make me a travel itinerary for a weekend in New York City.</span></pre><p name="fa4b" id="fa4b" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">With Trick</em></strong></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="markdown" name="e589" id="e589" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Act as an NYC native and cabbie who knows everything about the city. \<br />Please make me a travel itinerary for a weekend in New York City based on \<br />your experience. Don&#x27;t forget to include your charming NY accent in your \<br />response.</span></pre><h4 name="364a" id="364a" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Trick 6: Flipped Approach</strong></h4><p name="8bab" id="8bab" class="graf graf--p graf-after--h4">It can be difficult to optimally prompt an LLM when <strong class="markup--strong markup--p-strong">we do not know what it knows or how it thinks</strong>. That is where the “flipped approach” can be helpful. This is where you prompt the LLM to ask you questions until it has a sufficient understanding (i.e. context) of the problem you are trying to solve.</p><p name="4f20" id="4f20" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Without Trick</em></strong></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="markdown" name="e6a0" id="e6a0" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">What is an idea for an LLM-based application?</span></pre><p name="5a1e" id="5a1e" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">With Trick</em></strong></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="markdown" name="9105" id="9105" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">I want you to ask me questions to help me come up with an LLM-based \<br />application idea. Ask me one question at a time to keep things conversational.</span></pre><h4 name="b314" id="b314" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Trick 7: Reflect, Review, and Refine</strong></h4><p name="bf8a" id="bf8a" class="graf graf--p graf-after--h4">This final trick prompts the model to reflect on its past responses to improve them. Common use cases are having the model critically evaluate its own work by asking it if it “<em class="markup--em markup--p-em">completed the assignment</em>” or having it “<em class="markup--em markup--p-em">explain the reasoning and assumptions</em>” behind a response [1, 3].</p><p name="3a08" id="3a08" class="graf graf--p graf-after--p">Additionally, you can ask the LLM to refine not only its responses but <strong class="markup--strong markup--p-strong">your prompts</strong>. This is a simple way to automatically rewrite prompts so that they are easier for the model to “understand”.</p><p name="e208" id="e208" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">With Trick</em></strong></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="markdown" name="a0e0" id="a0e0" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Review your previous response, pinpoint areas for enhancement, and offer an \<br />improved version. Then explain your reasoning for how you improved the response.</span></pre><h3 name="bb1a" id="bb1a" class="graf graf--h3 graf-after--pre"><strong class="markup--strong markup--h3-strong">Example Code: Automatic Grader with LangChain</strong></h3><p name="4449" id="4449" class="graf graf--p graf-after--h3">Now that we’ve reviewed several prompting heuristics let’s see how we can apply them to a specific use case. To do this, we will return to the automatic grader example from before.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="markdown" name="0d30" id="0d30" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">You are a high school history teacher grading homework assignments. \<br />Based on the homework question indicated by &quot;Q:&quot; and the correct answer \<br />indicated by &quot;A:&quot;, your task is to determine whether the student&#x27;s answer is \<br />correct.<br />Grading is binary; therefore, student answers can be correct or wrong.<br />Simple misspellings are okay.<br /> <br />Q: {question}<br />A: {correct<span class="hljs-emphasis">_answer}<br /> <br />Student Answer: {student_</span>answer}</span></pre><p name="8b66" id="8b66" class="graf graf--p graf-after--pre">On second look, a few of the previously mentioned tricks should be apparent i.e. <strong class="markup--strong markup--p-strong">Trick 6</strong>: chatbot persona, <strong class="markup--strong markup--p-strong">Trick 3</strong>: use structured text, and <strong class="markup--strong markup--p-strong">Trick 1</strong>: be descriptive. This is what good prompting typically looks like in practice, namely combining multiple techniques in a single prompt.</p><p name="68cf" id="68cf" class="graf graf--p graf-after--p">While we could copy-paste this prompt template into ChatGPT and replace the <em class="markup--em markup--p-em">question</em>, <em class="markup--em markup--p-em">correct_answer</em>, and <em class="markup--em markup--p-em">student_answer </em>fields, <strong class="markup--strong markup--p-strong">this is not a scalable way to implement the automatic grader</strong>. Rather, what we want is to integrate this prompt into a larger software system so that we can build a user-friendly application that a human can use.</p><h4 name="2e15" id="2e15" class="graf graf--h4 graf-after--p">LangChain</h4><p name="b532" id="b532" class="graf graf--p graf-after--h4">One way we can do this is via <strong class="markup--strong markup--p-strong">LangChain</strong>, which is <strong class="markup--strong markup--p-strong">a Python library that helps simplify building applications on top of large language models</strong>. It does this by providing a variety of handy abstractions for using LLMs programmatically.</p><p name="93e5" id="93e5" class="graf graf--p graf-after--p">The central class that does this is called <strong class="markup--strong markup--p-strong">chain</strong> (hence the library name). This abstracts the process of generating a prompt, sending it to an LLM, and parsing the output so that it can be easily called and integrated into a larger script.</p><p name="9fda" id="9fda" class="graf graf--p graf-after--p">Let’s see how to use LangChain for our automatic grader use case. The example code is available on the <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/langchain-example" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/langchain-example" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub Repo</a> for this article.</p><h4 name="1830" id="1830" class="graf graf--h4 graf-after--p">Imports</h4><p name="07cf" id="07cf" class="graf graf--p graf-after--h4">We first start by importing the necessary library modules.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="089d" id="089d" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> langchain.chat_models <span class="hljs-keyword">import</span> ChatOpenAI<br /><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate<br /><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> LLMChain<br /><span class="hljs-keyword">from</span> langchain.schema <span class="hljs-keyword">import</span> BaseOutputParser</span></pre><p name="3acf" id="3acf" class="graf graf--p graf-after--pre">Here we will use gpt-3.5-turbo which requires a secret key for OpenAI’s API. If you don’t have one, I gave a step-by-step guide on how to get one in a <a href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971" data-href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">past article</a> of this series. I like to store the secret key in a separate Python file (<em class="markup--em markup--p-em">sk.py</em>) and import it with the following line of code.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="d22c" id="d22c" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> sk <span class="hljs-keyword">import</span> my_sk <span class="hljs-comment">#importing secret key from another python file</span></span></pre><h4 name="ef3e" id="ef3e" class="graf graf--h4 graf-after--pre">Our 1st chain</h4><p name="6cba" id="6cba" class="graf graf--p graf-after--h4">To define our chain, we need two core elements: the <strong class="markup--strong markup--p-strong">LLM</strong> and the <strong class="markup--strong markup--p-strong">prompt</strong>. We start by creating an object for the LLM.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="56cf" id="56cf" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># define LLM object</span><br />chat_model = ChatOpenAI(openai_api_key=my_sk, temperature=<span class="hljs-number">0</span>)</span></pre><p name="a4f0" id="a4f0" class="graf graf--p graf-after--pre">LangChain has a class specifically for OpenAI (and many other) chat models. I pass in my secret API key and set the temperature to 0. The default model here is <em class="markup--em markup--p-em">gpt-3.5-turbo</em>, but you can alternatively use <em class="markup--em markup--p-em">gpt-4</em> using the “model_name” input argument. You can further customize the chat model by setting other <a href="https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html#langchain.chat_models.openai.ChatOpenAI" data-href="https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html#langchain.chat_models.openai.ChatOpenAI" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">input arguments</a>.</p><p name="96b6" id="96b6" class="graf graf--p graf-after--p">Next, we define our <strong class="markup--strong markup--p-strong">prompt template</strong>. This object allows us to generate prompts dynamically via input strings that automatically update a base template. Here’s what that looks like.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="af04" id="af04" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># define prompt template</span><br />prompt_template_text = <span class="hljs-string">&quot;&quot;&quot;You are a high school history teacher grading \<br />homework assignments. Based on the homework question indicated by “**Q:**” \<br />and the correct answer indicated by “**A:**”, your task is to determine \<br />whether the student&#x27;s answer is correct. Grading is binary; therefore, \<br />student answers can be correct or wrong. Simple misspellings are okay.<br /><br />**Q:** {question}<br />**A:** {correct_answer}<br /><br />**Student&#x27;s Answer:** {student_answer}<br />&quot;&quot;&quot;</span><br /><br />prompt = PromptTemplate(<br />            input_variables=[<span class="hljs-string">&quot;question&quot;</span>, <span class="hljs-string">&quot;correct_answer&quot;</span>, <span class="hljs-string">&quot;student_answer&quot;</span>], \<br />            template = prompt_template_text)</span></pre><p name="9efa" id="9efa" class="graf graf--p graf-after--pre">With our LLM and prompt, we can now define our chain.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="85a2" id="85a2" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># define chain</span><br />chain = LLMChain(llm=chat_model, prompt=prompt)</span></pre><p name="7c53" id="7c53" class="graf graf--p graf-after--pre">Next, we can pass inputs to the chain and obtain a grade in one line of code.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="9e7c" id="9e7c" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># define inputs</span><br />question = <span class="hljs-string">&quot;Who was the 35th president of the United States of America?&quot;</span><br />correct_answer = <span class="hljs-string">&quot;John F. Kennedy&quot;</span><br />student_answer =  <span class="hljs-string">&quot;FDR&quot;</span><br /><br /><span class="hljs-comment"># run chain</span><br />chain.run({<span class="hljs-string">&#x27;question&#x27;</span>:question, <span class="hljs-string">&#x27;correct_answer&#x27;</span>:correct_answer, \<br />    <span class="hljs-string">&#x27;student_answer&#x27;</span>:student_answer})<br /><br /><span class="hljs-comment"># output: Student&#x27;s Answer is wrong. </span></span></pre><p name="107e" id="107e" class="graf graf--p graf-after--pre">While this chain can perform the grading task effectively, its outputs may not be suitable for an automated process. For instance, in the above code block, the LLM correctly said the student’s answer of “FDR” was wrong, but it would be better if the LLM gave us an output in a standard format that could be used in downstream processing.</p><h4 name="7d90" id="7d90" class="graf graf--h4 graf-after--p">Output parser</h4><p name="0da7" id="0da7" class="graf graf--p graf-after--h4">This is where <strong class="markup--strong markup--p-strong">output parsers</strong> come in handy. These are functions we can integrate into a chain to convert LLM outputs to a standard format. Let’s see how we can make an output parser that converts the LLM response to a boolean (i.e. <em class="markup--em markup--p-em">True</em> or <em class="markup--em markup--p-em">False</em>) output.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="7216" id="7216" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># define output parser</span><br /><span class="hljs-keyword">class</span> <span class="hljs-title class_">GradeOutputParser</span>(<span class="hljs-title class_ inherited__">BaseOutputParser</span>):<br />    <span class="hljs-string">&quot;&quot;&quot;Determine whether grade was correct or wrong&quot;&quot;&quot;</span><br /><br />    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, text: <span class="hljs-built_in">str</span></span>):<br />        <span class="hljs-string">&quot;&quot;&quot;Parse the output of an LLM call.&quot;&quot;&quot;</span><br />        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;wrong&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> text.lower()</span></pre><p name="e495" id="e495" class="graf graf--p graf-after--pre">Here, we create a simple output parser that checks if the word “wrong” is in the LLM’s output. If not, we return <em class="markup--em markup--p-em">True</em>, indicating the student&#39;s correct answer. Otherwise, we return <em class="markup--em markup--p-em">False</em>, indicating the student&#39;s answer was incorrect.</p><p name="3748" id="3748" class="graf graf--p graf-after--p">We can then incorporate this output parser into our chain to seamlessly parse text when we run the chain.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="ba5f" id="ba5f" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># update chain</span><br />chain = LLMChain(<br />    llm=chat_model,<br />    prompt=prompt,<br />    output_parser=GradeOutputParser()<br />)</span></pre><p name="b2ae" id="b2ae" class="graf graf--p graf-after--pre">Finally, we can run the chain for a whole list of student answers and print the outputs.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="321d" id="321d" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># run chain in for loop</span><br />student_answer_list = [<span class="hljs-string">&quot;John F. Kennedy&quot;</span>, <span class="hljs-string">&quot;JFK&quot;</span>, <span class="hljs-string">&quot;FDR&quot;</span>, <span class="hljs-string">&quot;John F. Kenedy&quot;</span>, \<br />                  <span class="hljs-string">&quot;John Kennedy&quot;</span>, <span class="hljs-string">&quot;Jack Kennedy&quot;</span>, <span class="hljs-string">&quot;Jacquelin Kennedy&quot;</span>, \<br />                  <span class="hljs-string">&quot;Robert F. Kenedy&quot;</span>]<br /><br /><span class="hljs-keyword">for</span> student_answer <span class="hljs-keyword">in</span> student_answer_list:<br />    <span class="hljs-built_in">print</span>(student_answer + <span class="hljs-string">&quot; - &quot;</span> + <br />      <span class="hljs-built_in">str</span>(chain.run({<span class="hljs-string">&#x27;question&#x27;</span>:question, <span class="hljs-string">&#x27;correct_answer&#x27;</span>:correct_answer, \<br />                    <span class="hljs-string">&#x27;student_answer&#x27;</span>:student_answer})))<br />    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#x27;</span>)<br /><br /><span class="hljs-comment"># Output:</span><br /><span class="hljs-comment"># John F. Kennedy - True</span><br /><span class="hljs-comment"># JFK - True</span><br /><span class="hljs-comment"># FDR - False</span><br /><span class="hljs-comment"># John F. Kenedy - True</span><br /><span class="hljs-comment"># John Kennedy - True</span><br /><span class="hljs-comment"># Jack Kennedy - True</span><br /><span class="hljs-comment"># Jacqueline Kennedy - False</span><br /><span class="hljs-comment"># Robert F. Kenedy - False</span></span></pre><div name="8c1a" id="8c1a" class="graf graf--mixtapeEmbed graf-after--pre"><a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/langchain-example" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/langchain-example" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/langchain-example"><strong class="markup--strong markup--mixtapeEmbed-strong">YouTube-Blog/LLMs/langchain-example at main · ShawhinT/YouTube-Blog</strong><br><em class="markup--em markup--mixtapeEmbed-em">Codes to complement YouTube videos and blog posts on Medium. - YouTube-Blog/LLMs/langchain-example at main ·…</em>github.com</a><a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/langchain-example" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="f34ee7128be5d1652b71f065aba2b4f6" data-thumbnail-img-id="0*kVhUeuQQM49LsKjO" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*kVhUeuQQM49LsKjO);"></a></div><h3 name="206b" id="206b" class="graf graf--h3 graf-after--mixtapeEmbed">Limitations</h3><p name="3aaf" id="3aaf" class="graf graf--p graf-after--h3">Prompt Engineering is more than asking ChatGPT for help writing an email or learning about Quantum Computing. It is a <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">new programming paradigm that changes how developers can build applications</em></strong>.</p><p name="06a2" id="06a2" class="graf graf--p graf-after--p">While this is a powerful innovation, it has its limitations. For one, optimal prompting strategies are LLM-dependent. For example, prompting GPT-3 to “think step-by-step” resulted in significant performance gains on simple mathematical reasoning tasks [8]. However, for the latest version of ChatGPT, the same strategy doesn’t seem helpful (it already thinks step-by-step).</p><p name="f4b8" id="f4b8" class="graf graf--p graf-after--p">Another limitation of Prompt Engineering is it requires large-scale general-purpose language models such as ChatGPT, which come at significant computational and financial costs. This may be overkill for many use cases that are more narrowly defined e.g. string matching, sentiment analysis, or text summarization.</p><p name="87a1" id="87a1" class="graf graf--p graf-after--p">We can overcome both these limitations via <strong class="markup--strong markup--p-strong">fine-tuning </strong>pre-trained language models. This is where we <strong class="markup--strong markup--p-strong">take an existing language model and tweak it for a particular use case. </strong>In the <a href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--p-anchor" target="_blank">next article</a> of this series, we will explore popular fine-tuning techniques supplemented with example Python code.</p><p name="0caf" id="0caf" class="graf graf--p graf-after--p">👉 <strong class="markup--strong markup--p-strong">More on LLMs</strong>: <a href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" data-href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Introduction</a> | <a href="https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971" data-href="https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971" class="markup--anchor markup--p-anchor" target="_blank">OpenAI API</a> | <a href="https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" data-href="https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Hugging Face Transformers</a> | <a href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--p-anchor" target="_blank">Fine-tuning</a> | <a href="https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9" data-href="https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Build an LLM</a> | <a href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" data-href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">QLoRA</a> | <a href="https://towardsdatascience.com/how-to-improve-llms-with-rag-abdc132f76ac" data-href="https://towardsdatascience.com/how-to-improve-llms-with-rag-abdc132f76ac" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">RAG</a> | <a href="https://towardsdatascience.com/text-embeddings-classification-and-semantic-search-8291746220be" data-href="https://towardsdatascience.com/text-embeddings-classification-and-semantic-search-8291746220be" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Text Embeddings</a></p><div name="d15f" id="d15f" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/list/8e009ae3054c" data-href="https://shawhin.medium.com/list/8e009ae3054c" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/list/8e009ae3054c"><strong class="markup--strong markup--mixtapeEmbed-strong">Large Language Models (LLMs)</strong><br><em class="markup--em markup--mixtapeEmbed-em">A series on how to use LLMs in practice.</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/list/8e009ae3054c" class="js-mixtapeImage mixtapeImage mixtapeImage--mediumCatalog  u-ignoreBlock" data-media-id="e38ca2d0de6aa61d7c09af603d8f43e1" data-thumbnail-img-id="0*ae40ceaca4c0bdcbdf431a66c777e68bffc1e356.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/304/160/0*ae40ceaca4c0bdcbdf431a66c777e68bffc1e356.jpeg);"></a></div></div></div></section><section name="840b" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="adb9" id="adb9" class="graf graf--h3 graf--leading">Resources</h3><p name="161b" id="161b" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Connect</strong>: <a href="https://shawhintalebi.com/" data-href="https://shawhintalebi.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">My website</a> | <a href="https://calendly.com/shawhintalebi" data-href="https://calendly.com/shawhintalebi" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Book a call</a> | <a href="https://shawhintalebi.com/contact/" data-href="https://shawhintalebi.com/contact/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Ask me anything</a></p><p name="984e" id="984e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Socials</strong>: <a href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" data-href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">YouTube 🎥</a> | <a href="https://www.linkedin.com/in/shawhintalebi/" data-href="https://www.linkedin.com/in/shawhintalebi/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">LinkedIn</a> | <a href="https://twitter.com/ShawhinT" data-href="https://twitter.com/ShawhinT" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Twitter</a></p><p name="a2c5" id="a2c5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Support</strong>: <a href="https://www.buymeacoffee.com/shawhint" data-href="https://www.buymeacoffee.com/shawhint" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Buy me a coffee</a> ☕️</p><div name="bb8c" id="bb8c" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/subscribe" data-href="https://shawhin.medium.com/subscribe" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/subscribe"><strong class="markup--strong markup--mixtapeEmbed-strong">Get FREE access to every new story I write</strong><br><em class="markup--em markup--mixtapeEmbed-em">Get FREE access to every new story I write P.S. I do not share your email with anyone By signing up, you will create a…</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/subscribe" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8d6d6c2d301f2c367b618a850fa468b3" data-thumbnail-img-id="0*O69FxQ5FttNEA_r4" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*O69FxQ5FttNEA_r4);"></a></div></div></div></section><section name="cbc0" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="81b3" id="81b3" class="graf graf--p graf--leading">[1] <a href="https://arxiv.org/abs/2302.11382" data-href="https://arxiv.org/abs/2302.11382" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2302.11382</a><strong class="markup--strong markup--p-strong"> [cs.SE]</strong></p><p name="c824" id="c824" class="graf graf--p graf-after--p">[2] <a href="https://arxiv.org/abs/2106.09685" data-href="https://arxiv.org/abs/2106.09685" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2106.09685</a><strong class="markup--strong markup--p-strong"> [cs.CL]</strong></p><p name="d860" id="d860" class="graf graf--p graf-after--p">[3] <a href="https://www.youtube.com/watch?v=bZQun8Y4L2A" data-href="https://www.youtube.com/watch?v=bZQun8Y4L2A" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">State of GPT</a> by <a href="https://medium.com/u/ac9d9a35533e" data-href="https://medium.com/u/ac9d9a35533e" data-anchor-type="2" data-user-id="ac9d9a35533e" data-action-value="ac9d9a35533e" data-action="show-user-card" data-action-type="hover" class="markup--user markup--p-user" target="_blank">Andrej Karpathy</a> at Microsoft Build 2023</p><p name="84a8" id="84a8" class="graf graf--p graf-after--p">[4] <a href="https://arxiv.org/abs/2206.07682" data-href="https://arxiv.org/abs/2206.07682" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2206.07682</a><strong class="markup--strong markup--p-strong"> [cs.CL]</strong></p><p name="a7b2" id="a7b2" class="graf graf--p graf-after--p">[5] <a href="https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/" data-href="https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ChatGPT Prompt Engineering for Developers</a> by deeplearning.ai</p><p name="934c" id="934c" class="graf graf--p graf-after--p">[6] <a href="https://arxiv.org/abs/2005.14165" data-href="https://arxiv.org/abs/2005.14165" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2005.14165</a><strong class="markup--strong markup--p-strong"> [cs.CL]</strong></p><p name="1d26" id="1d26" class="graf graf--p graf-after--p">[7] <a href="https://arxiv.org/abs/2201.11903" data-href="https://arxiv.org/abs/2201.11903" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2201.11903</a><strong class="markup--strong markup--p-strong"> [cs.CL]</strong></p><p name="2973" id="2973" class="graf graf--p graf-after--p graf--trailing">[8] <a href="https://arxiv.org/abs/2210.03493" data-href="https://arxiv.org/abs/2210.03493" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2210.03493</a><strong class="markup--strong markup--p-strong"> [cs.CL]</strong></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@shawhin" class="p-author h-card">Shaw Talebi</a> on <a href="https://medium.com/p/7ce1ed3b553f"><time class="dt-published" datetime="2023-08-25T06:41:41.193Z">August 25, 2023</time></a>.</p><p><a href="https://medium.com/@shawhin/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 2, 2024.</p></footer></article></body></html>