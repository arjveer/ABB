<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>How to Improve LLMs with RAG</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">How to Improve LLMs with RAG</h1>
</header>
<section data-field="subtitle" class="p-summary">
A beginner-friendly introduction w/ Python code
</section>
<section data-field="body" class="e-content">
<section name="74ee" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="24be" id="24be" class="graf graf--h3 graf--leading graf--title">How to Improve LLMs with¬†RAG</h3><h4 name="4eb4" id="4eb4" class="graf graf--h4 graf-after--h3 graf--subtitle">A beginner-friendly introduction w/ Python¬†code</h4><p name="dfcf" id="dfcf" class="graf graf--p graf-after--h4">This article is part of a <a href="https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c" data-href="https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">larger series</a> on using large language models in practice. In the <a href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" data-href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">previous post</a>, we fine-tuned Mistral-7b-Instruct to respond to YouTube comments using QLoRA. Although the fine-tuned model successfully captured my style when responding to viewer feedback, its responses to technical questions didn‚Äôt match my explanations. Here, I‚Äôll discuss how we can improve LLM performance using retrieval augmented generation (i.e. RAG).</p><figure name="2ec4" id="2ec4" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="1*N0Ad_oCIrAyzMYRdH3trqg.png" data-width="1280" data-height="720" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*N0Ad_oCIrAyzMYRdH3trqg.png"><figcaption class="imageCaption">The original RAG system. Image from¬†Canva.</figcaption></figure></div></div></section><section name="859a" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="9a6b" id="9a6b" class="graf graf--p graf--leading">Large language models (LLMs) have demonstrated an impressive ability to store and deploy vast knowledge in response to user queries. While this has enabled the creation of powerful AI systems like ChatGPT, compressing world knowledge in this way has <strong class="markup--strong markup--p-strong">two key limitations</strong>.</p><p name="cb37" id="cb37" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">First</strong>, an LLM‚Äôs knowledge is static, i.e., not updated as new information becomes available. <strong class="markup--strong markup--p-strong">Second</strong>, LLMs may have an insufficient ‚Äúunderstanding‚Äù of niche and specialized information that was not prominent in their training data. These limitations can result in undesirable (and even fictional) model responses to user queries.</p><p name="ecc4" id="ecc4" class="graf graf--p graf-after--p">One way we can mitigate these limitations is to <strong class="markup--strong markup--p-strong">augment a model via a specialized and mutable knowledge base</strong>, e.g., customer FAQs, software documentation, or product catalogs. This enables the creation of more robust and adaptable AI systems.</p><p name="2f6f" id="2f6f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Retrieval augmented generation</strong>, or <strong class="markup--strong markup--p-strong">RAG</strong>, is one such approach. Here, I provide a high-level introduction to RAG and share example Python code for implementing a RAG system using LlamaIndex.</p><figure name="81d7" id="81d7" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/Ylz779Op9Pw?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><h3 name="14c3" id="14c3" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">What is¬†RAG?</strong></h3><p name="a5de" id="a5de" class="graf graf--p graf-after--h3">The basic usage of an LLM consists of giving it a prompt and getting back a response.</p><figure name="5bf6" id="5bf6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*sM1p-3FoTaGZunqx918G9A.png" data-width="677" data-height="245" src="https://cdn-images-1.medium.com/max/800/1*sM1p-3FoTaGZunqx918G9A.png"><figcaption class="imageCaption">Basic usage of an LLM i.e. prompt in, response out. Image by¬†author.</figcaption></figure><p name="a0c3" id="a0c3" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">RAG works by adding a step to this basic process</strong>. Namely, a retrieval step is performed where, based on the user‚Äôs prompt, the relevant information is extracted from an external knowledge base and injected into the prompt before being passed to the LLM.</p><figure name="6047" id="6047" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*EhJZj1blu7a8EPmVAPsNcA.png" data-width="1746" data-height="440" src="https://cdn-images-1.medium.com/max/800/1*EhJZj1blu7a8EPmVAPsNcA.png"><figcaption class="imageCaption">Overview of RAG system. Image by¬†author.</figcaption></figure><h3 name="fd7f" id="fd7f" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">Why we¬†care</strong></h3><p name="2111" id="2111" class="graf graf--p graf-after--h3">Notice that RAG does not fundamentally change how we use an LLM; it&#39;s still <em class="markup--em markup--p-em">prompt-in and response-out</em>. RAG simply augments this process (hence the name).</p><p name="2924" id="2924" class="graf graf--p graf-after--p">This makes <strong class="markup--strong markup--p-strong">RAG a flexible and (relatively) straightforward way to improve LLM-based systems</strong>. Additionally, since knowledge is stored in an external database, updating system knowledge is as simple as adding or removing records from a table.</p><h4 name="b439" id="b439" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Why not fine-tune?</strong></h4><p name="764d" id="764d" class="graf graf--p graf-after--h4">Previous articles in this series discussed <a href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">fine-tuning</a>, which adapts an existing model for a particular use case. While this is an alternative way to endow an LLM with specialized knowledge, empirically, <strong class="markup--strong markup--p-strong">fine-tuning seems to be less effective than RAG</strong> <strong class="markup--strong markup--p-strong">at doing this</strong> [1].</p><h3 name="9344" id="9344" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">How it¬†works</strong></h3><p name="7ed2" id="7ed2" class="graf graf--p graf-after--h3">There are 2 key elements of a RAG system: a <strong class="markup--strong markup--p-strong">retriever</strong> and a <strong class="markup--strong markup--p-strong">knowledge base</strong>.</p><h4 name="3934" id="3934" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Retriever</strong></h4><p name="2269" id="2269" class="graf graf--p graf-after--h4">A retriever takes a user prompt and returns relevant items from a knowledge base. This typically works using so-called <strong class="markup--strong markup--p-strong">text embeddings</strong>, numerical representations of text in concept space. In other words, these are <strong class="markup--strong markup--p-strong">numbers that represent the <em class="markup--em markup--p-em">meaning</em> of a given text</strong>.</p><p name="b435" id="b435" class="graf graf--p graf-after--p">Text embeddings can be used to compute a similarity score between the user‚Äôs query and each item in the knowledge base. The result of this process is a <strong class="markup--strong markup--p-strong">ranking of each item‚Äôs relevance to the input query</strong>.</p><p name="e6b8" id="e6b8" class="graf graf--p graf-after--p">The retriever can then take the top k (say k=3) most relevant items and inject them into the user prompt. This augmented prompt is then passed into the LLM for generation.</p><figure name="fe24" id="fe24" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jpTwdBmoTlJlfPAm0oJiVQ.png" data-width="1020" data-height="522" src="https://cdn-images-1.medium.com/max/800/1*jpTwdBmoTlJlfPAm0oJiVQ.png"><figcaption class="imageCaption">Overview of retrieval step. Image by¬†author.</figcaption></figure><h4 name="14ff" id="14ff" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Knowledge Base</strong></h4><p name="ac37" id="ac37" class="graf graf--p graf-after--h4">The next key element of a RAG system is a knowledge base. This <strong class="markup--strong markup--p-strong">houses all the information you want to make available to the LLM</strong>. While there are countless ways to construct a knowledge base for RAG, here I‚Äôll focus on building one from a set of documents.</p><p name="174b" id="174b" class="graf graf--p graf-after--p">The process can be broken down into <strong class="markup--strong markup--p-strong">4 key steps</strong> [2,3].</p><ol class="postList"><li name="effa" id="effa" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Load docs</strong>‚Ää‚Äî‚ÄäThis consists of gathering a collection of documents and ensuring they are in a ready-to-parse format (more on this later).</li><li name="e037" id="e037" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Chunk docs‚Äî</strong>Since LLMs have limited context windows, documents must be split into smaller chunks<strong class="markup--strong markup--li-strong"> (e.g.,</strong> 256 or 512 characters long).</li><li name="5fc4" id="5fc4" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Embed chunks</strong>‚Ää‚Äî‚ÄäTranslate each chunk into numbers using a text embedding model.</li><li name="7e7c" id="7e7c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Load into Vector DB</strong>‚Äî Load text embeddings into a database (aka a vector database).</li></ol><figure name="4078" id="4078" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*VWG6Tr0OxCnD5Mvygm5DCA.png" data-width="1033" data-height="260" src="https://cdn-images-1.medium.com/max/800/1*VWG6Tr0OxCnD5Mvygm5DCA.png"><figcaption class="imageCaption">Overview of knowledge base creation. Image by¬†author.</figcaption></figure><h3 name="bf88" id="bf88" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">Some Nuances</strong></h3><p name="e571" id="e571" class="graf graf--p graf-after--h3">While the steps for building a RAG system are conceptually simple, several nuances can make building one (in the real world) more complicated.</p><p name="9c2b" id="9c2b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Document preparation</strong>‚ÄîThe quality of a RAG system is driven by how well useful information can be extracted from source documents. For example, if a document is unformatted and full of images and tables, it will be more difficult to parse than a well-formatted text file.</p><p name="28a5" id="28a5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Choosing the right chunk size</strong>‚ÄîWe already mentioned the need for chunking due to LLM context windows. However, there are 2 additional motivations for chunking.</p><p name="3312" id="3312" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">First</strong>, it keeps (compute) costs down. The more text you inject into the prompt, the more compute required to generate a completion. The <strong class="markup--strong markup--p-strong">second</strong> is performance. Relevant information for a particular query tends to be localized in source documents (often, just 1 sentence can answer a question). Chunking helps minimize the amount of irrelevant information passed into the model [4].</p><p name="f290" id="f290" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Improving search</strong>‚Ää‚Äî‚ÄäWhile text embeddings enable a powerful and fast way to do search, it doesn‚Äôt always work as one might hope. In other words, it may return results that are ‚Äúsimilar‚Äù to the user query, yet not helpful for answering it, e.g., ‚Äú<em class="markup--em markup--p-em">How‚Äôs the weather in LA?</em>‚Äù may return ‚Äú<em class="markup--em markup--p-em">How‚Äôs the weather in NYC?</em>‚Äù.</p><p name="3ba3" id="3ba3" class="graf graf--p graf-after--p">The simplest way to mitigate this is through good document preparation and chunking. However, for some use cases, additional strategies for improving search might be necessary, such as using <strong class="markup--strong markup--p-strong">meta-tags</strong> for each chunk, employing <strong class="markup--strong markup--p-strong">hybrid search</strong>, which combines keyword‚Äîand embedding-based search, or using a <strong class="markup--strong markup--p-strong">reranker</strong>, which is a specialized model that computes the similarity of 2 input pieces of text.</p><h3 name="f021" id="f021" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Example code: Improving YouTube Comment Responder with¬†RAG</strong></h3><p name="eefe" id="eefe" class="graf graf--p graf-after--h3">With a basic understanding of how RAG works, let‚Äôs see how to use it in practice. I will build upon the example from the <a href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" data-href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">previous article</a>, where I fine-tuned Mistral-7B-Instruct to respond to YouTube comments using QLoRA. We will use LlamaIndex to add a RAG system to the fine-tuned model from before.</p><p name="d36a" id="d36a" class="graf graf--p graf-after--p">The example code is freely available in a <a href="https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing" data-href="https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Colab Notebook</a>, which can run on the (free) T4 GPU provided. The source files for this example are available at the <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub repository</a>.</p><p name="e3ca" id="e3ca" class="graf graf--p graf-after--p graf--trailing">üîó <a href="https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing" data-href="https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Google Colab</a> | <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub Repo</a></p></div></div></section><section name="c3f6" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h4 name="e569" id="e569" class="graf graf--h4 graf--leading"><strong class="markup--strong markup--h4-strong">Imports</strong></h4><p name="9393" id="9393" class="graf graf--p graf-after--h4">We start by installing and importing necessary Python libraries.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="4090" id="4090" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">!pip install llama-index<br />!pip install llama-index-embeddings-huggingface<br />!pip install peft<br />!pip install auto-gptq<br />!pip install optimum<br />!pip install bitsandbytes<br /><span class="hljs-comment"># if not running on Colab ensure transformers is installed too</span></span></pre><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="2cef" id="2cef" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> llama_index.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbedding<br /><span class="hljs-keyword">from</span> llama_index.core <span class="hljs-keyword">import</span> Settings, SimpleDirectoryReader, VectorStoreIndex<br /><span class="hljs-keyword">from</span> llama_index.core.retrievers <span class="hljs-keyword">import</span> VectorIndexRetriever<br /><span class="hljs-keyword">from</span> llama_index.core.query_engine <span class="hljs-keyword">import</span> RetrieverQueryEngine<br /><span class="hljs-keyword">from</span> llama_index.core.postprocessor <span class="hljs-keyword">import</span> SimilarityPostprocessor</span></pre><h4 name="81a3" id="81a3" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Setting up Knowledge Base</strong></h4><p name="e0ab" id="e0ab" class="graf graf--p graf-after--h4">We can configure our knowledge base by defining our embedding model, chunk size, and chunk overlap. Here, we use the ~33M parameter <a href="https://huggingface.co/BAAI/bge-small-en-v1.5" data-href="https://huggingface.co/BAAI/bge-small-en-v1.5" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">bge-small-en-v1.5</a> embedding model from BAAI, which is available on the Hugging Face hub. Other embedding model options are available on this <a href="https://huggingface.co/spaces/mteb/leaderboard" data-href="https://huggingface.co/spaces/mteb/leaderboard" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">text embedding leaderboard</a>.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="5f00" id="5f00" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># import any embedding model on HF hub</span><br />Settings.embed_model = HuggingFaceEmbedding(model_name=<span class="hljs-string">&quot;BAAI/bge-small-en-v1.5&quot;</span>)<br /><br />Settings.llm = <span class="hljs-literal">None</span> <span class="hljs-comment"># we won&#x27;t use LlamaIndex to set up LLM</span><br />Settings.chunk_size = <span class="hljs-number">256</span><br />Settings.chunk_overlap = <span class="hljs-number">25</span></span></pre><p name="95c6" id="95c6" class="graf graf--p graf-after--pre">Next, we load our source documents. Here, I have a folder called ‚Äú<a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag/articles" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag/articles" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">articles</em></a>,‚Äù which contains PDF versions of 3 Medium articles I wrote on <a href="https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a" data-href="https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">fat tails</a>. If running this in Colab, you must download the articles folder from the <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub repo</a> and manually upload it to your Colab environment.</p><p name="04be" id="04be" class="graf graf--p graf-after--p">For each file in this folder, the function below will read the text from the PDF, split it into chunks (based on the settings defined earlier), and store each chunk in a list called <em class="markup--em markup--p-em">documents</em>.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="7297" id="7297" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">documents = SimpleDirectoryReader(<span class="hljs-string">&quot;articles&quot;</span>).load_data()</span></pre><p name="cbf9" id="cbf9" class="graf graf--p graf-after--pre">Since the blogs were downloaded directly as PDFs from Medium, they resemble a webpage more than a well-formatted article. Therefore, some chunks may include text unrelated to the article, e.g., webpage headers and Medium article recommendations.</p><p name="b362" id="b362" class="graf graf--p graf-after--p">In the code block below, I refine the chunks in documents, removing most of the chunks before or after the meat of an article.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="bf34" id="bf34" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(documents)) <span class="hljs-comment"># prints: 71</span><br /><span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> documents:<br />    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;Member-only story&quot;</span> <span class="hljs-keyword">in</span> doc.text:<br />        documents.remove(doc)<br />        <span class="hljs-keyword">continue</span><br /><br />    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;The Data Entrepreneurs&quot;</span> <span class="hljs-keyword">in</span> doc.text:<br />        documents.remove(doc)<br /><br />    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot; min read&quot;</span> <span class="hljs-keyword">in</span> doc.text:<br />        documents.remove(doc)<br /><br /><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(documents)) <span class="hljs-comment"># prints: 61</span></span></pre><p name="add7" id="add7" class="graf graf--p graf-after--pre">Finally, we can store the refined chunks in a vector database.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="bb53" id="bb53" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">index = VectorStoreIndex.from_documents(documents)</span></pre><h4 name="2170" id="2170" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Setting up Retriever</strong></h4><p name="1574" id="1574" class="graf graf--p graf-after--h4">With our knowledge base in place, we can create a retriever using LlamaIndex‚Äôs <em class="markup--em markup--p-em">VectorIndexRetreiver(),</em> which returns the top 3 most similar chunks to a user query.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="4a2a" id="4a2a" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># set number of docs to retreive</span><br />top_k = <span class="hljs-number">3</span><br /><br /><span class="hljs-comment"># configure retriever</span><br />retriever = VectorIndexRetriever(<br />    index=index,<br />    similarity_top_k=top_k,<br />)</span></pre><p name="8f22" id="8f22" class="graf graf--p graf-after--pre">Next, we define a query engine that uses the retriever and query to return a set of relevant chunks.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="0c9d" id="0c9d" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># assemble query engine</span><br />query_engine = RetrieverQueryEngine(<br />    retriever=retriever,<br />    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=<span class="hljs-number">0.5</span>)],<br />)</span></pre><h4 name="fc03" id="fc03" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Use Query¬†Engine</strong></h4><p name="dbda" id="dbda" class="graf graf--p graf-after--h4">Now, with our knowledge base and retrieval system set up, let‚Äôs use it to return chunks relevant to a query. Here, we‚Äôll pass the same technical question we asked ShawGPT (the YouTube comment responder) from the <a href="https://medium.com/towards-data-science/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" data-href="https://medium.com/towards-data-science/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" class="markup--anchor markup--p-anchor" target="_blank">previous article</a>.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="d9ee" id="d9ee" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">query = <span class="hljs-string">&quot;What is fat-tailedness?&quot;</span><br />response = query_engine.query(query)</span></pre><p name="7fdd" id="7fdd" class="graf graf--p graf-after--pre">The query engine returns a response object containing the text, metadata, and indexes of relevant chunks. The code block below returns a more readable version of this information.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="0ac7" id="0ac7" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># reformat response</span><br />context = <span class="hljs-string">&quot;Context:\n&quot;</span><br /><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(top_k):<br />    context = context + response.source_nodes[i].text + <span class="hljs-string">&quot;\n\n&quot;</span><br /><br /><span class="hljs-built_in">print</span>(context)</span></pre><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="plaintext" name="458e" id="458e" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content">Context:<br />Some of the controversy might be explained by the observation that log-<br />normal distributions behave like Gaussian for low sigma and like Power Law<br />at high sigma [2].<br />However, to avoid controversy, we can depart (for now) from whether some<br />given data fits a Power Law or not and focus instead on fat tails.<br />Fat-tailedness ‚Äî measuring the space between Mediocristan<br />and Extremistan<br />Fat Tails are a more general idea than Pareto and Power Law distributions.<br />One way we can think about it is that ‚Äúfat-tailedness‚Äù is the degree to which<br />rare events drive the aggregate statistics of a distribution. From this point of<br />view, fat-tailedness lives on a spectrum from not fat-tailed (i.e. a Gaussian) to<br />very fat-tailed (i.e. Pareto 80 ‚Äì 20).<br />This maps directly to the idea of Mediocristan vs Extremistan discussed<br />earlier. The image below visualizes different distributions across this<br />conceptual landscape [2].<br /><br />print(&quot;mean kappa_1n = &quot; + str(np.mean(kappa_dict[filename])))<br />    print(&quot;&quot;)<br />Mean Œ∫ (1,100) values from 1000 runs for each dataset. Image by author.<br />These more stable results indicate Medium followers are the most fat-tailed,<br />followed by LinkedIn Impressions and YouTube earnings.<br />Note: One can compare these values to Table III in ref [3] to better understand each<br />Œ∫ value. Namely, these values are comparable to a Pareto distribution with Œ±<br />between 2 and 3.<br />Although each heuristic told a slightly different story, all signs point toward<br />Medium followers gained being the most fat-tailed of the 3 datasets.<br />Conclusion<br />While binary labeling data as fat-tailed (or not) may be tempting, fat-<br />tailedness lives on a spectrum. Here, we broke down 4 heuristics for<br />quantifying how fat-tailed data are.<br /><br />Pareto, Power Laws, and Fat Tails<br />What they don‚Äôt teach you in statistics<br />towardsdatascience.com<br />Although Pareto (and more generally power law) distributions give us a<br />salient example of fat tails, this is a more general notion that lives on a<br />spectrum ranging from thin-tailed (i.e. a Gaussian) to very fat-tailed (i.e.<br />Pareto 80 ‚Äì 20).<br />The spectrum of Fat-tailedness. Image by author.<br />This view of fat-tailedness provides us with a more flexible and precise way of<br />categorizing data than simply labeling it as a Power Law (or not). However,<br />this begs the question: how do we define fat-tailedness?<br />4 Ways to Quantify Fat Tails</span></pre><h4 name="e736" id="e736" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Adding RAG to¬†LLM</strong></h4><p name="e117" id="e117" class="graf graf--p graf-after--h4">We start by downloading the <a href="https://medium.com/towards-data-science/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" data-href="https://medium.com/towards-data-science/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" class="markup--anchor markup--p-anchor" target="_blank">fine-tuned model</a> from the Hugging Face hub.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="76cf" id="76cf" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># load fine-tuned model from hub</span><br /><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> PeftModel, PeftConfig<br /><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer<br /><br />model_name = <span class="hljs-string">&quot;TheBloke/Mistral-7B-Instruct-v0.2-GPTQ&quot;</span><br />model = AutoModelForCausalLM.from_pretrained(model_name,<br />                                             device_map=<span class="hljs-string">&quot;auto&quot;</span>,<br />                                             trust_remote_code=<span class="hljs-literal">False</span>,<br />                                             revision=<span class="hljs-string">&quot;main&quot;</span>)<br /><br />config = PeftConfig.from_pretrained(<span class="hljs-string">&quot;shawhin/shawgpt-ft&quot;</span>)<br />model = PeftModel.from_pretrained(model, <span class="hljs-string">&quot;shawhin/shawgpt-ft&quot;</span>)<br /><br /><span class="hljs-comment"># load tokenizer</span><br />tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=<span class="hljs-literal">True</span>)</span></pre><p name="ec8c" id="ec8c" class="graf graf--p graf-after--pre">As a baseline, we can see how the model responds to the technical question without any context from the articles. To do this, we create a prompt template using a lambda function, which takes in a viewer comment and returns a prompt for the LLM. For more details on where this prompt comes from, see the <a href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32#5aad" data-href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32#5aad" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">previous article</a> of this series.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="4d6c" id="4d6c" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># prompt (no context)</span><br />intstructions_string = <span class="hljs-string">f&quot;&quot;&quot;ShawGPT, functioning as a virtual data science \<br />consultant on YouTube, communicates in clear, accessible language, escalating \<br />to technical depth upon request. It reacts to feedback aptly and ends \<br />responses with its signature &#x27;‚ÄìShawGPT&#x27;.<br /><br />ShawGPT will tailor the length of its responses to match the viewer&#x27;s comment, \<br />providing concise acknowledgments to brief expressions of gratitude or \<br />feedback, thus keeping the interaction natural and engaging.<br /><br />Please respond to the following comment.<br />&quot;&quot;&quot;</span><br /><br />prompt_template = <span class="hljs-keyword">lambda</span> comment: <span class="hljs-string">f&#x27;&#x27;&#x27;[INST] <span class="hljs-subst">{intstructions_string}</span> \n<span class="hljs-subst">{comment}</span> \n[/INST]&#x27;&#x27;&#x27;</span><br />comment = <span class="hljs-string">&quot;What is fat-tailedness?&quot;</span><br /><br />prompt = prompt_template(comment)<br /><span class="hljs-built_in">print</span>(prompt)</span></pre><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="plaintext" name="4541" id="4541" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content">[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature &#x27;‚ÄìShawGPT&#x27;. ShawGPT will tailor the length of its responses to match the viewer&#x27;s comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.<br /><br />Please respond to the following comment.<br /> <br />What is fat-tailedness? <br />[/INST]</span></pre><p name="e1b1" id="e1b1" class="graf graf--p graf-after--pre">Next, we can pass this prompt to the model using the code below.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="makefile" name="2416" id="2416" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">model.eval()<br /><br />inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br />outputs = model.generate(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].to(<span class="hljs-string">&quot;cuda&quot;</span>), <br />                          max_new_tokens=280)<br /><br />print(tokenizer.batch_decode(outputs)[0])</span></pre><p name="9d01" id="9d01" class="graf graf--p graf-after--pre">Here‚Äôs the model‚Äôs response (no context).</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="plaintext" name="ee55" id="ee55" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Great question!<br /><br />Fat-tailedness is a statistical property of a distribution. In simple terms, <br />it refers to the presence of extreme outliers or heavy tails in the <br />distribution.<br /><br />For instance, consider the distribution of heights in a population. A normal <br />distribution would have most people clustered around an average height with a <br />few people deviating slightly from the mean. However, in a fat-tailed <br />distribution, you would observe a larger number of people being significantly <br />taller or shorter than the average.<br /><br />In finance, fat-tailedness is often used to describe the presence of extreme <br />market moves or outliers in financial returns. It&#x27;s an important concept in <br />risk management and financial modeling.<br /><br />I hope this explanation helps! Let me know if you have any questions.<br />‚ÄìShawGPT</span></pre><p name="d14e" id="d14e" class="graf graf--p graf-after--pre">Although the response&#39;s style and formatting are great, the model&#39;s explanation of fat-tailedness differs from how I defined it in my <a href="https://www.youtube.com/playlist?list=PLz-ep5RbHosVrT89BRNX-IGKHYxvx9wqi" data-href="https://www.youtube.com/playlist?list=PLz-ep5RbHosVrT89BRNX-IGKHYxvx9wqi" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">video</a> and <a href="https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a" data-href="https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">blog series</a>.</p><p name="1c6a" id="1c6a" class="graf graf--p graf-after--p">Let‚Äôs see what happens to the model‚Äôs response when we include the appropriate context. To do this, we create another prompt template, which can also take in context from the RAG system.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="0631" id="0631" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># prompt (with context)</span><br />prompt_template_w_context = <span class="hljs-keyword">lambda</span> context, comment: <span class="hljs-string">f&quot;&quot;&quot;[INST]ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \<br />It reacts to feedback aptly and ends responses with its signature &#x27;‚ÄìShawGPT&#x27;. \<br />ShawGPT will tailor the length of its responses to match the viewer&#x27;s comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \<br />thus keeping the interaction natural and engaging.<br /><br /><span class="hljs-subst">{context}</span><br />Please respond to the following comment. Use the context above if it is helpful.<br /><br /><span class="hljs-subst">{comment}</span><br />[/INST]<br />&quot;&quot;&quot;</span></span></pre><p name="05e0" id="05e0" class="graf graf--p graf-after--pre">Next, we pass the prompt with context from the query engine and the view comment to the model.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="b63c" id="b63c" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">prompt = prompt_template_w_context(context, comment)<br /><br />inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br />outputs = model.generate(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].to(<span class="hljs-string">&quot;cuda&quot;</span>), max_new_tokens=<span class="hljs-number">280</span>)<br /><br /><span class="hljs-built_in">print</span>(tokenizer.batch_decode(outputs)[<span class="hljs-number">0</span>])</span></pre><p name="268c" id="268c" class="graf graf--p graf-after--pre">Here‚Äôs the new response (with context).</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="plaintext" name="a079" id="a079" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Great question!<br /><br />Fat-tailedness is a property of a distribution that describes how much the <br />presence of extreme values (rare events) affects the overall statistics of the <br />distribution.<br /><br />In the context of the video, the author discusses how Mediocristan and <br />Extremistan distributions differ in terms of fat-tailedness. Mediocristan <br />distributions have a relatively small impact from extreme values, while <br />Extremistan distributions are heavily influenced by extreme values.<br /><br />The author also mentions that fat-tailedness is a more general concept than <br />specific distributions like Pareto or Power Laws. Instead, it lives on a <br />spectrum ranging from thin-tailed (Gaussian) to very fat-tailed (Pareto 80-20).<br /><br />I hope that helps clarify things a bit! Let me know if you have any questions.<br />‚ÄìShawGPT</span></pre><p name="2433" id="2433" class="graf graf--p graf-after--pre">This does a much better job of capturing my explanation of fat tails than the no-context response and even calls out the niche concepts of Mediocristan and Extremistan.</p><div name="f3b8" id="f3b8" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing" data-href="https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing"><strong class="markup--strong markup--mixtapeEmbed-strong">Google Colaboratory</strong><br><em class="markup--em markup--mixtapeEmbed-em">RAG Example Code</em>colab.research.google.com</a><a href="https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="e37ca2309e77eaae1065c45365e1b4ee" data-thumbnail-img-id="0*Rajo3nJXfYWGBJlY" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*Rajo3nJXfYWGBJlY);"></a></div><h3 name="e13d" id="e13d" class="graf graf--h3 graf-after--mixtapeEmbed"><strong class="markup--strong markup--h3-strong">What‚Äôs next?</strong></h3><p name="13ac" id="13ac" class="graf graf--p graf-after--h3">Here, I gave a beginner-friendly introduction to RAG and shared a concrete example of how to implement it using LlamaIndex. RAG allows us to improve an LLM system with updateable and domain-specific knowledge.</p><p name="13c9" id="13c9" class="graf graf--p graf-after--p">While much of the recent AI hype has centered around building AI assistants, a powerful (yet less popular) innovation has come from text embeddings (i.e. the things we used to do retrieval). In the next article of this series, I will explore <strong class="markup--strong markup--p-strong">text embeddings</strong> in more detail, including how they can be used for <strong class="markup--strong markup--p-strong">semantic search</strong> and <strong class="markup--strong markup--p-strong">classification tasks</strong>.</p><p name="345b" id="345b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">More on LLMs üëá</strong></p><div name="c511" id="c511" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/list/8e009ae3054c" data-href="https://shawhin.medium.com/list/8e009ae3054c" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/list/8e009ae3054c"><strong class="markup--strong markup--mixtapeEmbed-strong">Large Language Models (LLMs)</strong><br><em class="markup--em markup--mixtapeEmbed-em">A series on how to use LLMs in practice.</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/list/8e009ae3054c" class="js-mixtapeImage mixtapeImage mixtapeImage--mediumCatalog  u-ignoreBlock" data-media-id="e38ca2d0de6aa61d7c09af603d8f43e1" data-thumbnail-img-id="0*a12807dcd1484c74906e2316ada10e0e70ce6dcd.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/304/160/0*a12807dcd1484c74906e2316ada10e0e70ce6dcd.jpeg);"></a></div></div></div></section><section name="f74e" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="9017" id="9017" class="graf graf--h3 graf--leading">Resources</h3><p name="f0c1" id="f0c1" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Connect</strong>: <a href="https://shawhintalebi.com/" data-href="https://shawhintalebi.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">My website</a> | <a href="https://calendly.com/shawhintalebi" data-href="https://calendly.com/shawhintalebi" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Book a call</a></p><p name="1a0a" id="1a0a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Socials</strong>: <a href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" data-href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">YouTube üé•</a> | <a href="https://www.linkedin.com/in/shawhintalebi/" data-href="https://www.linkedin.com/in/shawhintalebi/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">LinkedIn</a> | <a href="https://www.instagram.com/shawhintalebi" data-href="https://www.instagram.com/shawhintalebi" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Instagram</a></p><p name="0549" id="0549" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Support</strong>: <a href="https://www.buymeacoffee.com/shawhint" data-href="https://www.buymeacoffee.com/shawhint" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Buy me a coffee</a> ‚òïÔ∏è</p><div name="fe26" id="fe26" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/subscribe" data-href="https://shawhin.medium.com/subscribe" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/subscribe"><strong class="markup--strong markup--mixtapeEmbed-strong">Get FREE access to every new story I write</strong><br><em class="markup--em markup--mixtapeEmbed-em">Get FREE access to every new story I write P.S. I do not share your email with anyone By signing up, you will create a‚Ä¶</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/subscribe" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8d6d6c2d301f2c367b618a850fa468b3" data-thumbnail-img-id="0*O69FxQ5FttNEA_r4" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*O69FxQ5FttNEA_r4);"></a></div></div></div></section><section name="41fa" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="cb95" id="cb95" class="graf graf--p graf--leading">[1] <a href="https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb" data-href="https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">RAG &gt; FT (empirical)</a></p><p name="d8de" id="d8de" class="graf graf--p graf-after--p">[2] <a href="https://www.youtube.com/watch?v=efbn-3tPI_M" data-href="https://www.youtube.com/watch?v=efbn-3tPI_M" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LlamaIndex Webinar: Building LLM Apps for Production, Part 1 (co-hosted with Anyscale)</a></p><p name="2419" id="2419" class="graf graf--p graf-after--p">[3] <a href="https://docs.llamaindex.ai/en/stable/understanding/loading/loading.html" data-href="https://docs.llamaindex.ai/en/stable/understanding/loading/loading.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LlamaIndex doc</a></p><p name="db9e" id="db9e" class="graf graf--p graf-after--p graf--trailing">[4] <a href="https://www.youtube.com/watch?v=Zj5RCweUHIk&amp;list=WL&amp;index=4" data-href="https://www.youtube.com/watch?v=Zj5RCweUHIk&amp;list=WL&amp;index=4" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LlamaIndex Webinar: Make RAG Production-Ready</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@shawhin" class="p-author h-card">Shaw Talebi</a> on <a href="https://medium.com/p/abdc132f76ac"><time class="dt-published" datetime="2024-03-09T15:00:34.960Z">March 9, 2024</time></a>.</p><p><a href="https://medium.com/@shawhin/how-to-improve-llms-with-rag-abdc132f76ac" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 2, 2024.</p></footer></article></body></html>