<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Cracking Open the Hugging Face Transformers Library</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Cracking Open the Hugging Face Transformers Library</h1>
</header>
<section data-field="subtitle" class="p-summary">
A quick-start guide to using open-source LLMs
</section>
<section data-field="body" class="e-content">
<section name="1476" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="f703" id="f703" class="graf graf--h3 graf--leading graf--title"><strong class="markup--strong markup--h3-strong">Cracking Open the Hugging Face Transformers Library</strong></h3><h4 name="5565" id="5565" class="graf graf--h4 graf-after--h3 graf--subtitle">A quick-start guide to using open-source LLMs</h4><p name="6eaa" id="6eaa" class="graf graf--p graf-after--h4">This is the 3rd article in a <a href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" data-href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">series on using large language models (LLMs)</a> in practice. Here I will give a beginner-friendly guide to the Hugging Face Transformers library, which provides an easy and cost-free way to work with a wide variety of open-source language models. I will start by reviewing key concepts and then dive into example Python code.</p><figure name="302a" id="302a" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="0*Rkoquyw55K6qbFWF" data-width="8192" data-height="5464" data-unsplash-photo-id="xrrX5_J3Jm4" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*Rkoquyw55K6qbFWF"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@chinatravelchannel?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@chinatravelchannel?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener noopener" target="_blank">Jéan Béller</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener noopener" target="_blank">Unsplash</a></figcaption></figure></div></div></section><section name="4fc7" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="d123" id="d123" class="graf graf--p graf--leading">In the <a href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971" data-href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">previous article</a> of this series, we explored the OpenAI Python API and used it to make a custom chatbot. One downside of this API, however, is that API calls cost money, which may not scale well for some use cases.</p><p name="4c66" id="4c66" class="graf graf--p graf-after--p">In these scenarios, it may be advantageous to turn to open-source solutions. One popular way to do this is via Hugging Face’s Transformers library.</p><figure name="5b48" id="5b48" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/jan07gloaRg?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><h3 name="db10" id="db10" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">What is Hugging Face?</strong></h3><p name="14c0" id="14c0" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Hugging Face</strong> is an <strong class="markup--strong markup--p-strong">AI company that has become a major hub for open-source machine learning (ML)</strong>. Their platform has 3 major elements which allow users to access and share machine learning resources.</p><p name="ddc7" id="ddc7" class="graf graf--p graf-after--p">First is their rapidly growing repository of pre-trained open-source ML <strong class="markup--strong markup--p-strong">models</strong> for things such as natural language processing (NLP), computer vision, and more. Second is their library of <strong class="markup--strong markup--p-strong">datasets</strong> for training ML models for almost any task. Third, and finally, is <strong class="markup--strong markup--p-strong">Spaces</strong> which is a collection of open-source ML apps hosted by Hugging Face.</p><p name="c7d4" id="c7d4" class="graf graf--p graf-after--p">The power of these resources is that they are community generated, which leverages all the benefits of open-source (i.e. cost-free, wide diversity of tools, high-quality resources, and rapid pace of innovation). While these make building powerful ML projects more accessible than before, there is another key element of the Hugging Face ecosystem — the Transformers library.</p><h3 name="c41c" id="c41c" class="graf graf--h3 graf-after--p">🤗<strong class="markup--strong markup--h3-strong">Transformers</strong></h3><p name="ab68" id="ab68" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Transformers</strong> is a <strong class="markup--strong markup--p-strong">Python library that makes downloading and training state-of-the-art ML models easy</strong>. Although it was initially made for developing language models, its functionality has expanded to include models for computer vision, audio processing, and beyond.</p><p name="6da6" id="6da6" class="graf graf--p graf-after--p">Two big strengths of this library are, <strong class="markup--strong markup--p-strong">one</strong>, it easily integrates with Hugging Face’s (previously mentioned) Models, Datasets, and Spaces repositories, and <strong class="markup--strong markup--p-strong">two</strong>, the library supports other popular ML frameworks such as PyTorch and TensorFlow.</p><p name="874d" id="874d" class="graf graf--p graf-after--p">This results in a simple and flexible all-in-one platform for downloading, training, and deploying machine learning models and apps.</p><h4 name="7827" id="7827" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Pipeline()</strong></h4><p name="f27d" id="f27d" class="graf graf--p graf-after--h4">The easiest way to start using the library is via the <em class="markup--em markup--p-em">pipeline()</em> function, which abstracts NLP (and other) tasks into 1 line of code. For example, if we want to do sentiment analysis, we would need to select a model, tokenize the input text, pass it through the model, and decode the numerical output to determine the sentiment label (positive or negative).</p><p name="db12" id="db12" class="graf graf--p graf-after--p">While this may seem like a lot of steps, we can do all this in 1 line via the <em class="markup--em markup--p-em">pipeline()</em> function, as shown in the code snippet below.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="dfe5" id="dfe5" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">pipeline(task=<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)(<span class="hljs-string">&quot;Love this!&quot;</span>)<br /><br /><span class="hljs-comment"># output -&gt; [{&#x27;label&#x27;: &#x27;POSITIVE&#x27;, &#x27;score&#x27;: 0.9998745918273926}]</span></span></pre><p name="f696" id="f696" class="graf graf--p graf-after--pre">Of course, sentiment analysis is not the only thing we can do here. Almost any NLP task can be done in this way e.g. summarization, translation, question-answering, feature extraction (i.e. text embedding), text generation, zero-shot-classification, and more — <em class="markup--em markup--p-em">for a full list of the built-in tasks, check out the </em><a href="https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.task" data-href="https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.task" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">pipleine() documentation</em></a>.</p><p name="3277" id="3277" class="graf graf--p graf-after--p">In the above example code, since we did not specify a model, the default model for sentiment analysis was used (i.e. <em class="markup--em markup--p-em">distilbert-base-uncased-finetuned-sst-2-english</em>). However, if we wanted to be more explicit, we could have used the following line of code.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="af96" id="af96" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">pipeline(task=<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, <br />        model=<span class="hljs-string">&#x27;distilbert-base-uncased-finetuned-sst-2-english&#x27;</span>)(<span class="hljs-string">&quot;Love this!&quot;</span>)<br /><br /><span class="hljs-comment"># ouput -&gt; [{&#x27;label&#x27;: &#x27;POSITIVE&#x27;, &#x27;score&#x27;: 0.9998745918273926}]</span></span></pre><p name="da46" id="da46" class="graf graf--p graf-after--pre">One of the greatest benefits of the Transformers library is we could have just as easily used any of the 28,000+ text classification models on Hugging Face’s <a href="https://huggingface.co/models?pipeline_tag=text-classification&amp;sort=trending" data-href="https://huggingface.co/models?pipeline_tag=text-classification&amp;sort=trending" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Models repository</a> by simply changing the model name passed into the <em class="markup--em markup--p-em">pipeline()</em> function.</p><h4 name="6478" id="6478" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Models</strong></h4><p name="acee" id="acee" class="graf graf--p graf-after--h4">There is a massive repository of pre-trained models available on <a href="https://huggingface.co/models" data-href="https://huggingface.co/models" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Hugging Face</a> (277,528 at the time of writing this). Almost all these models can be easily used via Transformers, using the same syntax we saw in the above code block.</p><p name="b250" id="b250" class="graf graf--p graf-after--p">However, the models on Hugging Face <strong class="markup--strong markup--p-strong">aren’t only for the Transformers library. </strong>There are models for other popular machine learning frameworks e.g. PyTorch, Tensorflow, Jax. This makes Hugging Face’s Models repository useful to ML practitioners beyond the context of the Transformers library.</p><p name="f86a" id="f86a" class="graf graf--p graf-after--p">To see what navigating the repository looks like, let’s consider an example. Say we want a model that can do text generation, but we want it to be available via the Transformers library so we can use it in one line of code (as we did above). We can easily view all models that fit these criteria using the “Tasks” and “Libraries” filters.</p><p name="0e09" id="0e09" class="graf graf--p graf-after--p">A model that meets these criteria is the newly released Llama 2. More specifically, <em class="markup--em markup--p-em">Llama-2–7b-chat-hf</em>, which is a model in the Llama 2 family with about 7 billion parameters, optimized for chat, and in the Hugging Face Transformers format. We can get more information about this model via its <strong class="markup--strong markup--p-strong">model card</strong>, which is shown in the figure below.</p><figure name="bb42" id="bb42" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*0B5_bK2QAoNNTepbcZNFNQ.jpeg" data-width="2732" data-height="922" src="https://cdn-images-1.medium.com/max/800/1*0B5_bK2QAoNNTepbcZNFNQ.jpeg"><figcaption class="imageCaption">Touring the <a href="https://huggingface.co/meta-llama/Llama-2-70b-chat-hf" data-href="https://huggingface.co/meta-llama/Llama-2-70b-chat-hf" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Llama-2–7b-chat-hf model card</a>. Image by author.</figcaption></figure><h3 name="2554" id="2554" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">Installing </strong>🤗<strong class="markup--strong markup--h3-strong">Transformers (with Conda)</strong></h3><p name="1273" id="1273" class="graf graf--p graf-after--h3">Now that we have a basic idea of the resources offered by Hugging Face and the Transformers library let’s see how we can use them. We start by installing the library and other dependencies.</p><p name="f4a7" id="f4a7" class="graf graf--p graf-after--p">Hugging Face provides an <a href="https://huggingface.co/docs/transformers/installation" data-href="https://huggingface.co/docs/transformers/installation" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">installation guide</a> on its website. So, I won’t try to (poorly) duplicate that guide here. However, I will provide a quick 2-step guide on <strong class="markup--strong markup--p-strong">how to set up the conda environment for the example code below</strong>.</p><p name="b42a" id="b42a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Step 1)</strong> The first step is to download the hf-env.yml file available at the <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub repository</a>. You can either download the file directly or clone the whole repo.</p><p name="da61" id="da61" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Step 2)</strong> Next, in your terminal (or anaconda command prompt), you can create a new conda environment based on hf-env.yml using the following commands</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="bash" name="6091" id="6091" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">&gt;&gt;&gt; <span class="hljs-built_in">cd</span> &lt;directory with hf-env.yml&gt;<br /><br />&gt;&gt;&gt; conda <span class="hljs-built_in">env</span> create --file hf-env.yml</span></pre><p name="bbb7" id="bbb7" class="graf graf--p graf-after--pre">This may take a couple of minutes to install, but once it’s complete, you should be ready to go!</p><h3 name="dd41" id="dd41" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Example Code: NLP with </strong>🤗<strong class="markup--strong markup--h3-strong">Transformers</strong></h3><p name="cac9" id="cac9" class="graf graf--p graf-after--h3">With the necessary libraries installed, let’s jump into some example code. Here we will survey <strong class="markup--strong markup--p-strong">3 NLP use cases</strong>, namely, <strong class="markup--strong markup--p-strong">sentiment analysis, summarization, and conversational text generation</strong>, using the <em class="markup--em markup--p-em">pipeline()</em> function.</p><p name="9424" id="9424" class="graf graf--p graf-after--p">Toward the end, we will use Gradio to quickly generate a User Interface (UI) for any of these use cases and deploy it as an app on Hugging Face Spaces. All example code is available on the <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub repository</a>.</p><h4 name="a3c3" id="a3c3" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Sentiment Analysis</strong></h4><p name="324c" id="324c" class="graf graf--p graf-after--h4">We start sentiment analysis. Recall from earlier when we used the pipeline function to do something like the code block below, where we create a classifier that can label the input text as being either positive or negative.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="77b7" id="77b7" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br /><br />classifier = pipeline(task=<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, \<br />                      model=<span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>)<br /><br />classifier(<span class="hljs-string">&quot;Hate this.&quot;</span>)<br /><br /><span class="hljs-comment"># output -&gt; [{&#x27;label&#x27;: &#x27;NEGATIVE&#x27;, &#x27;score&#x27;: 0.9997110962867737}]</span></span></pre><p name="3176" id="3176" class="graf graf--p graf-after--pre">To go one step further, instead of processing text one by one, we can pass a list to the classifier to process as a batch.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="3d47" id="3d47" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">text_list = [<span class="hljs-string">&quot;This is great&quot;</span>, \<br />             <span class="hljs-string">&quot;Thanks for nothing&quot;</span>, \<br />             <span class="hljs-string">&quot;You&#x27;ve got to work on your face&quot;</span>, \<br />             <span class="hljs-string">&quot;You&#x27;re beautiful, never change!&quot;</span>]<br /><br />classifier(text_list)<br /><br /><span class="hljs-comment"># output -&gt; [{&#x27;label&#x27;: &#x27;POSITIVE&#x27;, &#x27;score&#x27;: 0.9998785257339478},</span><br /><span class="hljs-comment"># {&#x27;label&#x27;: &#x27;POSITIVE&#x27;, &#x27;score&#x27;: 0.9680058360099792},</span><br /><span class="hljs-comment"># {&#x27;label&#x27;: &#x27;NEGATIVE&#x27;, &#x27;score&#x27;: 0.8776106238365173},</span><br /><span class="hljs-comment"># {&#x27;label&#x27;: &#x27;POSITIVE&#x27;, &#x27;score&#x27;: 0.9998120665550232}]</span></span></pre><p name="ddc5" id="ddc5" class="graf graf--p graf-after--pre">However, the text classification models on Hugging Face are not limited to just positive-negative sentiment. For example, the “<em class="markup--em markup--p-em">roberta-base-go_emotions</em>” model by SamLowe generates a suite of class labels. We can just as easily apply this model to text, as shown in the code snippet below.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="6625" id="6625" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">classifier = pipeline(task=<span class="hljs-string">&quot;text-classification&quot;</span>, \<br />                      model=<span class="hljs-string">&quot;SamLowe/roberta-base-go_emotions&quot;</span>, top_k=<span class="hljs-literal">None</span>)<br /><br />classifier(text_list[<span class="hljs-number">0</span>])<br /><br /><span class="hljs-comment"># output -&gt; [[{&#x27;label&#x27;: &#x27;admiration&#x27;, &#x27;score&#x27;: 0.9526104927062988},</span><br /><span class="hljs-comment">#  {&#x27;label&#x27;: &#x27;approval&#x27;, &#x27;score&#x27;: 0.03047208860516548},</span><br /><span class="hljs-comment">#  {&#x27;label&#x27;: &#x27;neutral&#x27;, &#x27;score&#x27;: 0.015236231498420238},</span><br /><span class="hljs-comment">#  {&#x27;label&#x27;: &#x27;excitement&#x27;, &#x27;score&#x27;: 0.006063772831112146},</span><br /><span class="hljs-comment">#  {&#x27;label&#x27;: &#x27;gratitude&#x27;, &#x27;score&#x27;: 0.005296189337968826},</span><br /><span class="hljs-comment">#  {&#x27;label&#x27;: &#x27;joy&#x27;, &#x27;score&#x27;: 0.004475208930671215},</span><br /><span class="hljs-comment">#  ... and many more</span></span></pre><h4 name="cf35" id="cf35" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Summarization</strong></h4><p name="cc27" id="cc27" class="graf graf--p graf-after--h4">Another way we can use the <em class="markup--em markup--p-em">pipeline() </em>function is for text summarization. Although this is an entirely different task than sentiment analysis, the syntax is almost identical.</p><p name="f5ae" id="f5ae" class="graf graf--p graf-after--p">We first load in a summarization model. Then pass in some text along with a couple of input parameters.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="45d3" id="45d3" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">summarizer = pipeline(<span class="hljs-string">&quot;summarization&quot;</span>, model=<span class="hljs-string">&quot;facebook/bart-large-cnn&quot;</span>)<br /><br />text = <span class="hljs-string">&quot;&quot;&quot;<br />Hugging Face is an AI company that has become a major hub for open-source machine learning. <br />Their platform has 3 major elements which allow users to access and share machine learning resources. <br />First, is their rapidly growing repository of pre-trained open-source machine learning models for things such as natural language processing (NLP), computer vision, and more. <br />Second, is their library of datasets for training machine learning models for almost any task. <br />Third, and finally, is Spaces which is a collection of open-source ML apps.<br /><br />The power of these resources is that they are community generated, which leverages all the benefits of open source i.e. cost-free, wide diversity of tools, high quality resources, and rapid pace of innovation. <br />While these make building powerful ML projects more accessible than before, there is another key element of the Hugging Face ecosystem—their Transformers library.<br />&quot;&quot;&quot;</span><br />summarized_text = summarizer(text, min_length=<span class="hljs-number">5</span>, max_length=<span class="hljs-number">140</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;summary_text&#x27;</span>]<br /><span class="hljs-built_in">print</span>(summarized_text)<br /><br /><span class="hljs-comment"># output -&gt; &#x27;Hugging Face is an AI company that has become a major hub for </span><br /><span class="hljs-comment"># open-source machine learning. They have 3 major elements which allow users </span><br /><span class="hljs-comment"># to access and share machine learning resources.&#x27; </span></span></pre><p name="473f" id="473f" class="graf graf--p graf-after--pre">For more sophisticated use cases, it may be necessary to use multiple models in succession. For example, we can apply sentiment analysis to the summarized text to speed up the runtime.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="a912" id="a912" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">classifier(summarized_text)<br /><br /><span class="hljs-comment"># output -&gt; [[{&#x27;label&#x27;: &#x27;neutral&#x27;, &#x27;score&#x27;: 0.9101783633232117},  </span><br /><span class="hljs-comment"># {&#x27;label&#x27;: &#x27;approval&#x27;, &#x27;score&#x27;: 0.08781372010707855},  </span><br /><span class="hljs-comment"># {&#x27;label&#x27;: &#x27;realization&#x27;, &#x27;score&#x27;: 0.023256294429302216},  </span><br /><span class="hljs-comment"># {&#x27;label&#x27;: &#x27;annoyance&#x27;, &#x27;score&#x27;: 0.006623792927712202},  </span><br /><span class="hljs-comment"># {&#x27;label&#x27;: &#x27;admiration&#x27;, &#x27;score&#x27;: 0.004981081001460552},  </span><br /><span class="hljs-comment"># {&#x27;label&#x27;: &#x27;disapproval&#x27;, &#x27;score&#x27;: 0.004730119835585356},  </span><br /><span class="hljs-comment"># {&#x27;label&#x27;: &#x27;optimism&#x27;, &#x27;score&#x27;: 0.0033590723760426044},  </span><br /><span class="hljs-comment"># ... and many more</span></span></pre><h4 name="fbae" id="fbae" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Conversational</strong></h4><p name="73f5" id="73f5" class="graf graf--p graf-after--h4">Finally, we can use models developed specifically to generate conversational text. Since conversations require past prompts and responses to be passed to subsequent model responses, the syntax is a little different here. However, we start by instantiating our model using the <em class="markup--em markup--p-em">pipeline()</em> function.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="23da" id="23da" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">chatbot = pipeline(model=<span class="hljs-string">&quot;facebook/blenderbot-400M-distill&quot;</span>)</span></pre><p name="c393" id="c393" class="graf graf--p graf-after--pre">Next, we can use the <em class="markup--em markup--p-em">Conversation()</em> class to handle the back-and-forth. We initialize it with a user prompt, then pass it into the chatbot model from the previous code block.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="c9ba" id="c9ba" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Conversation<br /><br />conversation = Conversation(<span class="hljs-string">&quot;Hi I&#x27;m Shaw, how are you?&quot;</span>)<br />conversation = chatbot(conversation)<br /><span class="hljs-built_in">print</span>(conversation)<br /><br /><span class="hljs-comment"># output -&gt; Conversation id: 9248ee7d-2a58-4355-9fba-525189fae206 </span><br /><span class="hljs-comment"># user &gt;&gt; Hi I&#x27;m Shaw, how are you? </span><br /><span class="hljs-comment"># bot &gt;&gt;  I&#x27;m doing well. How are you doing this evening? I just got home from work. </span></span></pre><p name="a3e7" id="a3e7" class="graf graf--p graf-after--pre">To keep the conversation going, we can use the <em class="markup--em markup--p-em">add_user_input()</em> method to add another prompt to the conversation. We then pass the conversation object back into the chatbot.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="ad09" id="ad09" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">conversation.add_user_input(<span class="hljs-string">&quot;Where do you work?&quot;</span>)<br />conversation = chatbot(conversation)<br /><span class="hljs-built_in">print</span>(conversation)<br /><br /><span class="hljs-comment"># output -&gt; Conversation id: 9248ee7d-2a58-4355-9fba-525189fae206 </span><br /><span class="hljs-comment"># user &gt;&gt; Hi I&#x27;m Shaw, how are you? </span><br /><span class="hljs-comment"># bot &gt;&gt;  I&#x27;m doing well. How are you doing this evening? I just got home from work.</span><br /><span class="hljs-comment"># user &gt;&gt; Where do you work? </span><br /><span class="hljs-comment"># bot &gt;&gt;  I work at a grocery store. What about you? What do you do for a living? </span></span></pre><h4 name="53d2" id="53d2" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Chatbot UI with Gradio</strong></h4><p name="ae1b" id="ae1b" class="graf graf--p graf-after--h4">While we get the base chatbot functionality with the Transformer library, this is an inconvenient way to interact with a chatbot. To make the interaction a bit more intuitive, we can use <strong class="markup--strong markup--p-strong">Gradio</strong> to <strong class="markup--strong markup--p-strong">spin up a front end in a few lines of Python code</strong>.</p><p name="6e60" id="6e60" class="graf graf--p graf-after--p">This is done with the code shown below. At the top, we initialize two lists to store user messages and model responses, respectively. Then we define a function that will take the user prompt and generate a chatbot output. Next, we create the chat UI using the Gradio <em class="markup--em markup--p-em">ChatInterface()</em> class. Finally, we launch the app.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="2739" id="2739" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">message_list = []<br />response_list = []<br /><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">vanilla_chatbot</span>(<span class="hljs-params">message, history</span>):<br />    conversation = Conversation(text=message, past_user_inputs=message_list, generated_responses=response_list)<br />    conversation = chatbot(conversation)<br /><br />    <span class="hljs-keyword">return</span> conversation.generated_responses[-<span class="hljs-number">1</span>]<br /><br />demo_chatbot = gr.ChatInterface(vanilla_chatbot, title=<span class="hljs-string">&quot;Vanilla Chatbot&quot;</span>, description=<span class="hljs-string">&quot;Enter text to start chatting.&quot;</span>)<br /><br />demo_chatbot.launch()</span></pre><p name="e23b" id="e23b" class="graf graf--p graf-after--pre">This will spin up the UI via a local URL. If the window does not open automatically, you can copy and paste the URL directly into your browser.</p><figure name="6e99" id="6e99" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*okUwxEBv2oZuUYi7Pks3sw.gif" data-width="600" data-height="255" src="https://cdn-images-1.medium.com/max/800/1*okUwxEBv2oZuUYi7Pks3sw.gif"><figcaption class="imageCaption">Gradio interface. GIF by author.</figcaption></figure><h4 name="efa1" id="efa1" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Hugging Face Spaces</strong></h4><p name="42bc" id="42bc" class="graf graf--p graf-after--h4">To go one step further, we can quickly deploy this UI via <strong class="markup--strong markup--p-strong">Hugging Face Spaces</strong>. These are <strong class="markup--strong markup--p-strong">Git repositories hosted by Hugging Face and augmented by computational resources</strong>. Both free and paid options are available depending on the use case. Here we will stick with the free option.</p><p name="96a2" id="96a2" class="graf graf--p graf-after--p">To make a new Space, we first go to the <a href="https://huggingface.co/spaces" data-href="https://huggingface.co/spaces" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Spaces page</a> and click “Create new space”. Then, configure the Space by giving it the name e.g. “my-first-space” and selecting Gradio as the SDK. Then hit “Create Space”.</p><figure name="d843" id="d843" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3vKV5gW4sdc6qvF7mdKUcg.png" data-width="638" data-height="961" src="https://cdn-images-1.medium.com/max/800/1*3vKV5gW4sdc6qvF7mdKUcg.png"><figcaption class="imageCaption">Hugging Face Space configuration. Image by author.</figcaption></figure><p name="dc77" id="dc77" class="graf graf--p graf-after--figure">Next, we need to upload app.py and requirements.txt files to the Space. The app.py file houses the code we used to generate the Gradio UI, and the requirements.txt file specifies the app’s dependencies. The files for this example are available at the <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face/my-first-space" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/hugging-face/my-first-space" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub repo</a> and the <a href="https://huggingface.co/spaces/shawhin/my-first-space/tree/main" data-href="https://huggingface.co/spaces/shawhin/my-first-space/tree/main" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Hugging Face Space</a>.</p><p name="4ab0" id="4ab0" class="graf graf--p graf-after--p">Finally, we push the code to the Space just like we would to GitHub. The end result is a public application hosted on Hugging Face Spaces.</p><p name="e1fb" id="e1fb" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">App link</strong>: <a href="https://huggingface.co/spaces/shawhin/my-first-space" data-href="https://huggingface.co/spaces/shawhin/my-first-space" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://huggingface.co/spaces/shawhin/my-first-space</a></p><h3 name="1834" id="1834" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Conclusion</strong></h3><p name="571b" id="571b" class="graf graf--p graf-after--h3">Hugging Face has become synonymous with open-source language models and machine learning. The biggest advantage of their ecosystem is it gives small-time developers, researchers, and tinkers access to powerful ML resources.</p><p name="0270" id="0270" class="graf graf--p graf-after--p">While we covered a lot of material in this post, we’ve only scratched the surface of what the Hugging Face ecosystem can do. In future articles of this series, we will explore more advanced use cases and cover <a href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--p-anchor" target="_blank">how to fine-tune models</a> using 🤗Transformers.</p><p name="aa06" id="aa06" class="graf graf--p graf-after--p">👉 <strong class="markup--strong markup--p-strong">More on LLMs</strong>: <a href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" data-href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Introduction</a> | <a href="https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971" data-href="https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenAI API</a> | <a href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" data-href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" class="markup--anchor markup--p-anchor" target="_blank">Prompt Engineering</a> | <br><a href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--p-anchor" target="_blank">Fine-tuning</a> | <a href="https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9" data-href="https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Build an LLM</a> | <a href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" data-href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">QLoRA</a> | <a href="https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac" data-href="https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac" class="markup--anchor markup--p-anchor" target="_blank">RAG</a> | <a href="https://towardsdatascience.com/text-embeddings-classification-and-semantic-search-8291746220be" data-href="https://towardsdatascience.com/text-embeddings-classification-and-semantic-search-8291746220be" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Text Embeddings</a></p><div name="6535" id="6535" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/list/8e009ae3054c" data-href="https://shawhin.medium.com/list/8e009ae3054c" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/list/8e009ae3054c"><strong class="markup--strong markup--mixtapeEmbed-strong">Large Language Models (LLMs)</strong><br><em class="markup--em markup--mixtapeEmbed-em">A series on how to use LLMs in practice.</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/list/8e009ae3054c" class="js-mixtapeImage mixtapeImage mixtapeImage--mediumCatalog  u-ignoreBlock" data-media-id="e38ca2d0de6aa61d7c09af603d8f43e1" data-thumbnail-img-id="0*ae40ceaca4c0bdcbdf431a66c777e68bffc1e356.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/304/160/0*ae40ceaca4c0bdcbdf431a66c777e68bffc1e356.jpeg);"></a></div></div></div></section><section name="99ab" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="aad7" id="aad7" class="graf graf--h3 graf--leading">Resources</h3><p name="fe2f" id="fe2f" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Connect</strong>: <a href="https://shawhintalebi.com/" data-href="https://shawhintalebi.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">My website</a> | <a href="https://calendly.com/shawhintalebi" data-href="https://calendly.com/shawhintalebi" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Book a call</a> | <a href="https://shawhintalebi.com/contact/" data-href="https://shawhintalebi.com/contact/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Ask me anything</a></p><p name="36c7" id="36c7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Socials</strong>: <a href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" data-href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">YouTube 🎥</a> | <a href="https://www.linkedin.com/in/shawhintalebi/" data-href="https://www.linkedin.com/in/shawhintalebi/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">LinkedIn</a> | <a href="https://twitter.com/ShawhinT" data-href="https://twitter.com/ShawhinT" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Twitter</a></p><p name="6fab" id="6fab" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Support</strong>: <a href="https://www.buymeacoffee.com/shawhint" data-href="https://www.buymeacoffee.com/shawhint" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener noopener" target="_blank">Buy me a coffee</a> ☕️</p><div name="4a91" id="4a91" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/subscribe" data-href="https://shawhin.medium.com/subscribe" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/subscribe"><strong class="markup--strong markup--mixtapeEmbed-strong">Get FREE access to every new story I write</strong><br><em class="markup--em markup--mixtapeEmbed-em">Get FREE access to every new story I write P.S. I do not share your email with anyone By signing up, you will create a…</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/subscribe" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8d6d6c2d301f2c367b618a850fa468b3" data-thumbnail-img-id="0*O69FxQ5FttNEA_r4" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*O69FxQ5FttNEA_r4);"></a></div></div></div></section><section name="b2db" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="fab5" id="fab5" class="graf graf--p graf--leading">[1] Hugging Face — <a href="https://huggingface.co/" data-href="https://huggingface.co/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://huggingface.co/</a></p><p name="5a55" id="5a55" class="graf graf--p graf-after--p graf--trailing">[2] Hugging Face Course — <a href="https://huggingface.co/learn/nlp-course/chapter1/1" data-href="https://huggingface.co/learn/nlp-course/chapter1/1" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://huggingface.co/learn/nlp-course/chapter1/1</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@shawhin" class="p-author h-card">Shaw Talebi</a> on <a href="https://medium.com/p/350aa0ef0161"><time class="dt-published" datetime="2023-08-05T04:33:35.681Z">August 5, 2023</time></a>.</p><p><a href="https://medium.com/@shawhin/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 2, 2024.</p></footer></article></body></html>