<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Text Embeddings, Classification, and Semantic Search</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Text Embeddings, Classification, and Semantic Search</h1>
</header>
<section data-field="subtitle" class="p-summary">
An introduction with example Python code
</section>
<section data-field="body" class="e-content">
<section name="b2b2" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="e6cd" id="e6cd" class="graf graf--h3 graf--leading graf--title">Text Embeddings, Classification, and Semantic Search</h3><h4 name="c031" id="c031" class="graf graf--h4 graf-after--h3 graf--subtitle">An introduction with example Python code</h4><p name="9c5e" id="9c5e" class="graf graf--p graf-after--h4">This article is part of a <a href="https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c" data-href="https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">larger series</a> on using large language models (LLMs) in practice. In the <a href="https://towardsdatascience.com/how-to-improve-llms-with-rag-abdc132f76ac" data-href="https://towardsdatascience.com/how-to-improve-llms-with-rag-abdc132f76ac" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">previous post</a>, we saw how to improve an LLM via retrieval-augmented generation (i.e. RAG). A key part of RAG was using text embeddings to retrieve relevant information from a knowledge base automatically. Here, I will discuss text embeddings more deeply and share two simple (yet powerful) applications: text classification and semantic search.</p><figure name="1e68" id="1e68" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="0*wka3oBwbhobH8HW_" data-width="3840" data-height="2560" data-unsplash-photo-id="fr3YLb9UHSQ" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*wka3oBwbhobH8HW_"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@dlerman6?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@dlerman6?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Daniel Lerman</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure></div></div></section><section name="f29a" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="3ab0" id="3ab0" class="graf graf--p graf--leading">ChatGPT captured the world’s imagination regarding AI and its potential. A key contributor to this impact was ChatGPT’s chat interface, which made the power of AI more accessible than ever before.</p><p name="4002" id="4002" class="graf graf--p graf-after--p">While this unlocked a new level of AI hype and awareness, all the excitement around this “chatbot paradigm” left another key innovation (largely) unnoticed.</p><p name="4fae" id="4fae" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">LLMs brought</strong> <strong class="markup--strong markup--p-strong">major innovations</strong> <strong class="markup--strong markup--p-strong">in text embeddings</strong>. Here, I’ll explain these and how we can use them for simple yet high-value use cases.</p><figure name="83ed" id="83ed" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/sNa_uiqSlJo?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><h3 name="db66" id="db66" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">Text embeddings</strong></h3><p name="5de6" id="5de6" class="graf graf--p graf-after--h3">Text embeddings <strong class="markup--strong markup--p-strong">translate words into numbers</strong>. However, these aren’t just any numbers. They are numbers that capture the <em class="markup--em markup--p-em">meaning</em> of the underlying text.</p><p name="41df" id="41df" class="graf graf--p graf-after--p">This is important because numbers are (much) easier to analyze than words.</p><p name="07e3" id="07e3" class="graf graf--p graf-after--p">For example, if you are at a networking event and want to know the typical height of people in the room, you can measure everyone’s height and compute the average using Microsoft Excel. However, if you want to know the typical job title of the people in the room, there’s no Excel function to help you.</p><p name="3242" id="3242" class="graf graf--p graf-after--p">This is where text embeddings can help. If we have a good way to turn text into numbers, <strong class="markup--strong markup--p-strong">we unlock a massive toolbox of existing statistical and machine-learning techniques</strong> to investigate textual data.</p><h3 name="e320" id="e320" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Text embeddings (Visually Explained)</strong></h3><p name="9cd1" id="9cd1" class="graf graf--p graf-after--h3">Let&#39;s look at a visual example to better understand what it means to “translate text into numbers” [1].</p><p name="37c7" id="37c7" class="graf graf--p graf-after--p">Consider the following set of text: <em class="markup--em markup--p-em">tree</em>, <em class="markup--em markup--p-em">lotus</em> <em class="markup--em markup--p-em">flower</em>, <em class="markup--em markup--p-em">daisy</em>, <em class="markup--em markup--p-em">sun</em>, <em class="markup--em markup--p-em">Saturn</em>, <em class="markup--em markup--p-em">Jupiter</em>, <em class="markup--em markup--p-em">satellite</em>, <em class="markup--em markup--p-em">space</em> <em class="markup--em markup--p-em">shuttle</em>, <em class="markup--em markup--p-em">basketball</em>, and <em class="markup--em markup--p-em">baseball</em>.</p><p name="2cc8" id="2cc8" class="graf graf--p graf-after--p">While this may seem like a random assortment of words, <strong class="markup--strong markup--p-strong">some of these concepts are more <em class="markup--em markup--p-em">similar</em> than others</strong>. We can convey these similarities (and differences) in the following way.</p><figure name="e71f" id="e71f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*YHxNDy26jDdpEZe3vKQDpg.png" data-width="495" data-height="503" src="https://cdn-images-1.medium.com/max/800/1*YHxNDy26jDdpEZe3vKQDpg.png"><figcaption class="imageCaption">Visual representation of text embeddings. Image by author.</figcaption></figure><p name="773d" id="773d" class="graf graf--p graf-after--figure">The above visualization intuitively organizes the concepts. Similar items (e.g., tree, daisy, and lotus flower) tend to be close together, while dissimilar items (e.g., tree and baseball) tend to be far apart.</p><p name="43a5" id="43a5" class="graf graf--p graf-after--p">Numbers fit into this picture because we can assign coordinates to each word based on its location in the plot above. <strong class="markup--strong markup--p-strong">These coordinates (i.e. numbers) can then be used to analyze the underlying text</strong>.</p><figure name="27c7" id="27c7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NI36ZN1MTHxRePIcZ3LGPw.png" data-width="532" data-height="505" src="https://cdn-images-1.medium.com/max/800/1*NI36ZN1MTHxRePIcZ3LGPw.png"><figcaption class="imageCaption">Coordinates can be used to analyze relationships between text. Image by author.</figcaption></figure><h3 name="073c" id="073c" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">Where do they come from?</strong></h3><p name="364c" id="364c" class="graf graf--p graf-after--h3">Turning text into numbers to make it more computable is not a new idea. Researchers have explored this since the early days of computing (circa 1950) [2].</p><p name="e74f" id="e74f" class="graf graf--p graf-after--p">While there are countless ways people have done this over the years [2], these days, state-of-the-art text representations are derived from large language models (LLMs).</p><p name="3261" id="3261" class="graf graf--p graf-after--p">This works because <strong class="markup--strong markup--p-strong">LLMs learn (very) good numerical representations of text</strong> during their training. The layers that generate these representations can be dissected from the model and used in a stand-alone way. The result of this process is a text embedding model.</p><figure name="7320" id="7320" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*hFYc2borW8dsRqyePdrw_A.png" data-width="1473" data-height="349" src="https://cdn-images-1.medium.com/max/800/1*hFYc2borW8dsRqyePdrw_A.png"><figcaption class="imageCaption">3-step recipe for creating an embedding model. Image by author.</figcaption></figure><h3 name="26fc" id="26fc" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">Can’t I just use ChatGPT?</strong></h3><p name="a8e8" id="a8e8" class="graf graf--p graf-after--h3">Before moving on to the example use cases, you might think, “<em class="markup--em markup--p-em">Shaw, why should I care about these text embedding things? Can’t I just make a custom ChatGPT to analyze the text for me?</em>”</p><p name="880d" id="880d" class="graf graf--p graf-after--p">Of course, one can use techniques like <a href="https://towardsdatascience.com/how-to-improve-llms-with-rag-abdc132f76ac" data-href="https://towardsdatascience.com/how-to-improve-llms-with-rag-abdc132f76ac" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">RAG</a> or <a href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--p-anchor" target="_blank">Fine-tuning</a> to build an AI agent tailored to their specific problem set. However, it’s still the early days for these systems, which makes <strong class="markup--strong markup--p-strong">building a robust AI agent</strong> (i.e. not a prototype) <strong class="markup--strong markup--p-strong">an expensive and non-trivial engineering problem</strong> (e.g. major computational costs, <a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/" data-href="https://owasp.org/www-project-top-10-for-large-language-model-applications/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LLM security risks</a>, unpredictable responses, and hallucinations)</p><p name="b9ec" id="b9ec" class="graf graf--p graf-after--p">On the other hand, text embeddings have been around for decades, are lightweight, and non-stochastic (i.e. predictable). Thus, building AI systems with them is <strong class="markup--strong markup--p-strong">much simpler and cheaper than building an AI agent</strong> (while still capturing much of the value — if not more).</p><h3 name="9625" id="9625" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Use Case 1: Text Classification</strong></h3><p name="f467" id="f467" class="graf graf--p graf-after--h3">With a basic understanding of text embeddings, let’s see how we can use them to help solve real-world problems.</p><p name="55d1" id="55d1" class="graf graf--p graf-after--p">First, we will discuss <strong class="markup--strong markup--p-strong">text classification</strong>. This consists of <strong class="markup--strong markup--p-strong">assigning a label to a given text</strong>. For example, labeling an email as spam or not spam, a credit application as high risk or low risk, or a security alert as the real deal or a false alarm.</p><p name="03dc" id="03dc" class="graf graf--p graf-after--p">In this example, I will use text embeddings to classify resumes as “<em class="markup--em markup--p-em">Data Scientist</em>” or “<em class="markup--em markup--p-em">Not Data Scientist</em>,” which may be relevant for recruiters trying to navigate an ocean of job candidates.</p><p name="b678" id="b678" class="graf graf--p graf-after--p">To avoid privacy issues, I created a synthetic dataset of resumes using <a href="https://platform.openai.com/docs/models/gpt-3-5-turbo" data-href="https://platform.openai.com/docs/models/gpt-3-5-turbo" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">gpt-3.5-turbo</a>. While using synthetic data requires us to take the results with a grain of salt, this example still provides an instructive demonstration of how to use text embeddings to classify.</p><p name="3b62" id="3b62" class="graf graf--p graf-after--p graf--trailing">The example code and data are freely available at the <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/text-embeddings" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/text-embeddings" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub repository</a>.</p></div></div></section><section name="3ca3" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h4 name="3a2c" id="3a2c" class="graf graf--h4 graf--leading">Imports</h4><p name="f5c5" id="f5c5" class="graf graf--p graf-after--h4">We start by importing dependencies. In this example, I use a text embedding model from <a href="https://platform.openai.com/docs/guides/embeddings" data-href="https://platform.openai.com/docs/guides/embeddings" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenAI</a>, which requires an API key. If you are unfamiliar with the OpenAI API, I give a beginner-friendly primer in a <a href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971" data-href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">previous article</a> of this series. Here, my API key is stored in a separate file called <em class="markup--em markup--p-em">sk.py</em>.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="dbb5" id="dbb5" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> openai<br /><span class="hljs-keyword">from</span> sk <span class="hljs-keyword">import</span> my_sk<br /><br /><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br /><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><br /><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br /><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> roc_auc_score</span></pre><p name="a909" id="a909" class="graf graf--p graf-after--pre">Next, we read our synthetic training dataset as a Pandas dataframe. The data comes from a .csv file with two columns consisting of resume text and the associated role.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="6814" id="6814" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">df_resume = pd.read_csv(<span class="hljs-string">&#x27;resumes/resumes_train.csv&#x27;</span>)</span></pre><h4 name="c56b" id="c56b" class="graf graf--h4 graf-after--pre">Generate Embeddings</h4><p name="16e5" id="16e5" class="graf graf--p graf-after--h4">To translate the resumes into embeddings, we can make a simple API call to the OpenAI API. This is done by the function below.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="d506" id="d506" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_embeddings</span>(<span class="hljs-params">text, my_sk</span>):<br />    <span class="hljs-comment"># set credentials</span><br />    client = openai.OpenAI(api_key = my_sk)<br />    <br />    <span class="hljs-comment"># make api call</span><br />    response = client.embeddings.create(<br />        <span class="hljs-built_in">input</span>=text,<br />        model=<span class="hljs-string">&quot;text-embedding-3-small&quot;</span><br />    )<br />    <br />    <span class="hljs-comment"># return text embedding</span><br />    <span class="hljs-keyword">return</span> response.data</span></pre><p name="a28a" id="a28a" class="graf graf--p graf-after--pre">We can now apply this function to each resume in our dataframe and store the result in a list.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="1562" id="1562" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># generate embeddings</span><br />text_embeddings = generate_embeddings(df_resume[<span class="hljs-string">&#x27;resume&#x27;</span>], my_sk)<br /><br /><span class="hljs-comment"># extract embeddings</span><br />text_embedding_list = <br />  [text_embeddings[i].embedding <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(text_embeddings))]</span></pre><h4 name="3bff" id="3bff" class="graf graf--h4 graf-after--pre">Store Embeddings in Dataframe</h4><p name="8685" id="8685" class="graf graf--p graf-after--h4">Next, we’ll create a new dataframe to store the text embeddings and our target variable for model training.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="8698" id="8698" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># define df column names</span><br />column_names = <br />  [<span class="hljs-string">&quot;embedding_&quot;</span> + <span class="hljs-built_in">str</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(text_embedding_list[<span class="hljs-number">0</span>]))]<br /><br /><span class="hljs-comment"># store text embeddings in dataframe</span><br />df_train = pd.DataFrame(text_embedding_list, columns=column_names)<br /><br /><span class="hljs-comment"># create target variable</span><br />df_train[<span class="hljs-string">&#x27;is_data_scientist&#x27;</span>] = df_resume[<span class="hljs-string">&#x27;role&#x27;</span>]==<span class="hljs-string">&quot;Data Scientist&quot;</span></span></pre><h4 name="6b89" id="6b89" class="graf graf--h4 graf-after--pre">Model Training</h4><p name="205a" id="205a" class="graf graf--p graf-after--h4">With our training data prepared we can now train our classification model in one line of code. Here, I use a Random Forest classifier, which I discussed in a <a href="https://towardsdatascience.com/10-decision-trees-are-better-than-1-719406680564" data-href="https://towardsdatascience.com/10-decision-trees-are-better-than-1-719406680564" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">past article</a>.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="f42d" id="f42d" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># split variables by predictors and target</span><br />X = df_train.iloc[:,:-<span class="hljs-number">1</span>]<br />y = df_train.iloc[:,-<span class="hljs-number">1</span>]<br /><br /><span class="hljs-comment"># train rf model</span><br />clf = RandomForestClassifier(max_depth=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">0</span>).fit(X, y)</span></pre><h4 name="5c27" id="5c27" class="graf graf--h4 graf-after--pre">Model Evaluation</h4><p name="8f27" id="8f27" class="graf graf--p graf-after--h4">To get a quick sense of the model’s performance we can evaluate it on the training data. Here, I compute the <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score" data-href="https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">mean accuracy</a> and area under the ROC (i.e., <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics" data-href="https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AUC</a>).</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="8bca" id="8bca" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># model accuracy for training data</span><br /><span class="hljs-built_in">print</span>(clf.score(X,y))<br /><br /><span class="hljs-comment"># AUC value for training data</span><br /><span class="hljs-built_in">print</span>(roc_auc_score(y, clf.predict_proba(X)[:,<span class="hljs-number">1</span>]))<br /><br /><span class="hljs-comment"># output</span><br /><span class="hljs-comment"># 1</span><br /><span class="hljs-comment"># 1</span></span></pre><p name="d5be" id="d5be" class="graf graf--p graf-after--pre">The accuracy and AUC values of 1 indicate perfect performance on the training dataset, which is <em class="markup--em markup--p-em">suspicious</em>. So let’s evaluate it on a testing dataset the model has never seen before.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="makefile" name="fa2d" id="fa2d" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># import testing data</span><br />df_resume = pd.read_csv(&#x27;resumes/resumes_test.csv&#x27;)<br /><br /><span class="hljs-comment"># generate embeddings</span><br />text_embedding_list = generate_embeddings(df_resume[&#x27;resume&#x27;], my_sk)<br />text_embedding_list = <br />  [text_embedding_list[i].embedding for i in range(len(text_embedding_list))]<br /><br /><span class="hljs-comment"># store text embeddings in dataframe</span><br />df_test = pd.DataFrame(text_embedding_list, columns=column_names)<br /><br /><span class="hljs-comment"># create target variable</span><br />df_test[&#x27;is_data_scientist&#x27;] = df_resume[&#x27;role&#x27;]==<span class="hljs-string">&quot;Data Scientist&quot;</span><br /><br /><span class="hljs-comment"># define predictors and target</span><br />X_test = df_test.iloc[:,:-1]<br />y_test = df_test.iloc[:,-1]</span></pre><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="49c3" id="49c3" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content"><span class="hljs-comment"># model accuracy for testing data</span><br /><span class="hljs-built_in">print</span>(clf.score(X_test,y_test))<br /><br /><span class="hljs-comment"># AUC value for testing data</span><br /><span class="hljs-built_in">print</span>(roc_auc_score(y_test, clf.predict_proba(X_test)[:,<span class="hljs-number">1</span>]))<br /><br /><span class="hljs-comment"># output</span><br /><span class="hljs-comment"># 0.98</span><br /><span class="hljs-comment"># 0.9983333333333333</span></span></pre><h4 name="4b85" id="4b85" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Resolving overfitting</strong></h4><p name="d982" id="d982" class="graf graf--p graf-after--h4">Although the model performs well when applied to the testing data, it is still likely overfitting for two reasons.</p><p name="f1b8" id="f1b8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">One</strong>, we have 1537 predictors and only 100 resumes to predict, so it wouldn’t be hard for the model to “memorize” every example in the training data. <strong class="markup--strong markup--p-strong">Two</strong>, the training and testing data were generated from <a href="https://platform.openai.com/docs/models/gpt-3-5-turbo" data-href="https://platform.openai.com/docs/models/gpt-3-5-turbo" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">gpt-3.5-turbo</a> in a similar way. Thus, they share many characteristics, which makes the classification task easier than if applied to real data.</p><p name="c5ff" id="c5ff" class="graf graf--p graf-after--p">There are many tricks we can employ to <strong class="markup--strong markup--p-strong">overcome the overfitting problem</strong>, e.g., reducing predictor count using predictor importance ranking, increasing the minimum number of samples in a leaf node, or using a simpler classification technique like logistic regression. However, if our goal is to use this model in a practical setting, the best option would be to gather <strong class="markup--strong markup--p-strong">more data</strong> and use <strong class="markup--strong markup--p-strong">resumes from the real world</strong>.</p><h3 name="185f" id="185f" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Use Case 2: Semantic Search</strong></h3><p name="f68a" id="f68a" class="graf graf--p graf-after--h3">Next, let’s look at semantic search. In contrast to keyword-based search, semantic search<strong class="markup--strong markup--p-strong"> generates results based on the <em class="markup--em markup--p-em">meaning</em> of a user’s query </strong>rather than the particular words or phrases used.</p><p name="f182" id="f182" class="graf graf--p graf-after--p">For example, keyword-based searches may not provide great results for the query “<em class="markup--em markup--p-em">I need someone to build out my data infrastructure</em>” since it doesn’t specifically mention the role that builds data infrastructure (i.e., a data engineer). However, this is not a concern for semantic search since it can match the query to candidates with experience like “<em class="markup--em markup--p-em">Proficient in data modeling, ETL processes, and data warehousing.</em>”</p><p name="aabc" id="aabc" class="graf graf--p graf-after--p graf--trailing">Here, we will use text embeddings to enable this type of search over the same dataset as in the previous use case. Example code is (again) available at the <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/text-embeddings" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/text-embeddings" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub repo</a>.</p></div></div></section><section name="c75d" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h4 name="e300" id="e300" class="graf graf--h4 graf--leading">Imports</h4><p name="7b4a" id="7b4a" class="graf graf--p graf-after--h4">We start by importing dependencies and the synthetic dataset.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="68b1" id="68b1" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br /><br /><span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer<br /><br /><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br /><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> DistanceMetric<br /><br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">import</span> matplotlib <span class="hljs-keyword">as</span> mpl</span></pre><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="68e8" id="68e8" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content">df_resume = pd.read_csv(<span class="hljs-string">&#x27;resumes/resumes_train.csv&#x27;</span>)<br /><br /><span class="hljs-comment"># relabel random role as &quot;other&quot;</span><br />df_resume[<span class="hljs-string">&#x27;role&#x27;</span>][df_resume[<span class="hljs-string">&#x27;role&#x27;</span>].iloc[-<span class="hljs-number">1</span>] == df_resume[<span class="hljs-string">&#x27;role&#x27;</span>]] = <span class="hljs-string">&quot;Other&quot;</span> </span></pre><h4 name="06eb" id="06eb" class="graf graf--h4 graf-after--pre">Generate Embeddings</h4><p name="3498" id="3498" class="graf graf--p graf-after--h4">Next, we’ll generate the text embeddings. Instead of using the <a href="https://platform.openai.com/docs/guides/embeddings" data-href="https://platform.openai.com/docs/guides/embeddings" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenAI API</a>, we will use an open-source model from the <a href="https://www.sbert.net/" data-href="https://www.sbert.net/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Sentence Transformers</a> Python library. This model was specifically fine-tuned for semantic search.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="1210" id="1210" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># import pre-trained model (full list: https://www.sbert.net/docs/pretrained_models.html)</span><br />model = SentenceTransformer(<span class="hljs-string">&quot;all-MiniLM-L6-v2&quot;</span>)<br /><br /><span class="hljs-comment"># encode text</span><br />embedding_arr = model.encode(df_resume[<span class="hljs-string">&#x27;resume&#x27;</span>])</span></pre><p name="e0a7" id="e0a7" class="graf graf--p graf-after--pre">To see the different resumes in the dataset and their relative locations in concept space, we can use <a href="https://towardsdatascience.com/principal-component-analysis-pca-79d228eb9d24" data-href="https://towardsdatascience.com/principal-component-analysis-pca-79d228eb9d24" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">PCA</a> to reduce the dimensionality of the embedding vectors and visualize the data on a 2D plot (code is on <a href="https://github.com/ShawhinT/YouTube-Blog/blob/main/LLMs/text-embeddings/2-semantic-search.ipynb" data-href="https://github.com/ShawhinT/YouTube-Blog/blob/main/LLMs/text-embeddings/2-semantic-search.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>).</p><p name="a7a0" id="a7a0" class="graf graf--p graf-after--p">From this view we see the resumes for a given role tend to clump together.</p><figure name="a0c7" id="a0c7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6NMvNijuM5QLdgDwDKritg.png" data-width="843" data-height="411" src="https://cdn-images-1.medium.com/max/800/1*6NMvNijuM5QLdgDwDKritg.png"><figcaption class="imageCaption">2D plot of resume embeddings colored by role. Image by author.</figcaption></figure><h4 name="8d6a" id="8d6a" class="graf graf--h4 graf-after--figure">Semantic Search</h4><p name="37a2" id="37a2" class="graf graf--p graf-after--h4">Now, to do a semantic search over these resumes, we can take a user query, translate it into a text embedding, and then return the nearest resumes in the embedding space. Here’s what that looks like in code.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="a353" id="a353" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># define query</span><br />query = <span class="hljs-string">&quot;I need someone to build out my data infrastructure&quot;</span><br /><br /><span class="hljs-comment"># encode query</span><br />query_embedding = model.encode(query)</span></pre><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="d202" id="d202" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content"><span class="hljs-comment"># define distance metric (other options: manhattan, chebyshev)</span><br />dist = DistanceMetric.get_metric(<span class="hljs-string">&#x27;euclidean&#x27;</span>)<br /><br /><span class="hljs-comment"># compute pair-wise distances between query embedding and resume embeddings</span><br />dist_arr = <br />  dist.pairwise(embedding_arr, query_embedding.reshape(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)).flatten()<br /><span class="hljs-comment"># sort results</span><br />idist_arr_sorted = np.argsort(dist_arr)</span></pre><p name="c47d" id="c47d" class="graf graf--p graf-after--pre">Printing the roles of the top 10 results, we see almost all are data engineers, which is a good sign.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="150c" id="150c" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># print roles of top 10 closest resumes to query in embedding space</span><br /><span class="hljs-built_in">print</span>(df_resume[<span class="hljs-string">&#x27;role&#x27;</span>].iloc[idist_arr_sorted[:<span class="hljs-number">10</span>]])</span></pre><figure name="9375" id="9375" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*qWG9RH-oImP9vkBPyqHl_w.png" data-width="342" data-height="316" src="https://cdn-images-1.medium.com/max/800/1*qWG9RH-oImP9vkBPyqHl_w.png"></figure><p name="b646" id="b646" class="graf graf--p graf-after--figure">Let’s look at the resume of the top search results.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="91f4" id="91f4" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># print resume closest to query in embedding space</span><br /><span class="hljs-built_in">print</span>(df_resume[<span class="hljs-string">&#x27;resume&#x27;</span>].iloc[idist_arr_sorted[<span class="hljs-number">0</span>]])</span></pre><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="plaintext" name="085b" id="085b" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content">**John Doe**<br /><br />---<br /><br />**Summary:**<br />Highly skilled and experienced Data Engineer with a strong background in <br />designing, implementing, and maintaining data pipelines. Proficient in data <br />modeling, ETL processes, and data warehousing. Adept at working with large <br />datasets and optimizing data workflows to improve efficiency.<br /><br />---<br /><br />**Professional Experience:**<br />- **Senior Data Engineer**  <br />  XYZ Tech, Anytown, USA  <br />  June 2018 - Present  <br />  - Designed and developed scalable data pipelines to handle terabytes of data daily.<br />  - Optimized ETL processes to improve data quality and processing time by 30%.<br />  - Collaborated with cross-functional teams to implement data architecture best practices.<br /><br />- **Data Engineer**  <br />  ABC Solutions, Sometown, USA  <br />  January 2015 - May 2018  <br />  - Built and maintained data pipelines for real-time data processing.<br />  - Developed data models and implemented data governance policies.<br />  - Worked on data integration projects to streamline data access for business users.<br /><br />---<br /><br />**Education:**<br />- **Master of Science in Computer Science**  <br />  University of Technology, Cityville, USA  <br />  Graduated: 2014<br /><br />- **Bachelor of Science in Computer Engineering**  <br />  State College, Hometown, USA  <br />  Graduated: 2012<br /><br />---<br /><br />**Technical Skills:**<br />- Programming: Python, SQL, Java<br />- Big Data Technologies: Hadoop, Spark, Kafka<br />- Databases: MySQL, PostgreSQL, MongoDB<br />- Data Warehousing: Amazon Redshift, Snowflake<br />- ETL Tools: Apache NiFi, Talend<br />- Data Visualization: Tableau, Power BI<br /><br />---<br /><br />**Certifications:**<br />- Certified Data Management Professional (CDMP)<br />- AWS Certified Big Data - Specialty<br /><br />---<br /><br />**Awards and Honors:**<br />- Employee of the Month - XYZ Tech (July 2020)<br />- Outstanding Achievement in Data Engineering - ABC Solutions (2017)</span></pre><p name="5801" id="5801" class="graf graf--p graf-after--pre">Although this is a made-up resume, the candidate likely has all the necessary skills and experience to fulfill the user’s needs.</p><p name="3ac0" id="3ac0" class="graf graf--p graf-after--p">Another way to look at the search results is via the 2D plot from before. Here’s what that looks like for a few queries (see plot titles).</p><figure name="b16c" id="b16c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*SdJwEcsXf9s2cZ-fKsRGEg.png" data-width="1764" data-height="676" src="https://cdn-images-1.medium.com/max/800/1*SdJwEcsXf9s2cZ-fKsRGEg.png"><figcaption class="imageCaption">2D PCA plots for 3 different queries. Image by author.</figcaption></figure><h4 name="819f" id="819f" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Improving search</strong></h4><p name="c2ea" id="c2ea" class="graf graf--p graf-after--h4">While this simple search example does a good job of matching particular candidates to a given query, it is not perfect. One shortcoming is when the user query includes a specific skill. For example, in the query “<em class="markup--em markup--p-em">Data Engineer with Apache Airflow experience</em>,” only 1 of the top 5 results have Airflow experience.</p><p name="c84d" id="c84d" class="graf graf--p graf-after--p">This highlights that <strong class="markup--strong markup--p-strong">semantic search is not better than keyword-based search in all situations</strong>. Each has its strengths and weaknesses.</p><p name="6e94" id="6e94" class="graf graf--p graf-after--p">Thus, a robust search system will employ so-called <strong class="markup--strong markup--p-strong">hybrid search</strong>, which <strong class="markup--strong markup--p-strong">combines the best of both techniques</strong>. While there are many ways to design such a system, a simple approach is applying keyword-based search to filter down results, followed by semantic search.</p><p name="ab71" id="ab71" class="graf graf--p graf-after--p">Two additional strategies for improving search are using a <strong class="markup--strong markup--p-strong">Reranker</strong> and <strong class="markup--strong markup--p-strong">fine-tuning text embeddings</strong>.</p><p name="a954" id="a954" class="graf graf--p graf-after--p">A <strong class="markup--strong markup--p-strong">Reranker</strong> is a model that <strong class="markup--strong markup--p-strong">directly compares two pieces of text</strong>. In other words, instead of computing the similarity between pieces of text via a distance metric in the embedding space, a Reranker computes such a similarity score directly.</p><p name="53de" id="53de" class="graf graf--p graf-after--p">Rerankers are commonly used to refine search results. For example, one can return the top 25 results using semantic search and then refine to the top 5 with a Reranker.</p><p name="f887" id="f887" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Fine-tuning text embeddings</strong> involves <strong class="markup--strong markup--p-strong">adapting an embedding model for a particular domain</strong>. This is a powerful approach because most embedding models are based on a broad collection of text and knowledge. Thus, they may not optimally organize concepts for a specific industry, e.g. data science and AI.</p><div name="c9fa" id="c9fa" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/text-embeddings" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/text-embeddings" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/text-embeddings"><strong class="markup--strong markup--mixtapeEmbed-strong">YouTube-Blog/LLMs/text-embeddings at main · ShawhinT/YouTube-Blog</strong><br><em class="markup--em markup--mixtapeEmbed-em">Codes to complement YouTube videos and blog posts on Medium. - YouTube-Blog/LLMs/text-embeddings at main ·…</em>github.com</a><a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/text-embeddings" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="44c71978ae132654e35e4e81c5795d86" data-thumbnail-img-id="0*hcWTHkiAv9Ci6Vks" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*hcWTHkiAv9Ci6Vks);"></a></div><h3 name="f04a" id="f04a" class="graf graf--h3 graf-after--mixtapeEmbed">Conclusion</h3><p name="5878" id="5878" class="graf graf--p graf-after--h3">Although everyone seems focused on the potential for AI agents and assistants, recent innovations in text-embedding models have unlocked countless opportunities for simple yet high-value ML use cases.</p><p name="017c" id="017c" class="graf graf--p graf-after--p">Here, we reviewed two widely applicable use cases: text classification and semantic search. Text embeddings enable simpler and cheaper alternatives to LLM-based methods while still capturing much of the value.</p><p name="3fde" id="3fde" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">More on LLMs 👇</strong></p><div name="dad4" id="dad4" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/list/8e009ae3054c" data-href="https://shawhin.medium.com/list/8e009ae3054c" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/list/8e009ae3054c"><strong class="markup--strong markup--mixtapeEmbed-strong">Large Language Models (LLMs)</strong><br><em class="markup--em markup--mixtapeEmbed-em">A series on how to use LLMs in practice.</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/list/8e009ae3054c" class="js-mixtapeImage mixtapeImage mixtapeImage--mediumCatalog  u-ignoreBlock" data-media-id="e38ca2d0de6aa61d7c09af603d8f43e1" data-thumbnail-img-id="0*da96b7b2a8aed028c258b2c1094bb562cea2510e.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/304/160/0*da96b7b2a8aed028c258b2c1094bb562cea2510e.jpeg);"></a></div></div></div></section><section name="aa2b" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="85ed" id="85ed" class="graf graf--h3 graf--leading">Resources</h3><p name="9827" id="9827" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Connect</strong>: <a href="https://shawhintalebi.com/" data-href="https://shawhintalebi.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">My website</a> | <a href="https://calendly.com/shawhintalebi" data-href="https://calendly.com/shawhintalebi" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Book a call</a></p><p name="1e32" id="1e32" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Socials</strong>: <a href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" data-href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">YouTube 🎥</a> | <a href="https://www.linkedin.com/in/shawhintalebi/" data-href="https://www.linkedin.com/in/shawhintalebi/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">LinkedIn</a> | <a href="https://www.instagram.com/shawhintalebi" data-href="https://www.instagram.com/shawhintalebi" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Instagram</a></p><p name="f742" id="f742" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Support</strong>: <a href="https://www.buymeacoffee.com/shawhint" data-href="https://www.buymeacoffee.com/shawhint" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Buy me a coffee</a> ☕️</p><div name="fe26" id="fe26" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/subscribe" data-href="https://shawhin.medium.com/subscribe" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/subscribe"><strong class="markup--strong markup--mixtapeEmbed-strong">Get FREE access to every new story I write</strong><br><em class="markup--em markup--mixtapeEmbed-em">Get FREE access to every new story I write P.S. I do not share your email with anyone By signing up, you will create a…</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/subscribe" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8d6d6c2d301f2c367b618a850fa468b3" data-thumbnail-img-id="0*O69FxQ5FttNEA_r4" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*O69FxQ5FttNEA_r4);"></a></div></div></div></section><section name="33a6" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="b8e9" id="b8e9" class="graf graf--p graf--leading">[1] <a href="https://youtu.be/A8HEPBdKVMA?si=PA4kCnfgd3nx24LR" data-href="https://youtu.be/A8HEPBdKVMA?si=PA4kCnfgd3nx24LR" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://youtu.be/A8HEPBdKVMA?si=PA4kCnfgd3nx24LR</a></p><p name="3b25" id="3b25" class="graf graf--p graf-after--p graf--trailing">[2] R. Patil, S. Boit, V. Gudivada and J. Nandigam, “<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10098736" data-href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10098736" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">A Survey of Text Representation and Embedding Techniques in NLP</a>,” in IEEE Access, vol. 11, pp. 36120–36146, 2023, doi: 10.1109/ACCESS.2023.3266377.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@shawhin" class="p-author h-card">Shaw Talebi</a> on <a href="https://medium.com/p/8291746220be"><time class="dt-published" datetime="2024-03-27T13:04:18.819Z">March 27, 2024</time></a>.</p><p><a href="https://medium.com/@shawhin/text-embeddings-classification-and-semantic-search-8291746220be" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 2, 2024.</p></footer></article></body></html>