<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>QLoRA — How to Fine-Tune an LLM on a Single GPU</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">QLoRA — How to Fine-Tune an LLM on a Single GPU</h1>
</header>
<section data-field="subtitle" class="p-summary">
An introduction with Python example code (ft. Mistral-7b)
</section>
<section data-field="body" class="e-content">
<section name="c932" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="4681" id="4681" class="graf graf--h3 graf--leading graf--title"><strong class="markup--strong markup--h3-strong">QLoRA — How to Fine-Tune an LLM on a Single GPU</strong></h3><h4 name="b693" id="b693" class="graf graf--h4 graf-after--h3 graf--subtitle">An introduction with Python example code (ft. Mistral-7b)</h4><p name="fc6c" id="fc6c" class="graf graf--p graf-after--h4">This article is part of a <a href="https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c" data-href="https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">larger series</a> on using large language models (LLMs) in practice. In the <a href="https://towardsdatascience.com/how-to-build-an-ai-assistant-with-openai-python-8b3b5a636f69" data-href="https://towardsdatascience.com/how-to-build-an-ai-assistant-with-openai-python-8b3b5a636f69" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">previous post</a>, we saw how to fine-tune an LLM using OpenAI. The main limitation to this approach, however, is that OpenAI’s models are concealed behind their API, which limits what and how we can build with them. Here, I’ll discuss an alternative way to fine-tune an LLM using open-source models and QLoRA.</p><figure name="1f77" id="1f77" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="0*C783ZfFIphu8eFdm" data-width="9600" data-height="5400" data-unsplash-photo-id="yNvVnPcurD8" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*C783ZfFIphu8eFdm"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@dell?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@dell?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Dell</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure></div></div></section><section name="f1d0" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="de3d" id="de3d" class="graf graf--p graf--leading"><strong class="markup--strong markup--p-strong">Fine-tuning</strong> is when we <strong class="markup--strong markup--p-strong">take an existing model and tweak it for a particular use case</strong>. This has been a critical part of the recent explosion of AI innovations, giving rise to ChatGPT and the like.</p><p name="2aaf" id="2aaf" class="graf graf--p graf-after--p">Although fine-tuning is a simple (and powerful) idea, applying it to LLMs isn’t always straightforward. The key challenge is that <strong class="markup--strong markup--p-strong">LLMs are (very) computationally expensive</strong> (i.e. they aren’t something that can be trained on a typical laptop).</p><p name="c81e" id="c81e" class="graf graf--p graf-after--p">For example, standard fine-tuning of a 70B parameter model requires over 1TB of memory [1]. For context, an A100 GPU comes with up to 80GB of memory, so you’d (at best) need over a dozen of these $20,000 cards!</p><p name="b61e" id="b61e" class="graf graf--p graf-after--p">While this may deflate your dreams of building a custom AI, don’t give up just yet. The open-source community has been working hard to make building with these models more accessible. One popular method that has sprouted from these efforts is <strong class="markup--strong markup--p-strong">QLoRA (Quantized Low-Rank Adaptation)</strong>, <strong class="markup--strong markup--p-strong">an efficient way to fine-tune a model without sacrificing performance</strong>.</p><figure name="566e" id="566e" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/XpoKB3usmKc?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><h3 name="8b23" id="8b23" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">What is Quantization?</strong></h3><p name="88d0" id="88d0" class="graf graf--p graf-after--h3">A key part of QLoRA is so-called <strong class="markup--strong markup--p-strong">quantization</strong>. While this might sound like a scary and sophisticated word, it is a simple idea. When you hear “<em class="markup--em markup--p-em">quantizing</em>,” think of <strong class="markup--strong markup--p-strong">splitting a range of numbers into buckets</strong>.</p><p name="9840" id="9840" class="graf graf--p graf-after--p">For example, there are infinite possible numbers between 0 and 100, e.g. 1, 12, 27, 55.3, 83.7823, and so on. We could <em class="markup--em markup--p-em">quantize</em> this range by splitting them into buckets based on whole numbers so that (1, 12, 27, 55.3, 83.7823) becomes (1, 12, 27, 55, 83), or we could use factors of ten so that the numbers become (0, 0, 20, 50, 80). A visualization of this process is shown below.</p><figure name="5d02" id="5d02" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*wO1Qiz1NGDOfTTbFsmHloA.png" data-width="1033" data-height="633" src="https://cdn-images-1.medium.com/max/800/1*wO1Qiz1NGDOfTTbFsmHloA.png"><figcaption class="imageCaption">Visualization of quantizing numbers via whole numbers or 10s. Image by author.</figcaption></figure><h4 name="21f7" id="21f7" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Why we need it</strong></h4><p name="4ebf" id="4ebf" class="graf graf--p graf-after--h4">Quantization allows us to <strong class="markup--strong markup--p-strong">represent a given set of numbers with less <em class="markup--em markup--p-em">information</em></strong>. To see why this is important, let’s (briefly) talk about how computers work.</p><p name="5e24" id="5e24" class="graf graf--p graf-after--p">Computers encode <em class="markup--em markup--p-em">information</em> using binary digits (i.e. bits). For instance, if I want a computer to remember the number 83.7823, this number needs to be translated into a string of 1s and 0s (aka a bit string).</p><p name="3e1f" id="3e1f" class="graf graf--p graf-after--p">One way of doing this is via the <strong class="markup--strong markup--p-strong">single-precision floating-point format</strong> (i.e. <strong class="markup--strong markup--p-strong">FP32</strong>), which represents numbers as a sequence of 32 bits [2]. For example, 83.7823 can be represented as 01000010101001111001000010001010 [3].</p><p name="dcd8" id="dcd8" class="graf graf--p graf-after--p">Since a string of 32 bits has 2³² (= 4,294,967,296) unique combinations that means we can represent 4,294,967,296 unique values with FP32. Thus, if we have numbers from 0 to 100, the <strong class="markup--strong markup--p-strong">bit count sets the precision for representing numbers in that range</strong>.</p><p name="6801" id="6801" class="graf graf--p graf-after--p">But there is <strong class="markup--strong markup--p-strong">another side of the story</strong>. If we use 32 bits to represent each model parameter, each parameter will take up 4 bytes of memory (1 byte = 8 bits). Therefore, a 10B parameter model will consume 40 GB of memory. And if we want to do full parameter fine-tuning, <strong class="markup--strong markup--p-strong">that will require closer to 200GB of memory! </strong>[1]</p><p name="941e" id="941e" class="graf graf--p graf-after--p">This presents a dilemma for fine-tuning LLMs. Namely, <strong class="markup--strong markup--p-strong">we want high precision</strong> for successful model training, <strong class="markup--strong markup--p-strong">but we need to use as little memory as possible</strong> to ensure we don’t run out of it. Balancing this tradeoff is a key contribution of QLoRA.</p><h3 name="514b" id="514b" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">QLoRA</strong></h3><p name="42b4" id="42b4" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">QLoRA (or Quantized Low-Rank Adaptation)</strong> combines 4 ingredients to get the most out of a machine’s limited memory <strong class="markup--strong markup--p-strong">without sacrificing model performance</strong>. I will briefly summarize key points from each. More details are available in the QLoRA paper [4].</p><h4 name="e9b5" id="e9b5" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Ingredient 1: 4-bit NormalFloat</strong></h4><p name="b374" id="b374" class="graf graf--p graf-after--h4">This first ingredient takes the idea of quantization near its practical limits. In contrast to the typical 16-bit data type (i.e., half-precision floating point) used for language model parameters, QLoRA uses a special data type called <strong class="markup--strong markup--p-strong">4-bit NormalFloat</strong>.</p><p name="9ce0" id="9ce0" class="graf graf--p graf-after--p">As the name suggests, this data type encodes numbers with just 4 bits. While this means we only have 2⁴ (= 16) buckets to represent model parameters, 4-bit NormalFloat uses a <strong class="markup--strong markup--p-strong">special trick to get more out of the limited information capacity</strong>.</p><p name="2cf3" id="2cf3" class="graf graf--p graf-after--p">The naive way to quantize a set of numbers is what we saw earlier, where we split the numbers into <strong class="markup--strong markup--p-strong">equally-spaced</strong> <strong class="markup--strong markup--p-strong">buckets</strong>. However, a more efficient way would be to use <strong class="markup--strong markup--p-strong">equally-sized buckets</strong>. The difference between these two approaches is illustrated in the figure below.</p><figure name="8161" id="8161" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dgd3vP3r8v6WczxDZRaX_Q.png" data-width="632" data-height="739" src="https://cdn-images-1.medium.com/max/800/1*dgd3vP3r8v6WczxDZRaX_Q.png"><figcaption class="imageCaption">Difference between equally-spaced and equally-sized buckets</figcaption></figure><p name="668d" id="668d" class="graf graf--p graf-after--figure">More specifically, 4-bit NormalFloat employs an information-theoretically optimal quantization strategy for normally distributed data [4]. Since model parameters tend to clump around 0, this is an effective strategy for representing LLM parameters.</p><h4 name="4102" id="4102" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Ingredient 2: Double Quantization</strong></h4><p name="ebe1" id="ebe1" class="graf graf--p graf-after--h4">Despite the unfortunate name, <strong class="markup--strong markup--p-strong">double quantization</strong> generates memory savings by <strong class="markup--strong markup--p-strong">quantizing the quantization constants</strong> (see what I mean).</p><p name="878f" id="878f" class="graf graf--p graf-after--p">To break this down, consider the following quantization process. Given an FP32 tensor, a simple way to quantize it is using the mathematical formula below [4].</p><figure name="610e" id="610e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*v_Zz1JtE8A-lAn-ADfkVCA.png" data-width="1218" data-height="220" src="https://cdn-images-1.medium.com/max/800/1*v_Zz1JtE8A-lAn-ADfkVCA.png"><figcaption class="imageCaption">Simple quantization formula from FP32 to Int8. Example from [4]. Image by author.</figcaption></figure><p name="bfcd" id="bfcd" class="graf graf--p graf-after--figure">Here we are converting the FP32 representation into an Int8 (8-bit integer) representation within the range of [-127, 127]. Notice this boils down to <strong class="markup--strong markup--p-strong">rescaling the values</strong> in the tensor <em class="markup--em markup--p-em">X^(FP32)</em> and <strong class="markup--strong markup--p-strong">then rounding them</strong> to the nearest integer. We can then simplify the equation by defining a scaling term (or quantization constant) <em class="markup--em markup--p-em">c^FP32 = 127/absmax(X^FP32))</em>.</p><p name="c2d0" id="c2d0" class="graf graf--p graf-after--p">While this naive quantization approach isn’t how it’s done in practice (remember the trick we saw with 4-bit NormalFloat), it does illustrate that the <strong class="markup--strong markup--p-strong">quantization comes with some computational overhead</strong> to store the resulting constants in memory.</p><p name="f2a3" id="f2a3" class="graf graf--p graf-after--p">We could minimize this overhead by doing this process just once. In other words, compute one quantization constant for all the model parameters. However, this is not ideal since <strong class="markup--strong markup--p-strong">it is (very) sensitive to extreme values</strong>. In other words, one relatively large parameter value will skew all the others because of the <em class="markup--em markup--p-em">absmax()</em> function in <em class="markup--em markup--p-em">c^FP32</em>.</p><p name="aa94" id="aa94" class="graf graf--p graf-after--p">Alternatively, we could partition the model parameters into smaller blocks for quantization. This reduces the chances that a large value will skew other values but comes with a larger memory footprint.</p><p name="2b46" id="2b46" class="graf graf--p graf-after--p">To mitigate this memory cost, we can (again) <strong class="markup--strong markup--p-strong">employ quantization</strong>, but now <strong class="markup--strong markup--p-strong">on the constants generated from this block-wise approach</strong>. For a block size of 64, an FP32 quantization constant adds 0.5 bits/parameter. By quantizing these constants further, to say 8-bit, we can reduce this footprint to 0.127 bits/parameter [4].</p><figure name="fe5e" id="fe5e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zZL5x0AibbSC23sg_fci8g.png" data-width="1056" data-height="619" src="https://cdn-images-1.medium.com/max/800/1*zZL5x0AibbSC23sg_fci8g.png"><figcaption class="imageCaption">Visual comparison of standard vs block-wise quantization. Image by author.</figcaption></figure><h4 name="5e0f" id="5e0f" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Ingredient 3: Paged optimizers</strong></h4><p name="abc8" id="abc8" class="graf graf--p graf-after--h4">This ingredient uses Nvidia’s unified memory feature to help avoid out-of-memory errors during training. It transfers “pages” of memory from the GPU to the CPU when the GPU hits its limits. This is <strong class="markup--strong markup--p-strong">similar to how memory is handled between CPU RAM and machine storage</strong> [4].</p><p name="ac31" id="ac31" class="graf graf--p graf-after--p">More specifically, this memory paging feature moves pages of optimizer states to the CPU and back to the GPU as needed. This is important because there can be intermittent memory spikes during training, which can kill the process.</p><h4 name="b640" id="b640" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Ingredient 4: LoRA</strong></h4><p name="d75c" id="d75c" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">LoRA (Low-rank Adaptation)</strong> is a Parameter Efficient Fine-tuning (PEFT) method. The key idea is instead of retraining all the model parameters, LoRA <strong class="markup--strong markup--p-strong">adds a relatively small number of trainable parameters while keeping the original parameters fixed</strong> [5].</p><p name="3573" id="3573" class="graf graf--p graf-after--p">Since I covered the details of LoRA in a <a href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91#8e86" data-href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91#8e86" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">previous article</a> of this series, I will just say we can use it to reduce the number of trainable parameters by 100–1000X without sacrificing model performance.</p><div name="9584" id="9584" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91"><strong class="markup--strong markup--mixtapeEmbed-strong">Fine-Tuning Large Language Models (LLMs)</strong><br><em class="markup--em markup--mixtapeEmbed-em">A conceptual overview with example Python code</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="e657c47fa395561999203e29522991f0" data-thumbnail-img-id="1*YHNrnuaHtS2meh39gGmCCw.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*YHNrnuaHtS2meh39gGmCCw.png);"></a></div><h4 name="4c3c" id="4c3c" class="graf graf--h4 graf-after--mixtapeEmbed"><strong class="markup--strong markup--h4-strong">Bringing it all together</strong></h4><p name="1263" id="1263" class="graf graf--p graf-after--h4">Now that we know all the ingredients of QLoRA, let’s see how we can bring them together.</p><p name="b058" id="b058" class="graf graf--p graf-after--p">To start, consider a <strong class="markup--strong markup--p-strong">standard fine-tuning process</strong>, which consists of retraining every model parameter. What this might look like is using FP16 for the model parameters and gradients (4 total bytes/parameters) and FP32 for the optimizer states, e.g. momentum and variance, and parameters (12 bytes/parameter) [1]. So, a <strong class="markup--strong markup--p-strong">10B parameter model would require about 160GB of memory to fine-tune</strong>.</p><p name="7b96" id="7b96" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Using LoRA</strong>, we can immediately reduce this computational cost by decreasing the number of trainable parameters. This works by freezing the original parameters and adding a set of (small) adapters housing the trainable parameters [5]. The computational cost for the model parameters and gradients would be the same as before (4 total bytes/parameters) [1].</p><p name="bbfc" id="bbfc" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">savings</strong>, however, <strong class="markup--strong markup--p-strong">comes from the optimizer states</strong>. If we have 100X fewer trainable parameters and use FP16 for the adapter, we’d have an additional 0.04 bytes per parameter in the original model (as opposed to 4 bytes/parameter). Similarly, using FP32 for the optimizer states, we have an additional 0.12 bytes/parameter [4]. Therefore, <strong class="markup--strong markup--p-strong">a 10B parameter model would require about 41.6GB of memory to fine-tune</strong>. A significant savings, but still a lot to ask for from consumer hardware.</p><p name="7adf" id="7adf" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">QLoRA</strong> takes things further by quantizing the original model parameters using Ingredients 1 and 2. This reduces the cost from 4 bytes/parameter to about 1 byte/parameter. Then, by using LoRA in the same way as before, that would add another 0.16 bytes/parameter. Thus, <strong class="markup--strong markup--p-strong">a 10B model can be fine-tuned with just 11.6GB of memory!</strong> This can easily run on consumer hardware like the free T4 GPU on Google Colab.</p><p name="aa28" id="aa28" class="graf graf--p graf-after--p">A visual comparison of the 3 approaches is shown below [4].</p><figure name="551e" id="551e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9_4c9SHEF0tL9n61ugAIzA.png" data-width="1174" data-height="476" src="https://cdn-images-1.medium.com/max/800/1*9_4c9SHEF0tL9n61ugAIzA.png"><figcaption class="imageCaption">Visual comparison of 3 fine-tuning techniques. Based on the figure in [4]. Re-illustrated by author.</figcaption></figure><h3 name="f072" id="f072" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments</strong></h3><p name="5478" id="5478" class="graf graf--p graf-after--h3">Now that we have a basic understanding of how QLoRA works let’s see what using it looks like in code. Here, we will use a 4-bit version of the Mistral-7B-Instruct model provided by <a href="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GPTQ" data-href="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GPTQ" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TheBloke</a> and the Hugging Face ecosystem for fine-tuning.</p><p name="7cc5" id="7cc5" class="graf graf--p graf-after--p">This example code is available in a <a href="https://colab.research.google.com/drive/1AErkPgDderPW0dgE230OOjEysd0QV1sR?usp=sharing" data-href="https://colab.research.google.com/drive/1AErkPgDderPW0dgE230OOjEysd0QV1sR?usp=sharing" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Google Colab notebook</a>, which can run on the (free) GPU provided by Colab. The <a href="https://huggingface.co/datasets/shawhin/shawgpt-youtube-comments" data-href="https://huggingface.co/datasets/shawhin/shawgpt-youtube-comments" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">dataset</a> is also available on Hugging Face.</p><p name="af40" id="af40" class="graf graf--p graf-after--p graf--trailing">🔗 <a href="https://colab.research.google.com/drive/1AErkPgDderPW0dgE230OOjEysd0QV1sR?usp=sharing" data-href="https://colab.research.google.com/drive/1AErkPgDderPW0dgE230OOjEysd0QV1sR?usp=sharing" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Google Colab</a> | <a href="https://huggingface.co/datasets/shawhin/shawgpt-youtube-comments" data-href="https://huggingface.co/datasets/shawhin/shawgpt-youtube-comments" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Training Dataset</a> | <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/qlora" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/qlora" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub Repo</a></p></div></div></section><section name="04df" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h4 name="d381" id="d381" class="graf graf--h4 graf--leading"><strong class="markup--strong markup--h4-strong">Imports</strong></h4><p name="7a0a" id="7a0a" class="graf graf--p graf-after--h4">We import modules from Hugging Face’s <em class="markup--em markup--p-em">transforms</em>, <em class="markup--em markup--p-em">peft</em>, and <em class="markup--em markup--p-em">datasets</em> libraries.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="3f63" id="3f63" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, pipeline<br /><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> prepare_model_for_kbit_training<br /><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig, get_peft_model<br /><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br /><span class="hljs-keyword">import</span> transformers</span></pre><p name="5c7a" id="5c7a" class="graf graf--p graf-after--pre">Additionally, we need the following dependencies installed for some of the previous modules to work.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="2d4c" id="2d4c" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">!pip install auto-gptq<br />!pip install optimum<br />!pip install bitsandbytes</span></pre><h4 name="6aa4" id="6aa4" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Load Base Model &amp; Tokenizer</strong></h4><p name="0fd7" id="0fd7" class="graf graf--p graf-after--h4">Next, we load the quantized model from Hugging Face. Here, we use a version of <a href="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GPTQ" data-href="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GPTQ" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Mistral-7B-Instruct-v0.2 prepared by TheBloke</a>, who has freely quantized and shared thousands of <a href="https://huggingface.co/TheBloke" data-href="https://huggingface.co/TheBloke" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LLMs</a>.</p><p name="1978" id="1978" class="graf graf--p graf-after--p">Notice we are using the “Instruct” version of Mistral-7b. This indicates that the model has undergone<strong class="markup--strong markup--p-strong"> instruction tuning</strong>, <strong class="markup--strong markup--p-strong">a fine-tuning process</strong> that <strong class="markup--strong markup--p-strong">aims to</strong> <strong class="markup--strong markup--p-strong">improve model performance in answering questions and responding to user prompts</strong>.</p><p name="a407" id="a407" class="graf graf--p graf-after--p">Other than specifying the model repo we want to download, we also set the following arguments: <em class="markup--em markup--p-em">device_map</em>, <em class="markup--em markup--p-em">trust_remote_code</em>, and <em class="markup--em markup--p-em">revision</em>. <em class="markup--em markup--p-em">device_map</em> lets the method automatically figure out how to best allocate computational resources for loading the model on the machine. Next, <em class="markup--em markup--p-em">trust_remote_code=False</em> prevents custom model files from running on your machine. Then, finally, <em class="markup--em markup--p-em">revision</em> specifies which version of the model we want to use from the repo.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="5b3b" id="5b3b" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">model_name = <span class="hljs-string">&quot;TheBloke/Mistral-7B-Instruct-v0.2-GPTQ&quot;</span><br />model = AutoModelForCausalLM.from_pretrained(<br />    model_name,<br />    device_map=<span class="hljs-string">&quot;auto&quot;</span>, <br />    trust_remote_code=<span class="hljs-literal">False</span>,<br />    revision=<span class="hljs-string">&quot;main&quot;</span>) </span></pre><p name="539a" id="539a" class="graf graf--p graf-after--pre">Once loaded, we see the 7B parameter model only takes us <strong class="markup--strong markup--p-strong">4.16GB of memory</strong>, which can easily fit in either the CPU or GPU memory available for free on Colab.</p><p name="6f98" id="6f98" class="graf graf--p graf-after--p">Next, we load the tokenizer for the model. This is necessary because the model expects the text to be encoded in a specific way. I discussed <a href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91#4457" data-href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91#4457" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">tokenization</a> in <a href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971#c72b" data-href="https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971#c72b" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">previous articles</a> of this series.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="af27" id="af27" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=<span class="hljs-literal">True</span>)</span></pre><h4 name="db2f" id="db2f" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Using the Base Model</strong></h4><p name="e7b3" id="e7b3" class="graf graf--p graf-after--h4">Next, we can use the model for text generation. As a first pass, let’s try to input a test comment to the model. We can do this in 3 steps.</p><p name="f0ad" id="f0ad" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">First</strong>, we craft the prompt in the proper format. Namely, Mistral-7b-Instruct expects input text to start and end with the special tokens [INST] and [/INST], respectively. <strong class="markup--strong markup--p-strong">Second</strong>, we tokenize the prompt. <strong class="markup--strong markup--p-strong">Third</strong>, we pass the prompt into the model to generate text.</p><p name="eb92" id="eb92" class="graf graf--p graf-after--p">The code to do this is shown below with the test comment, “<em class="markup--em markup--p-em">Great content, thank you!</em>”</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="9de0" id="9de0" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">model.<span class="hljs-built_in">eval</span>() <span class="hljs-comment"># model in evaluation mode (dropout modules are deactivated)</span><br /><br /><span class="hljs-comment"># craft prompt</span><br />comment = <span class="hljs-string">&quot;Great content, thank you!&quot;</span><br />prompt=<span class="hljs-string">f&#x27;&#x27;&#x27;[INST] <span class="hljs-subst">{comment}</span> [/INST]&#x27;&#x27;&#x27;</span><br /><br /><span class="hljs-comment"># tokenize input</span><br />inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br /><br /><span class="hljs-comment"># generate output</span><br />outputs = model.generate(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].to(<span class="hljs-string">&quot;cuda&quot;</span>), <br />                            max_new_tokens=<span class="hljs-number">140</span>)<br /><br /><span class="hljs-built_in">print</span>(tokenizer.batch_decode(outputs)[<span class="hljs-number">0</span>])</span></pre><p name="4719" id="4719" class="graf graf--p graf-after--pre">The response from the model is shown below. While it gets off to a good start, the response seems to continue for no good reason and doesn’t sound like something I would say.</p><pre data-code-block-mode="0" spellcheck="false" name="eb34" id="eb34" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">I&#39;m glad you found the content helpful! If you have any specific questions or <br>topics you&#39;d like me to cover in the future, feel free to ask. I&#39;m here to <br>help.<br><br>In the meantime, I&#39;d be happy to answer any questions you have about the <br>content I&#39;ve already provided. Just let me know which article or blog post <br>you&#39;re referring to, and I&#39;ll do my best to provide you with accurate and <br>up-to-date information.<br><br>Thanks for reading, and I look forward to helping you with any questions you <br>may have!</span></pre><h4 name="5aad" id="5aad" class="graf graf--h4 graf-after--pre">Prompt Engineering</h4><p name="4931" id="4931" class="graf graf--p graf-after--h4">This is where <strong class="markup--strong markup--p-strong">prompt engineering</strong> is helpful. Since a <a href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" data-href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" class="markup--anchor markup--p-anchor" target="_blank">previous article</a> in this series covered this topic in-depth, I’ll just say that prompt engineering involves <strong class="markup--strong markup--p-strong">crafting instructions that lead to better model responses</strong>.</p><p name="4785" id="4785" class="graf graf--p graf-after--p">Typically, writing good instructions is something done <strong class="markup--strong markup--p-strong">through trial and error</strong>. To do this, I tried several prompt iterations using <a href="https://www.together.ai/" data-href="https://www.together.ai/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">together.ai</a>, which has a free UI for many open-source LLMs, such as Mistral-7B-Instruct-v0.2.</p><p name="02eb" id="02eb" class="graf graf--p graf-after--p">Once I got instructions I was happy with, I created a prompt template that automatically combines these instructions with a comment using a lambda function. The code for this is shown below.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="80c1" id="80c1" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">intstructions_string = <span class="hljs-string">f&quot;&quot;&quot;ShawGPT, functioning as a virtual data science \<br />consultant on YouTube, communicates in clear, accessible language, escalating \<br />to technical depth upon request. \<br />It reacts to feedback aptly and ends responses with its signature &#x27;–ShawGPT&#x27;. \<br />ShawGPT will tailor the length of its responses to match the viewer&#x27;s comment, <br />providing concise acknowledgments to brief expressions of gratitude or \<br />feedback, thus keeping the interaction natural and engaging.<br /><br />Please respond to the following comment.<br />&quot;&quot;&quot;</span><br /><br />prompt_template = <br />    <span class="hljs-keyword">lambda</span> comment: <span class="hljs-string">f&#x27;&#x27;&#x27;[INST] <span class="hljs-subst">{intstructions_string}</span> \n<span class="hljs-subst">{comment}</span> \n[/INST]&#x27;&#x27;&#x27;</span><br /><br />prompt = prompt_template(comment)</span></pre><pre data-code-block-mode="0" spellcheck="false" name="7afb" id="7afb" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content">The Prompt<br>-----------<br><br>[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, <br>communicates in clear, accessible language, escalating to technical depth upon <br>request. It reacts to feedback aptly and ends responses with its signature <br>&#39;–ShawGPT&#39;. ShawGPT will tailor the length of its responses to match the <br>viewer&#39;s comment, providing concise acknowledgments to brief expressions of <br>gratitude or feedback, thus keeping the interaction natural and engaging.<br><br>Please respond to the following comment.<br> <br>Great content, thank you! <br>[/INST]</span></pre><p name="0a4a" id="0a4a" class="graf graf--p graf-after--pre">We can see the power of a good prompt by comparing the new model response (below) to the previous one. Here, the model responds concisely and appropriately and identifies itself as <em class="markup--em markup--p-em">ShawGPT</em>.</p><pre data-code-block-mode="0" spellcheck="false" name="ea2f" id="ea2f" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Thank you for your kind words! I&#39;m glad you found the content helpful. –ShawGPT</span></pre><h4 name="f086" id="f086" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Prepare Model for Training</strong></h4><p name="d6d5" id="d6d5" class="graf graf--p graf-after--h4">Let’s see how we can improve the model’s performance through fine-tuning. We can start by enabling gradient checkpointing and quantized training. <strong class="markup--strong markup--p-strong">Gradient checkpointing </strong>is a memory-saving technique that clears specific activations and recomputes them during the backward pass [6]<strong class="markup--strong markup--p-strong">.</strong> <strong class="markup--strong markup--p-strong">Quantized training</strong> is enabled using the method imported from <em class="markup--em markup--p-em">peft</em>.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="1181" id="1181" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">model.train() <span class="hljs-comment"># model in training mode (dropout modules are activated)</span><br /><br /><span class="hljs-comment"># enable gradient check pointing</span><br />model.gradient_checkpointing_enable()<br /><br /><span class="hljs-comment"># enable quantized training</span><br />model = prepare_model_for_kbit_training(model)</span></pre><p name="9f2b" id="9f2b" class="graf graf--p graf-after--pre">Next, we can set up training with LoRA via a configuration object. Here, we target the <strong class="markup--strong markup--p-strong">query layers</strong> in the model and use an <strong class="markup--strong markup--p-strong">intrinsic rank of 8</strong>. Using this config, we can create a version of the model that can undergo fine-tuning with LoRA. Printing the number of trainable parameters, we observe a more than 100X reduction.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="4e1e" id="4e1e" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># LoRA config</span><br />config = LoraConfig(<br />    r=<span class="hljs-number">8</span>,<br />    lora_alpha=<span class="hljs-number">32</span>,<br />    target_modules=[<span class="hljs-string">&quot;q_proj&quot;</span>],<br />    lora_dropout=<span class="hljs-number">0.05</span>,<br />    bias=<span class="hljs-string">&quot;none&quot;</span>,<br />    task_type=<span class="hljs-string">&quot;CAUSAL_LM&quot;</span><br />)<br /><br /><span class="hljs-comment"># LoRA trainable version of model</span><br />model = get_peft_model(model, config)<br /><br /><span class="hljs-comment"># trainable parameter count</span><br />model.print_trainable_parameters()<br /><br /><span class="hljs-comment">### trainable params: 2,097,152 || all params: 264,507,392 || trainable%: 0.7928519441906561</span><br /><span class="hljs-comment"># Note: I&#x27;m not sure why its showing 264M parameters here.</span></span></pre><h4 name="36dd" id="36dd" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Prepare Training Dataset</strong></h4><p name="438f" id="438f" class="graf graf--p graf-after--h4">Now, we can import our training data. The dataset used here is available on the HuggingFace Dataset Hub. I generated this dataset using comments and responses from my <a href="https://www.youtube.com/@ShawhinTalebi" data-href="https://www.youtube.com/@ShawhinTalebi" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">YouTube channel</a>. The code to prepare and upload the dataset to the Hub is available at the <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/qlora" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/qlora" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub repo</a>.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="6ac1" id="6ac1" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># load dataset</span><br />data = load_dataset(<span class="hljs-string">&quot;shawhin/shawgpt-youtube-comments&quot;</span>)</span></pre><p name="5532" id="5532" class="graf graf--p graf-after--pre">Next, we must prepare the dataset for training. This involves ensuring examples are an appropriate length and are tokenized. The code for this is shown below.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="5609" id="5609" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># create tokenize function</span><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">examples</span>):<br />    <span class="hljs-comment"># extract text</span><br />    text = examples[<span class="hljs-string">&quot;example&quot;</span>]<br /><br />    <span class="hljs-comment">#tokenize and truncate text</span><br />    tokenizer.truncation_side = <span class="hljs-string">&quot;left&quot;</span><br />    tokenized_inputs = tokenizer(<br />        text,<br />        return_tensors=<span class="hljs-string">&quot;np&quot;</span>,<br />        truncation=<span class="hljs-literal">True</span>,<br />        max_length=<span class="hljs-number">512</span><br />    )<br /><br />    <span class="hljs-keyword">return</span> tokenized_inputs<br /><br /><span class="hljs-comment"># tokenize training and validation datasets</span><br />tokenized_data = data.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)</span></pre><p name="17d3" id="17d3" class="graf graf--p graf-after--pre">Two other things we need for training are a <strong class="markup--strong markup--p-strong">pad token</strong> and a <strong class="markup--strong markup--p-strong">data collator</strong>. Since not all examples are the same length, a pad token can be added to examples as needed to make it a particular size. A data collator will dynamically pad examples during training to ensure all examples in a given batch have the same length.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="b0bf" id="b0bf" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># setting pad token</span><br />tokenizer.pad_token = tokenizer.eos_token<br /><br /><span class="hljs-comment"># data collator</span><br />data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, <br />                                                              mlm=<span class="hljs-literal">False</span>)</span></pre><h4 name="2624" id="2624" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Fine-tuning the Model</strong></h4><p name="646a" id="646a" class="graf graf--p graf-after--h4">In the code block below, I define hyperparameters for model training.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="3691" id="3691" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># hyperparameters</span><br />lr = <span class="hljs-number">2e-4</span><br />batch_size = <span class="hljs-number">4</span><br />num_epochs = <span class="hljs-number">10</span><br /><br /><span class="hljs-comment"># define training arguments</span><br />training_args = transformers.TrainingArguments(<br />    output_dir= <span class="hljs-string">&quot;shawgpt-ft&quot;</span>,<br />    learning_rate=lr,<br />    per_device_train_batch_size=batch_size,<br />    per_device_eval_batch_size=batch_size,<br />    num_train_epochs=num_epochs,<br />    weight_decay=<span class="hljs-number">0.01</span>,<br />    logging_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,<br />    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,<br />    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,<br />    load_best_model_at_end=<span class="hljs-literal">True</span>,<br />    gradient_accumulation_steps=<span class="hljs-number">4</span>,<br />    warmup_steps=<span class="hljs-number">2</span>,<br />    fp16=<span class="hljs-literal">True</span>,<br />    optim=<span class="hljs-string">&quot;paged_adamw_8bit&quot;</span>,<br />)</span></pre><p name="f37c" id="f37c" class="graf graf--p graf-after--pre">While several are listed here, the two I want to highlight in the context of QLoRA are <em class="markup--em markup--p-em">fp16</em> and <em class="markup--em markup--p-em">optim</em>. <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">fp16=True</em></strong> has the trainer use FP16 values for the training process, which results in significant memory savings compared to the standard FP32. <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">optim=”paged_adamw_8bit”</em></strong> enables Ingredient 3 (i.e. paged optimizers) discussed previously.</p><p name="c020" id="c020" class="graf graf--p graf-after--p">With all the hyperparameters set, we can run the training process using the code below.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="c910" id="c910" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># configure trainer</span><br />trainer = transformers.Trainer(<br />    model=model,<br />    train_dataset=tokenized_data[<span class="hljs-string">&quot;train&quot;</span>],<br />    eval_dataset=tokenized_data[<span class="hljs-string">&quot;test&quot;</span>],<br />    args=training_args,<br />    data_collator=data_collator<br />)<br /><br /><span class="hljs-comment"># train model</span><br />model.config.use_cache = <span class="hljs-literal">False</span>  <span class="hljs-comment"># silence the warnings.</span><br />trainer.train()<br /><br /><span class="hljs-comment"># renable warnings</span><br />model.config.use_cache = <span class="hljs-literal">True</span></span></pre><p name="6600" id="6600" class="graf graf--p graf-after--pre">Since we only have 50 training examples, the process runs in about 10 minutes. The training and validation loss are shown in the table below. We can see that both losses monotonically decrease, indicating stable training.</p><figure name="cbb6" id="cbb6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*M6pvjdz3fc1e-rNUOLyPyQ.png" data-width="326" data-height="277" src="https://cdn-images-1.medium.com/max/800/1*M6pvjdz3fc1e-rNUOLyPyQ.png"><figcaption class="imageCaption">Training and Validation loss table. Image by author.</figcaption></figure><h4 name="c26e" id="c26e" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Loading Fine-tuned model</strong></h4><p name="f63b" id="f63b" class="graf graf--p graf-after--h4">The final model is freely available on the <a href="https://huggingface.co/shawhin/shawgpt-ft" data-href="https://huggingface.co/shawhin/shawgpt-ft" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">HF hub</a>. If you want to skip the training process and load it directly, you can use the following code.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="2bb1" id="2bb1" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># load model from hub</span><br /><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> PeftModel, PeftConfig<br /><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM<br /><br />model_name = <span class="hljs-string">&quot;TheBloke/Mistral-7B-Instruct-v0.2-GPTQ&quot;</span><br />model = AutoModelForCausalLM.from_pretrained(model_name,<br />                                             device_map=<span class="hljs-string">&quot;auto&quot;</span>,<br />                                             trust_remote_code=<span class="hljs-literal">False</span>,<br />                                             revision=<span class="hljs-string">&quot;main&quot;</span>)<br /><br />config = PeftConfig.from_pretrained(<span class="hljs-string">&quot;shawhin/shawgpt-ft&quot;</span>)<br />model = PeftModel.from_pretrained(model, <span class="hljs-string">&quot;shawhin/shawgpt-ft&quot;</span>)<br /><br /><span class="hljs-comment"># load tokenizer</span><br />tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=<span class="hljs-literal">True</span>)</span></pre><h4 name="7ed9" id="7ed9" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Using the Fine-tuned Model</strong></h4><p name="3df3" id="3df3" class="graf graf--p graf-after--h4">We can use the fine-tuned model for inference in the same way as before. Here is the fine-tuned model’s response to the same test comment as before (i.e. “<em class="markup--em markup--p-em">Great content, thank you!</em>”).</p><pre data-code-block-mode="0" spellcheck="false" name="09a7" id="09a7" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Glad you enjoyed it! –ShawGPT<br><br>(Note: I&#39;m an AI language model, I don&#39;t have the ability to feel emotions or <br>watch videos. I&#39;m here to help answer questions and provide explanations.)</span></pre><p name="de4a" id="de4a" class="graf graf--p graf-after--pre">The response is much more concise and even adds a disclaimer that it is an AI. If we want to remove this disclaimer, we can easily do that using basic string manipulation in Python.</p><p name="8de1" id="8de1" class="graf graf--p graf-after--p">As another example, let’s try a more technical test comment: “<em class="markup--em markup--p-em">What is fat-tailedness?</em>” The model’s response is given below.</p><pre data-code-block-mode="0" spellcheck="false" name="6bbc" id="6bbc" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Fat-tailedness is a statistical property of a distribution. In simple terms, <br>it refers to the presence of extreme outliers or heavy tails in the <br>distribution.<br><br>For instance, consider the distribution of heights in a population. A normal <br>distribution would have most people clustered around an average height with a <br>few people deviating slightly from the mean. However, in a fat-tailed <br>distribution, you would observe a larger number of people being significantly <br>taller or shorter than the average.<br><br>In the context of finance, fat-tailedness is often used to describe the <br>presence of extreme market moves or volatility. For example, the 2008 <br>financial crisis is an example of a fat-tailed event.<br><br>I hope this explanation helps! Let me know if you have any questions.<br>–ShawGPT</span></pre><p name="19be" id="19be" class="graf graf--p graf-after--pre">The response is similar to what we saw in the <a href="https://medium.com/towards-data-science/how-to-build-an-ai-assistant-with-openai-python-8b3b5a636f69" data-href="https://medium.com/towards-data-science/how-to-build-an-ai-assistant-with-openai-python-8b3b5a636f69" class="markup--anchor markup--p-anchor" target="_blank">previous article</a> of this series with the fine-tuned OpenAI model. It gives a concise and appropriate explanation of fat-tailedness, but this isn’t <a href="https://medium.com/towards-data-science/pareto-power-laws-and-fat-tails-0355a187ee6a" data-href="https://medium.com/towards-data-science/pareto-power-laws-and-fat-tails-0355a187ee6a" class="markup--anchor markup--p-anchor" target="_blank">how I explain fat-tailedness</a>.</p><p name="6123" id="6123" class="graf graf--p graf-after--p">While we could attempt to capture this specialized knowledge via further fine-tuning, a simpler approach would be to <strong class="markup--strong markup--p-strong">augment the fine-tuned model using external knowledge</strong> from my <a href="https://medium.com/towards-data-science/pareto-power-laws-and-fat-tails-0355a187ee6a" data-href="https://medium.com/towards-data-science/pareto-power-laws-and-fat-tails-0355a187ee6a" class="markup--anchor markup--p-anchor" target="_blank">article series</a> on fat tails (and other data science topics).</p><p name="c83c" id="c83c" class="graf graf--p graf-after--p">This brings up the idea of <strong class="markup--strong markup--p-strong">Retrieval Augmented Generation</strong> (i.e. <strong class="markup--strong markup--p-strong">RAG</strong>), which will be discussed in the next article of this series.</p><div name="00eb" id="00eb" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/qlora" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/qlora" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/qlora"><strong class="markup--strong markup--mixtapeEmbed-strong">YouTube-Blog/LLMs/qlora at main · ShawhinT/YouTube-Blog</strong><br><em class="markup--em markup--mixtapeEmbed-em">Codes to complement YouTube videos and blog posts on Medium. - YouTube-Blog/LLMs/qlora at main · ShawhinT/YouTube-Blog</em>github.com</a><a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/qlora" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="7018fa4a05069650974e58bfebe316ba" data-thumbnail-img-id="0*CTceIJs8M__921uG" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*CTceIJs8M__921uG);"></a></div><h3 name="6b4e" id="6b4e" class="graf graf--h3 graf-after--mixtapeEmbed"><strong class="markup--strong markup--h3-strong">What’s Next?</strong></h3><p name="77c9" id="77c9" class="graf graf--p graf-after--h3">QLoRA is a fine-tuning technique that has made building custom large language models more accessible. Here, I gave an overview of how the approach works and shared a concrete example of using QLoRA to create a YouTube comment responder.</p><p name="28be" id="28be" class="graf graf--p graf-after--p">While the fine-tuned model did a qualitatively good job mimicking my response style, it had some limitations in understanding specialized data science knowledge. In the <a href="https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac" data-href="https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac" class="markup--anchor markup--p-anchor" target="_blank">next article of this series</a>, we will see how we can overcome this limitation by improving the model with RAG.</p><p name="5a17" id="5a17" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">More on LLMs 👇</strong></p><div name="2ae7" id="2ae7" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/list/8e009ae3054c" data-href="https://shawhin.medium.com/list/8e009ae3054c" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/list/8e009ae3054c"><strong class="markup--strong markup--mixtapeEmbed-strong">Large Language Models (LLMs)</strong><br><em class="markup--em markup--mixtapeEmbed-em">A series on how to use LLMs in practice.</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/list/8e009ae3054c" class="js-mixtapeImage mixtapeImage mixtapeImage--mediumCatalog  u-ignoreBlock" data-media-id="e38ca2d0de6aa61d7c09af603d8f43e1" data-thumbnail-img-id="0*ae40ceaca4c0bdcbdf431a66c777e68bffc1e356.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/304/160/0*ae40ceaca4c0bdcbdf431a66c777e68bffc1e356.jpeg);"></a></div></div></div></section><section name="f5f8" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="f034" id="f034" class="graf graf--h3 graf--leading">Resources</h3><p name="b7d8" id="b7d8" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Connect</strong>: <a href="https://shawhintalebi.com/" data-href="https://shawhintalebi.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">My website</a> | <a href="https://calendly.com/shawhintalebi" data-href="https://calendly.com/shawhintalebi" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Book a call</a></p><p name="41dc" id="41dc" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Socials</strong>: <a href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" data-href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">YouTube 🎥</a> | <a href="https://www.linkedin.com/in/shawhintalebi/" data-href="https://www.linkedin.com/in/shawhintalebi/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">LinkedIn</a> | <a href="https://www.instagram.com/shawhintalebi" data-href="https://www.instagram.com/shawhintalebi" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Instagram</a></p><p name="13d4" id="13d4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Support</strong>: <a href="https://www.buymeacoffee.com/shawhint" data-href="https://www.buymeacoffee.com/shawhint" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Buy me a coffee</a> ☕️</p><div name="fe26" id="fe26" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/subscribe" data-href="https://shawhin.medium.com/subscribe" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/subscribe"><strong class="markup--strong markup--mixtapeEmbed-strong">Get FREE access to every new story I write</strong><br><em class="markup--em markup--mixtapeEmbed-em">Get FREE access to every new story I write P.S. I do not share your email with anyone By signing up, you will create a…</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/subscribe" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8d6d6c2d301f2c367b618a850fa468b3" data-thumbnail-img-id="0*O69FxQ5FttNEA_r4" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*O69FxQ5FttNEA_r4);"></a></div></div></div></section><section name="051b" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="52c7" id="52c7" class="graf graf--p graf--leading">[1] <a href="https://arxiv.org/pdf/1910.02054.pdf" data-href="https://arxiv.org/pdf/1910.02054.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Memory Optimization with ZeRO</a></p><p name="a44d" id="a44d" class="graf graf--p graf-after--p">[2] <a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format" data-href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">FP32</a></p><p name="5b28" id="5b28" class="graf graf--p graf-after--p">[3] <a href="https://www.h-schmidt.net/FloatConverter/IEEE754.html" data-href="https://www.h-schmidt.net/FloatConverter/IEEE754.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Float Converter</a></p><p name="c8af" id="c8af" class="graf graf--p graf-after--p">[4] <a href="https://arxiv.org/abs/2305.14314" data-href="https://arxiv.org/abs/2305.14314" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">QLoRA: Efficient Finetuning of Quantized LLMs</a></p><p name="0dbe" id="0dbe" class="graf graf--p graf-after--p">[5] <a href="https://arxiv.org/abs/2106.09685" data-href="https://arxiv.org/abs/2106.09685" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LoRA paper</a></p><p name="1b64" id="1b64" class="graf graf--p graf-after--p graf--trailing">[6] <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-checkpointing.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-checkpointing.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Gradient Checkpointing</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@shawhin" class="p-author h-card">Shaw Talebi</a> on <a href="https://medium.com/p/4e44d6b5be32"><time class="dt-published" datetime="2024-02-22T06:42:14.091Z">February 22, 2024</time></a>.</p><p><a href="https://medium.com/@shawhin/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 2, 2024.</p></footer></article></body></html>