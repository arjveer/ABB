<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Fine-Tuning BERT for Text Classification</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Fine-Tuning BERT for Text Classification</h1>
</header>
<section data-field="subtitle" class="p-summary">
A hackable example with Python code
</section>
<section data-field="body" class="e-content">
<section name="fb3f" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="4ec0" id="4ec0" class="graf graf--h3 graf--leading graf--title"><strong class="markup--strong markup--h3-strong">Fine-Tuning BERT for Text Classification</strong></h3><h4 name="e273" id="e273" class="graf graf--h4 graf-after--h3 graf--subtitle">A hackable example with Python code</h4><p name="a096" id="a096" class="graf graf--p graf-after--h4">Although today’s 100B+ parameter transformer models are state-of-the-art in AI, there’s still much we can accomplish with smaller (&lt; 1B parameter) models. In this article, I will walk through one such example, fine-tuning BERT (110M parameters) to classify phishing URLs. I’ll start by covering key concepts and then share example Python code.</p><figure name="a66e" id="a66e" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="1*S-5K3D3dt8EH_S2eLE8aSQ.png" data-width="1280" data-height="720" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*S-5K3D3dt8EH_S2eLE8aSQ.png"><figcaption class="imageCaption">Image from Canva.</figcaption></figure></div></div></section><section name="e5c7" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><figure name="d226" id="d226" class="graf graf--figure graf--iframe graf--leading"><iframe src="https://www.youtube.com/embed/4QHg8Ix8WWQ?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><h3 name="0e1b" id="0e1b" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">Fine-tuning</strong></h3><p name="13a9" id="13a9" class="graf graf--p graf-after--h3"><a href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Fine-tuning</strong></a> <strong class="markup--strong markup--p-strong">involves</strong> <strong class="markup--strong markup--p-strong">adapting a pre-trained model to a particular use case through additional training</strong>.</p><p name="750c" id="750c" class="graf graf--p graf-after--p">Pre-trained models are developed via unsupervised learning, which precludes the need for large-scale labeled datasets. Fine-tuned models can then exploit pre-trained model representations to significantly <strong class="markup--strong markup--p-strong">reduce training costs</strong> and <strong class="markup--strong markup--p-strong">improve model performance</strong> compared to training from scratch [1].</p><div name="138e" id="138e" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91" data-href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91"><strong class="markup--strong markup--mixtapeEmbed-strong">Fine-Tuning Large Language Models (LLMs)</strong><br><em class="markup--em markup--mixtapeEmbed-em">A conceptual overview with example Python code</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="c9fbb0f5ce2055b814fc0009112a88cf" data-thumbnail-img-id="1*YHNrnuaHtS2meh39gGmCCw.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*YHNrnuaHtS2meh39gGmCCw.png);"></a></div><p name="adde" id="adde" class="graf graf--p graf-after--mixtapeEmbed">Splitting the training process into multiple phases has led to today’s state-of-the-art transformer models, such as GPT-4o, Claude, and Llama 3.2. It also <strong class="markup--strong markup--p-strong">enables the democratization of AI</strong> since the expensive undertaking of model pre-training can be done by specialized research labs, who can then make these models publicly available for fine-tuning.</p><h3 name="e557" id="e557" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">BERT</strong></h3><p name="3cf2" id="3cf2" class="graf graf--p graf-after--h3">While model fine-tuning gained tremendous popularity post-ChatGPT, it’s been around since (at least) 2015 [2]. One of the early language models developed specifically for fine-tuning was Google’s BERT model, which was pre-trained on two unsupervised tasks: <strong class="markup--strong markup--p-strong">1) masked language modeling (MLM)</strong> and <strong class="markup--strong markup--p-strong">2) next sentence prediction</strong> [1].</p><p name="3364" id="3364" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">MLM</strong> pre-training task consists of <strong class="markup--strong markup--p-strong">predicting arbitrarily masked words in a sequence</strong>. This is in contrast to causal language modeling, which is restricted to predicting the word at the end of a sequence. Therefore, MLM enables models to leverage more context (i.e. text before AND after the masked word) to make predictions [1].</p><p name="9bd3" id="9bd3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Next sentence prediction</strong> is important for downstream tasks that require understanding the relationship between two sentences (e.g., Question Answering and Semantic Similarity). This is implemented using special input tokens to distinguish the sentence prediction task from the MLM [1].</p><p name="5c6a" id="5c6a" class="graf graf--p graf-after--p">These pre-training tasks enable BERT to be fine-tuned on a wide range of tasks such as sentiment analysis, sentence similarity, question answering, named entity recognition, common sense reasoning, and many others [1].</p><h3 name="c231" id="c231" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Text Classification</strong></h3><p name="352c" id="352c" class="graf graf--p graf-after--h3">Many of the tasks mentioned above (e.g. sentiment analysis, sentence similarity, named entity recognition) fall under the category of <strong class="markup--strong markup--p-strong">text classification</strong>, i.e., <strong class="markup--strong markup--p-strong">assigning a label to input text sequences</strong>.</p><p name="61f1" id="61f1" class="graf graf--p graf-after--p">There are countless practical applications of text classification, such as detecting spam in emails, categorizing IT support tickets, detecting toxic or harmful speech, and analyzing the sentiment of customer reviews. While each of these tasks is practically very different, their implementations are almost identical from a technical standpoint.</p><h3 name="66fb" id="66fb" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Example Code: Fine-tuning BERT for Phishing URL Identification</strong></h3><p name="f2d2" id="f2d2" class="graf graf--p graf-after--h3">Here, we will walk through an example of BERT fine-tuning to classify phishing URLs. We will use the <a href="https://huggingface.co/google-bert/bert-base-uncased" data-href="https://huggingface.co/google-bert/bert-base-uncased" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">bert-base-uncased</a> model freely available on the Hugging Face (HF) hub.</p><p name="cdc0" id="cdc0" class="graf graf--p graf-after--p">The model consists of 110M parameters, of which we will only train a small percentage. Therefore, this <strong class="markup--strong markup--p-strong">example should easily run on most consumer hardware (no GPU required)</strong>.</p><p name="4941" id="4941" class="graf graf--p graf-after--p graf--trailing">The fine-tuned model is also available on the <a href="https://huggingface.co/shawhin/bert-phishing-classifier_teacher" data-href="https://huggingface.co/shawhin/bert-phishing-classifier_teacher" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">HF hub</a>, and an example notebook is available on <a href="https://github.com/ShawhinT/YouTube-Blog/blob/main/LLMs/model-compression/0_train-teacher.ipynb" data-href="https://github.com/ShawhinT/YouTube-Blog/blob/main/LLMs/model-compression/0_train-teacher.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p></div></div></section><section name="0df5" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="4070" id="4070" class="graf graf--p graf--leading">We’ll start by importing a few handy libraries.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="b32e" id="b32e" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> DatasetDict, Dataset<br /><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification, <br />                         TrainingArguments, Trainer<br /><span class="hljs-keyword">import</span> evaluate<br /><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding</span></pre><p name="1ba2" id="1ba2" class="graf graf--p graf-after--pre">Next, we’ll load the training dataset. It consists of 3,000 text-label pairs with a 70–15–15 train-test-validation split. The data are originally from <a href="https://www.kaggle.com/datasets/taruntiwarihp/phishing-site-urls/data" data-href="https://www.kaggle.com/datasets/taruntiwarihp/phishing-site-urls/data" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a> (open database license).</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="5ed5" id="5ed5" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">dataset_dict = load_dataset(<span class="hljs-string">&quot;shawhin/phishing-site-classification&quot;</span>)</span></pre><p name="e919" id="e919" class="graf graf--p graf-after--pre">The Transformer library makes it super easy to <strong class="markup--strong markup--p-strong">load and adapt pre-trained models</strong>. Here’s what that looks like for the BERT model.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="f402" id="f402" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># define pre-trained model path</span><br />model_path = <span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span><br /><br /><span class="hljs-comment"># load model tokenizer</span><br />tokenizer = AutoTokenizer.from_pretrained(model_path)<br /><br /><span class="hljs-comment"># load model with binary classification head</span><br />id2label = {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;Safe&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;Not Safe&quot;</span>}<br />label2id = {<span class="hljs-string">&quot;Safe&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;Not Safe&quot;</span>: <span class="hljs-number">1</span>}<br />model = AutoModelForSequenceClassification.from_pretrained(model_path, <br />                                                           num_labels=<span class="hljs-number">2</span>, <br />                                                           id2label=id2label, <br />                                                           label2id=label2id,)</span></pre><p name="2ddb" id="2ddb" class="graf graf--p graf-after--pre">When we load a model like this, all the parameters will be set as trainable by default. However, training all 110M parameters will be computationally costly and potentially unnecessary.</p><p name="5329" id="5329" class="graf graf--p graf-after--p">Instead, we can freeze most of the model parameters and <strong class="markup--strong markup--p-strong">only train the model’s final layer and classification head</strong>.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="graphql" name="61ab" id="61ab" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># freeze all base model parameters</span><br />for name, param in model.base_model.named_parameters<span class="hljs-punctuation">(</span><span class="hljs-punctuation">)</span><span class="hljs-punctuation">:</span><br />    param.requires_grad <span class="hljs-punctuation">=</span> <span class="hljs-literal">False</span><br /><br /><span class="hljs-comment"># unfreeze base model pooling layers</span><br />for name, param in model.base_model.named_parameters<span class="hljs-punctuation">(</span><span class="hljs-punctuation">)</span><span class="hljs-punctuation">:</span><br />    if <span class="hljs-string">&quot;pooler&quot;</span> in <span class="hljs-symbol">name</span><span class="hljs-punctuation">:</span><br />        param.requires_grad <span class="hljs-punctuation">=</span> <span class="hljs-literal">True</span></span></pre><p name="8f7d" id="8f7d" class="graf graf--p graf-after--pre">Next, we will need to <strong class="markup--strong markup--p-strong">preprocess our data</strong>. This will consist of two key operations: tokenizing the URLs (i.e., converting them into integers) and truncating them.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="28e0" id="28e0" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># define text preprocessing</span><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):<br />    <span class="hljs-comment"># return tokenized text with truncation</span><br />    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], truncation=<span class="hljs-literal">True</span>)<br /><br /><span class="hljs-comment"># preprocess all datasets</span><br />tokenized_data = dataset_dict.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)</span></pre><p name="bcc8" id="bcc8" class="graf graf--p graf-after--pre">Another important step is creating a <strong class="markup--strong markup--p-strong">data collator</strong> that will dynamically pad token sequences in a batch during training so they have the same length. We can do this in one line of code.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="64e1" id="64e1" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># create data collator</span><br />data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span></pre><p name="750e" id="750e" class="graf graf--p graf-after--pre">As a final step before training, we can define a function to compute a set of metrics to help us monitor training progress. Here, we will consider model accuracy and AUC.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="4c16" id="4c16" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># load metrics</span><br />accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)<br />auc_score = evaluate.load(<span class="hljs-string">&quot;roc_auc&quot;</span>)<br /><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):<br />    <span class="hljs-comment"># get predictions</span><br />    predictions, labels = eval_pred<br />    <br />    <span class="hljs-comment"># apply softmax to get probabilities</span><br />    probabilities = np.exp(predictions) / np.exp(predictions).<span class="hljs-built_in">sum</span>(-<span class="hljs-number">1</span>, <br />                                                                 keepdims=<span class="hljs-literal">True</span>)<br />    <span class="hljs-comment"># use probabilities of the positive class for ROC AUC</span><br />    positive_class_probs = probabilities[:, <span class="hljs-number">1</span>]<br />    <span class="hljs-comment"># compute auc</span><br />    auc = np.<span class="hljs-built_in">round</span>(auc_score.compute(prediction_scores=positive_class_probs, <br />                                     references=labels)[<span class="hljs-string">&#x27;roc_auc&#x27;</span>],<span class="hljs-number">3</span>)<br />    <br />    <span class="hljs-comment"># predict most probable class</span><br />    predicted_classes = np.argmax(predictions, axis=<span class="hljs-number">1</span>)<br />    <span class="hljs-comment"># compute accuracy</span><br />    acc = np.<span class="hljs-built_in">round</span>(accuracy.compute(predictions=predicted_classes, <br />                                     references=labels)[<span class="hljs-string">&#x27;accuracy&#x27;</span>],<span class="hljs-number">3</span>)<br />    <br />    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;Accuracy&quot;</span>: acc, <span class="hljs-string">&quot;AUC&quot;</span>: auc}</span></pre><p name="b553" id="b553" class="graf graf--p graf-after--pre">Now, we are ready to fine-tune our model. We start by defining hyperparameters and other training arguments.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="986d" id="986d" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># hyperparameters</span><br />lr = <span class="hljs-number">2e-4</span><br />batch_size = <span class="hljs-number">8</span><br />num_epochs = <span class="hljs-number">10</span><br /><br />training_args = TrainingArguments(<br />    output_dir=<span class="hljs-string">&quot;bert-phishing-classifier_teacher&quot;</span>,<br />    learning_rate=lr,<br />    per_device_train_batch_size=batch_size,<br />    per_device_eval_batch_size=batch_size,<br />    num_train_epochs=num_epochs,<br />    logging_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,<br />    eval_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,<br />    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,<br />    load_best_model_at_end=<span class="hljs-literal">True</span>,<br />)</span></pre><p name="177d" id="177d" class="graf graf--p graf-after--pre">Then, we pass our training arguments into a trainer class and train the model.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="makefile" name="a2ee" id="a2ee" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">trainer = Trainer(<br />    model=model,<br />    args=training_args,<br />    train_dataset=tokenized_data[<span class="hljs-string">&quot;train&quot;</span>],<br />    eval_dataset=tokenized_data[<span class="hljs-string">&quot;test&quot;</span>],<br />    tokenizer=tokenizer,<br />    data_collator=data_collator,<br />    compute_metrics=compute_metrics,<br />)<br /><br />trainer.train()</span></pre><p name="2463" id="2463" class="graf graf--p graf-after--pre">The training results are shown below. We can see that the training and validation loss are monotonically decreasing while the accuracy and AUC increase with each epoch.</p><figure name="5533" id="5533" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*T71pn8_rrQHGDvS3NzyNZQ.png" data-width="1578" data-height="1196" src="https://cdn-images-1.medium.com/max/800/1*T71pn8_rrQHGDvS3NzyNZQ.png"><figcaption class="imageCaption">Training results. Image by author.</figcaption></figure><p name="8c92" id="8c92" class="graf graf--p graf-after--figure">As a final test, we can evaluate the performance of the model on the independent validation data, i.e., data not used for training or setting hyperparameters.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="07e3" id="07e3" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># apply model to validation dataset</span><br />predictions = trainer.predict(tokenized_data[<span class="hljs-string">&quot;validation&quot;</span>])<br /><br /><span class="hljs-comment"># Extract the logits and labels from the predictions object</span><br />logits = predictions.predictions<br />labels = predictions.label_ids<br /><br /><span class="hljs-comment"># Use your compute_metrics function</span><br />metrics = compute_metrics((logits, labels))<br /><span class="hljs-built_in">print</span>(metrics)<br /><br /><span class="hljs-comment"># &gt;&gt; {&#x27;Accuracy&#x27;: 0.889, &#x27;AUC&#x27;: 0.946}</span></span></pre><p name="0bb8" id="0bb8" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Bonus</em></strong>: Although a 110M parameter model is tiny compared to modern language models, we can reduce its computational requirements using <a href="https://towardsdatascience.com/compressing-large-language-models-llms-9f406eea5b5e" data-href="https://towardsdatascience.com/compressing-large-language-models-llms-9f406eea5b5e" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">model compression techniques</a>. I cover how to reduce the memory footprint model by 7X in the article below.</p><div name="e419" id="e419" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://towardsdatascience.com/compressing-large-language-models-llms-9f406eea5b5e" data-href="https://towardsdatascience.com/compressing-large-language-models-llms-9f406eea5b5e" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/compressing-large-language-models-llms-9f406eea5b5e"><strong class="markup--strong markup--mixtapeEmbed-strong">Compressing Large Language Models (LLMs)</strong><br><em class="markup--em markup--mixtapeEmbed-em">Make LLMs 10X smaller without sacrificing performance</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/compressing-large-language-models-llms-9f406eea5b5e" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="340ab9f04f4469c550fa6e66d6ebadfc" data-thumbnail-img-id="1*VDHOHFG-PA8VddO-tuTFbA.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*VDHOHFG-PA8VddO-tuTFbA.png);"></a></div><h3 name="ac38" id="ac38" class="graf graf--h3 graf-after--mixtapeEmbed">Conclusion</h3><p name="970a" id="970a" class="graf graf--p graf-after--h3">Fine-tuning pre-trained models is a powerful paradigm for developing better models at a lower cost than training them from scratch. Here, we saw how to do this with BERT using the Hugging Face Transformers library.</p><p name="0743" id="0743" class="graf graf--p graf-after--p">While the example code was for URL classification, it can be readily adapted to other text classification tasks.</p><p name="a9f8" id="a9f8" class="graf graf--p graf-after--p">More on LLMs 👇</p><div name="0335" id="0335" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/list/8e009ae3054c" data-href="https://shawhin.medium.com/list/8e009ae3054c" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/list/8e009ae3054c"><strong class="markup--strong markup--mixtapeEmbed-strong">Large Language Models (LLMs)</strong><br><em class="markup--em markup--mixtapeEmbed-em">A series on how to use LLMs in practice.</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/list/8e009ae3054c" class="js-mixtapeImage mixtapeImage mixtapeImage--mediumCatalog  u-ignoreBlock" data-media-id="e38ca2d0de6aa61d7c09af603d8f43e1" data-thumbnail-img-id="0*957ce7db3c332977b4af55faac3bb93b047051a3.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/304/160/0*957ce7db3c332977b4af55faac3bb93b047051a3.jpeg);"></a></div></div></div></section><section name="5556" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="3260" id="3260" class="graf graf--p graf--leading"><strong class="markup--strong markup--p-strong">My website</strong>: <a href="https://www.shawhintalebi.com/" data-href="https://www.shawhintalebi.com/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://www.shawhintalebi.com/</a></p><p name="ab4d" id="ab4d" class="graf graf--p graf-after--p">[1] <a href="https://arxiv.org/abs/1810.04805" data-href="https://arxiv.org/abs/1810.04805" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p><p name="4a35" id="4a35" class="graf graf--p graf-after--p graf--trailing">[2] <a href="https://arxiv.org/abs/1511.01432" data-href="https://arxiv.org/abs/1511.01432" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Semi-supervised Sequence Learning</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@shawhin" class="p-author h-card">Shaw Talebi</a> on <a href="https://medium.com/p/a01f89b179fc"><time class="dt-published" datetime="2024-10-17T05:31:47.148Z">October 17, 2024</time></a>.</p><p><a href="https://medium.com/@shawhin/fine-tuning-bert-for-text-classification-a01f89b179fc" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 2, 2024.</p></footer></article></body></html>