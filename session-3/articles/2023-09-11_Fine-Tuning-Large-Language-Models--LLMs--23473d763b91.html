<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Fine-Tuning Large Language Models (LLMs)</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Fine-Tuning Large Language Models (LLMs)</h1>
</header>
<section data-field="subtitle" class="p-summary">
A conceptual overview with example Python code
</section>
<section data-field="body" class="e-content">
<section name="47a1" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="018f" id="018f" class="graf graf--h3 graf--leading graf--title"><strong class="markup--strong markup--h3-strong">Fine-Tuning Large Language Models (LLMs)</strong></h3><h4 name="bae8" id="bae8" class="graf graf--h4 graf-after--h3 graf--subtitle">A conceptual overview with example Python code</h4><p name="6b06" id="6b06" class="graf graf--p graf-after--h4">This is the 5th article in a <a href="https://medium.com/towards-data-science/a-practical-introduction-to-llms-65194dda1148" data-href="https://medium.com/towards-data-science/a-practical-introduction-to-llms-65194dda1148" class="markup--anchor markup--p-anchor" target="_blank">series on using Large Language Models</a> (LLMs) in practice. In this post, we will discuss how to fine-tune (FT) a pre-trained LLM. We start by introducing key FT concepts and techniques, then finish with a concrete example of how to fine-tune a model (locally) using Python and Hugging Face’s software ecosystem.</p><figure name="1dbd" id="1dbd" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="1*YHNrnuaHtS2meh39gGmCCw.png" data-width="1500" data-height="750" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*YHNrnuaHtS2meh39gGmCCw.png"><figcaption class="imageCaption">Tuning a language model. Image by author.</figcaption></figure></div></div></section><section name="7c92" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="17e7" id="17e7" class="graf graf--p graf--leading">In the <a href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" data-href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" class="markup--anchor markup--p-anchor" target="_blank">previous article</a> of this series, we saw how we could build practical LLM-powered applications by integrating prompt engineering into our Python code. For the vast majority of LLM use cases, this is the initial approach I recommend because it requires significantly less resources and technical expertise than other methods while still providing much of the upside.</p><p name="7e57" id="7e57" class="graf graf--p graf-after--p">However, there are situations where prompting an existing LLM out-of-the-box doesn’t cut it, and a more sophisticated solution is required. This is where model fine-tuning can help.</p><figure name="6850" id="6850" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/eC6Hd1hFvos?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe><figcaption class="imageCaption">Supplemental Video.</figcaption></figure><h3 name="0b4f" id="0b4f" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">What is Fine-tuning?</strong></h3><p name="4239" id="4239" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Fine-tuning</strong> is taking a pre-trained model and <strong class="markup--strong markup--p-strong">training at least one internal model parameter</strong> (i.e. weights). In the context of LLMs, what this typically accomplishes is transforming a general-purpose base model (e.g. GPT-3) into a specialized model for a particular use case (e.g. ChatGPT) [1].</p><p name="ad3d" id="ad3d" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">key upside</strong> of this approach is that models can achieve better performance while requiring (far) fewer manually labeled examples compared to models that solely rely on supervised training.</p><p name="7618" id="7618" class="graf graf--p graf-after--p">While strictly self-supervised base models can exhibit impressive performance on a wide variety of tasks with the help of prompt engineering [2], they are still word predictors and may generate completions that are not entirely helpful or accurate. For example, let’s compare the completions of davinci (base GPT-3 model) and text-davinci-003 (a fine-tuned model).</p><figure name="688b" id="688b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*EkrxqoTYHuZrM_tBzvFdQg.png" data-width="1500" data-height="750" src="https://cdn-images-1.medium.com/max/800/1*EkrxqoTYHuZrM_tBzvFdQg.png"><figcaption class="imageCaption">Completion comparison of davinci (base GPT-3 model) and text-davinci-003 (a fine-tuned model). Image by author.</figcaption></figure><p name="eeb7" id="eeb7" class="graf graf--p graf-after--figure">Notice the base model is simply trying to complete the text by listing a set of questions like a Google search or homework assignment, while the <strong class="markup--strong markup--p-strong">fine-tuned model gives a more helpful response</strong>. The flavor of fine-tuning used for text-davinci-003 is <strong class="markup--strong markup--p-strong">alignment tuning,</strong> which aims to make the LLM’s responses more helpful, honest, and harmless, but more on that later [3,4].</p><h3 name="b82e" id="b82e" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Why Fine-tune</strong></h3><p name="fc4e" id="fc4e" class="graf graf--p graf-after--h3">Fine-tuning not only improves the performance of a base model, but <strong class="markup--strong markup--p-strong">a smaller (fine-tuned) model can often outperform larger (more expensive) models</strong> on the set of tasks on which it was trained [4]. This was demonstrated by OpenAI with their first generation “InstructGPT” models, where the 1.3B parameter InstructGPT model completions were preferred over the 175B parameter GPT-3 base model despite being 100x smaller [4].</p><p name="e514" id="e514" class="graf graf--p graf-after--p">Although most of the LLMs we may interact with these days are not strictly self-supervised models like GPT-3, there are still drawbacks to prompting an existing fine-tuned model for a specific use case.</p><p name="7d36" id="7d36" class="graf graf--p graf-after--p">A big one is LLMs have a finite context window. Thus, the model may perform sub-optimally on tasks that require a large knowledge base or domain-specific information [1]. Fine-tuned models can avoid this issue by “learning” this information during the fine-tuning process. This also precludes the need to jam-pack prompts with additional context and thus can result in lower inference costs.</p><h3 name="77ec" id="77ec" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">3 Ways to Fine-tune</strong></h3><p name="b46b" id="b46b" class="graf graf--p graf-after--h3">There are <strong class="markup--strong markup--p-strong">3 generic ways one can fine-tune </strong>a model: self-supervised, supervised, and reinforcement learning. These are not mutually exclusive in that any combination of these three approaches can be used in succession to fine-tune a single model.</p><h4 name="f8f3" id="f8f3" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Self-supervised Learning</strong></h4><p name="9a96" id="9a96" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">Self-supervised learning</strong> consists of <strong class="markup--strong markup--p-strong">training a model based on the inherent structure of the training data</strong>. In the context of LLMs, what this typically looks like is given a sequence of words (or tokens, to be more precise), predict the next word (token).</p><p name="ee51" id="ee51" class="graf graf--p graf-after--p">While this is how many pre-trained language models are developed these days, it can also be used for model fine-tuning. A potential use case of this is developing a model that can mimic a person’s writing style given a set of example texts.</p><h4 name="5ca3" id="5ca3" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Supervised Learning</strong></h4><p name="c755" id="c755" class="graf graf--p graf-after--h4">The next, and perhaps most popular, way to fine-tune a model is via <strong class="markup--strong markup--p-strong">supervised learning</strong>. This involves <strong class="markup--strong markup--p-strong">training a model on input-output pairs</strong> for a particular task. An example is <strong class="markup--strong markup--p-strong">instruction tuning,</strong> which aims to improve model performance in answering questions or responding to user prompts [1,3].</p><p name="e473" id="e473" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">key step</strong> in supervised learning is <strong class="markup--strong markup--p-strong">curating a training dataset</strong>. A simple way to do this is to create question-answer pairs and integrate them into a prompt template [1,3]. For example, the question-answer pair: <em class="markup--em markup--p-em">Who was the 35th President of the United States? — John F. Kennedy</em> could be pasted into the below prompt template. More example prompt templates are available in section A.2.1 of ref [4].</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="76bb" id="76bb" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-string">&quot;&quot;&quot;Please answer the following question.<br /><br />Q: {Question}<br /> <br />A: {Answer}&quot;&quot;&quot;</span></span></pre><p name="deb4" id="deb4" class="graf graf--p graf-after--pre">Using a prompt template is important because base models like GPT-3 are essentially “document completers”. Meaning, given some text, the model generates more text that (statistically) makes sense in that context. This goes back to the <a href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" data-href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" class="markup--anchor markup--p-anchor" target="_blank">previous blog</a> of this series and the idea of “tricking” a language model into solving your problem via prompt engineering.</p><div name="bb4a" id="bb4a" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://towardsdatascience.com/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" data-href="https://towardsdatascience.com/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f"><strong class="markup--strong markup--mixtapeEmbed-strong">Prompt Engineering — How to trick AI into solving your problems</strong><br><em class="markup--em markup--mixtapeEmbed-em">7 prompting tricks, Langchain, and Python example code</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="47d6a520aa635ed82852bc636764e286" data-thumbnail-img-id="0*ZcfH-qxXT4AYAqwr" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*ZcfH-qxXT4AYAqwr);"></a></div><h4 name="94a4" id="94a4" class="graf graf--h4 graf-after--mixtapeEmbed"><strong class="markup--strong markup--h4-strong">Reinforcement Learning</strong></h4><p name="6695" id="6695" class="graf graf--p graf-after--h4">Finally, one can use <strong class="markup--strong markup--p-strong">reinforcement learning (RL)</strong> to fine-tune models. RL <strong class="markup--strong markup--p-strong">uses a reward model to guide the training of the base model</strong>. This can take many different forms, but the basic idea is to train the reward model to score language model completions such that they reflect the preferences of human labelers [3,4]. The reward model can then be combined with a reinforcement learning algorithm (e.g. Proximal Policy Optimization (PPO)) to fine-tune the pre-trained model.</p><p name="970a" id="970a" class="graf graf--p graf-after--p">An example of how RL can be used for model fine-tuning is demonstrated by OpenAI’s InstructGPT models, which were developed through <strong class="markup--strong markup--p-strong">3 key steps</strong> [4].</p><ol class="postList"><li name="7b26" id="7b26" class="graf graf--li graf-after--p">Generate high-quality prompt-response pairs and fine-tune a pre-trained model using supervised learning. (~13k training prompts) <em class="markup--em markup--li-em">Note: One can (alternatively) skip to step 2 with the pre-trained model [3].</em></li><li name="ef3f" id="ef3f" class="graf graf--li graf-after--li">Use the fine-tuned model to generate completions and have human-labelers rank responses based on their preferences. Use these preferences to train the reward model. (~33k training prompts)</li><li name="33b5" id="33b5" class="graf graf--li graf-after--li">Use the reward model and an RL algorithm (e.g. PPO) to fine-tune the model further. (~31k training prompts)</li></ol><p name="d1ba" id="d1ba" class="graf graf--p graf-after--li">While the strategy above does generally result in LLM completions that are significantly more preferable to the base model, it can also come at a cost of lower performance in a subset of tasks. This drop in performance is also known as an <strong class="markup--strong markup--p-strong">alignment tax</strong> [3,4].</p><h3 name="c66c" id="c66c" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Supervised Fine-tuning Steps (High-level)</strong></h3><p name="b42d" id="b42d" class="graf graf--p graf-after--h3">As we saw above, there are many ways in which one can fine-tune an existing language model. However, for the remainder of this article, we will focus on fine-tuning via supervised learning. Below is a high-level procedure for supervised model fine-tuning [1].</p><ol class="postList"><li name="c608" id="c608" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Choose fine-tuning task</strong> (e.g. summarization, question answering, text classification)</li><li name="eaae" id="eaae" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Prepare training dataset</strong> i.e. create (100–10k) input-output pairs and preprocess data (i.e. tokenize, truncate, and pad text).</li><li name="706c" id="706c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Choose a base model </strong>(experiment with different models and choose one that performs best on the desired task).</li><li name="34c9" id="34c9" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Fine-tune model via supervised learning</strong></li><li name="af53" id="af53" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Evaluate model performance</strong></li></ol><p name="e083" id="e083" class="graf graf--p graf-after--li">While each of these steps could be an article of their own, I want to focus on <strong class="markup--strong markup--p-strong">step 4</strong> and discuss how we can go about training the fine-tuned model.</p><h3 name="b101" id="b101" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">3 Options for Parameter Training</strong></h3><p name="8656" id="8656" class="graf graf--p graf-after--h3">When it comes to fine-tuning a model with ~100M-100B parameters, one needs to be thoughtful of computational costs. Toward this end, an important question is — <em class="markup--em markup--p-em">which parameters do we (re)train?</em></p><p name="b1dc" id="b1dc" class="graf graf--p graf-after--p">With the mountain of parameters at play, we have countless choices for which ones we train. Here, I will focus on <strong class="markup--strong markup--p-strong">three generic options </strong>of which to choose.</p><h4 name="5f08" id="5f08" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Option 1: Retrain all parameters</strong></h4><p name="012d" id="012d" class="graf graf--p graf-after--h4">The first option is to <strong class="markup--strong markup--p-strong">train all internal model parameters</strong> (called <strong class="markup--strong markup--p-strong">full parameter tuning</strong>) [3]. While this option is simple (conceptually), it is the most computationally expensive. Additionally, a known issue with full parameter tuning is the phenomenon of catastrophic forgetting. This is where the model “forgets” useful information it “learned” in its initial training [3].</p><p name="dd72" id="dd72" class="graf graf--p graf-after--p">One way we can mitigate the downsides of Option 1 is to freeze a large portion of the model parameters, which brings us to Option 2.</p><h4 name="d89b" id="d89b" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Option 2: Transfer Learning</strong></h4><p name="8687" id="8687" class="graf graf--p graf-after--h4">The big idea with <strong class="markup--strong markup--p-strong">transfer learning (TL)</strong> is to preserve the useful representations/features the model has learned from past training when applying the model to a new task. This generally consists of <strong class="markup--strong markup--p-strong">dropping “the head” of a neural network (NN) and replacing it with a new one</strong> (e.g. adding new layers with randomized weights). <em class="markup--em markup--p-em">Note: The head of an NN includes its final layers, which translate the model’s internal representations to output values.</em></p><p name="8c6e" id="8c6e" class="graf graf--p graf-after--p">While leaving the majority of parameters untouched mitigates the huge computational cost of training an LLM, TL may not necessarily resolve the problem of catastrophic forgetting. To better handle both of these issues, we can turn to a different set of approaches.</p><h4 name="8e86" id="8e86" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Option 3: Parameter Efficient Fine-tuning (PEFT)</strong></h4><p name="ef39" id="ef39" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">PEFT</strong> involves <strong class="markup--strong markup--p-strong">augmenting a base model with a relatively small number of trainable parameters</strong>. The key result of this is a fine-tuning methodology that demonstrates comparable performance to full parameter tuning at a tiny fraction of the computational and storage cost [5].</p><p name="c9b9" id="c9b9" class="graf graf--p graf-after--p">PEFT encapsulates a family of techniques, one of which is the popular <strong class="markup--strong markup--p-strong">LoRA (Low-Rank Adaptation)</strong> method [6]. The basic idea behind LoRA is to pick a subset of layers in an existing model and modify their weights according to the following equation.</p><figure name="681c" id="681c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*GmCISYhd-JLqHNEvAQU1tQ.png" data-width="607" data-height="225" src="https://cdn-images-1.medium.com/max/800/1*GmCISYhd-JLqHNEvAQU1tQ.png"><figcaption class="imageCaption">Equation showing how weight matrices are modified for fine-tuning using LoRA [6]. Image by author.</figcaption></figure><p name="3878" id="3878" class="graf graf--p graf-after--figure">Where <em class="markup--em markup--p-em">h()</em> = a hidden layer that will be tuned, <em class="markup--em markup--p-em">x</em> = the input to <em class="markup--em markup--p-em">h()</em>, <em class="markup--em markup--p-em">W₀</em> = the original weight matrix for the <em class="markup--em markup--p-em">h</em>, and <em class="markup--em markup--p-em">ΔW</em> = a matrix of trainable parameters injected into <em class="markup--em markup--p-em">h</em>. <em class="markup--em markup--p-em">ΔW</em> is decomposed according to<em class="markup--em markup--p-em"> ΔW</em>=<em class="markup--em markup--p-em">BA</em>, where <em class="markup--em markup--p-em">ΔW</em> is a d by k matrix, <em class="markup--em markup--p-em">B</em> is d by r, and <em class="markup--em markup--p-em">A</em> is r by k<em class="markup--em markup--p-em">.</em> r is the assumed “intrinsic rank” of <em class="markup--em markup--p-em">ΔW </em>(which can be as small as 1 or 2) [6].</p><p name="66ef" id="66ef" class="graf graf--p graf-after--p">Sorry for all the math, but the <strong class="markup--strong markup--p-strong">key point is the (d * k) weights in <em class="markup--em markup--p-em">W₀</em> are frozen and, thus, not included in optimization</strong>. Instead, the ((d * r) + (r * k)) weights making up matrices <em class="markup--em markup--p-em">B</em> and <em class="markup--em markup--p-em">A </em>are the only ones that are trained.</p><p name="227f" id="227f" class="graf graf--p graf-after--p">Plugging in some made-up numbers for d=100, k=100, and r=2 to get a sense of the efficiency gains, the <strong class="markup--strong markup--p-strong">number of trainable parameters drops from 10,000 to 400</strong> in that layer. In practice, the authors of the LoRA paper cited a <strong class="markup--strong markup--p-strong">10,000x reduction in parameter checkpoint size</strong> using LoRA fine-tune GPT-3 compared to full parameter tuning [6].</p><p name="7de5" id="7de5" class="graf graf--p graf-after--p">To make this more concrete, let’s see how we can use LoRA to fine-tune a language model efficiently enough to run on a personal computer.</p><h3 name="c43b" id="c43b" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Example Code: Fine-tuning an LLM using LoRA</strong></h3><p name="9a7c" id="9a7c" class="graf graf--p graf-after--h3">In this example, we will use the Hugging Face ecosystem to fine-tune a language model to classify text as ‘positive’ or ‘negative’. Here, we fine-tune <a href="https://huggingface.co/distilbert-base-uncased" data-href="https://huggingface.co/distilbert-base-uncased" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">distilbert-base-uncased</em></a>, a ~70M parameter model based on <a href="https://arxiv.org/pdf/1810.04805.pdf" data-href="https://arxiv.org/pdf/1810.04805.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">BERT</a>. Since this base model was trained to do language modeling and not classification, we employ <strong class="markup--strong markup--p-strong">transfer learning</strong> to replace the base model head with a classification head. Additionally, we use <strong class="markup--strong markup--p-strong">LoRA</strong> to fine-tune the model efficiently enough that it can run on my Mac Mini (M1 chip with 16GB memory) in a reasonable amount of time (~20 min).</p><p name="6722" id="6722" class="graf graf--p graf-after--p">The code, along with the conda environment files, are available on the <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub repository</a>. The <a href="https://huggingface.co/shawhin/distilbert-base-uncased-lora-text-classification" data-href="https://huggingface.co/shawhin/distilbert-base-uncased-lora-text-classification" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">final model</a> and <a href="https://huggingface.co/datasets/shawhin/imdb-truncated" data-href="https://huggingface.co/datasets/shawhin/imdb-truncated" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">dataset</a> [7] are available on Hugging Face.</p><div name="42f7" id="42f7" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning"><strong class="markup--strong markup--mixtapeEmbed-strong">YouTube-Blog/LLMs/fine-tuning at main · ShawhinT/YouTube-Blog</strong><br><em class="markup--em markup--mixtapeEmbed-em">Codes to complement YouTube videos and blog posts on Medium. - YouTube-Blog/LLMs/fine-tuning at main ·…</em>github.com</a><a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="13dbc7cc88a27990cc4110a48ea2c9ed" data-thumbnail-img-id="0*b-hC4JBsGqy5oH-V" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*b-hC4JBsGqy5oH-V);"></a></div><h4 name="dbde" id="dbde" class="graf graf--h4 graf-after--mixtapeEmbed">Imports</h4><p name="4b4c" id="4b4c" class="graf graf--p graf-after--h4">We start by importing helpful libraries and modules. <a href="https://huggingface.co/docs/datasets/index" data-href="https://huggingface.co/docs/datasets/index" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Datasets</a>, <a href="https://huggingface.co/docs/transformers/index" data-href="https://huggingface.co/docs/transformers/index" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">transformers</a>, <a href="https://huggingface.co/docs/peft/index" data-href="https://huggingface.co/docs/peft/index" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">peft</a>, and <a href="https://huggingface.co/docs/evaluate/index" data-href="https://huggingface.co/docs/evaluate/index" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">evaluate</a> are all libraries from <a href="https://huggingface.co/" data-href="https://huggingface.co/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Hugging Face</a> (HF).</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="6e6d" id="6e6d" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, DatasetDict, Dataset<br /><br /><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (<br />    AutoTokenizer,<br />    AutoConfig, <br />    AutoModelForSequenceClassification,<br />    DataCollatorWithPadding,<br />    TrainingArguments,<br />    Trainer)<br /><br /><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> PeftModel, PeftConfig, get_peft_model, LoraConfig<br /><span class="hljs-keyword">import</span> evaluate<br /><span class="hljs-keyword">import</span> torch<br /><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span></pre><h4 name="2903" id="2903" class="graf graf--h4 graf-after--pre">Base model</h4><p name="1e0a" id="1e0a" class="graf graf--p graf-after--h4">Next, we load in our base model. The base model here is a relatively small one, but there are several other (larger) ones that we could have used (e.g. roberta-base, llama2, gpt2). A full list is available <a href="https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification" data-href="https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="5495" id="5495" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">model_checkpoint = <span class="hljs-string">&#x27;distilbert-base-uncased&#x27;</span><br /><br /><span class="hljs-comment"># define label maps</span><br />id2label = {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;Negative&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;Positive&quot;</span>}<br />label2id = {<span class="hljs-string">&quot;Negative&quot;</span>:<span class="hljs-number">0</span>, <span class="hljs-string">&quot;Positive&quot;</span>:<span class="hljs-number">1</span>}<br /><br /><span class="hljs-comment"># generate classification model from model_checkpoint</span><br />model = AutoModelForSequenceClassification.from_pretrained(<br />    model_checkpoint, num_labels=<span class="hljs-number">2</span>, id2label=id2label, label2id=label2id)</span></pre><h4 name="fef1" id="fef1" class="graf graf--h4 graf-after--pre">Load data</h4><p name="85f7" id="85f7" class="graf graf--p graf-after--h4">We can then load our <a href="https://huggingface.co/datasets/shawhin/imdb-truncated" data-href="https://huggingface.co/datasets/shawhin/imdb-truncated" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">training and validation data</a> from HF’s datasets library. This is a dataset of 2000 movie reviews (1000 for training and 1000 for validation) with binary labels indicating whether the review is positive (or not).</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="175b" id="175b" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># load dataset</span><br />dataset = load_dataset(<span class="hljs-string">&quot;shawhin/imdb-truncated&quot;</span>)<br />dataset<br /><br /><span class="hljs-comment"># dataset = </span><br /><span class="hljs-comment"># DatasetDict({</span><br /><span class="hljs-comment">#     train: Dataset({</span><br /><span class="hljs-comment">#         features: [&#x27;label&#x27;, &#x27;text&#x27;],</span><br /><span class="hljs-comment">#         num_rows: 1000</span><br /><span class="hljs-comment">#     })</span><br /><span class="hljs-comment">#     validation: Dataset({</span><br /><span class="hljs-comment">#         features: [&#x27;label&#x27;, &#x27;text&#x27;],</span><br /><span class="hljs-comment">#         num_rows: 1000</span><br /><span class="hljs-comment">#     })</span><br /><span class="hljs-comment"># }) </span></span></pre><h4 name="4457" id="4457" class="graf graf--h4 graf-after--pre">Preprocess data</h4><p name="e0d7" id="e0d7" class="graf graf--p graf-after--h4">Next, we need to preprocess our data so that it can be used for training. This consists of using a tokenizer to convert the text into an integer representation understood by the base model.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="0900" id="0900" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># create tokenizer</span><br />tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=<span class="hljs-literal">True</span>)</span></pre><p name="5739" id="5739" class="graf graf--p graf-after--pre">To apply the tokenizer to the dataset, we use the .<em class="markup--em markup--p-em">map()</em> method. This takes in a custom function that specifies how the text should be preprocessed. In this case, that function is called <em class="markup--em markup--p-em">tokenize_function()</em>. In addition to translating text to integers, this function truncates integer sequences such that they are no longer than 512 numbers to conform to the base model’s max input length.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="2f84" id="2f84" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># create tokenize function</span><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">examples</span>):<br />    <span class="hljs-comment"># extract text</span><br />    text = examples[<span class="hljs-string">&quot;text&quot;</span>]<br /><br />    <span class="hljs-comment">#tokenize and truncate text</span><br />    tokenizer.truncation_side = <span class="hljs-string">&quot;left&quot;</span><br />    tokenized_inputs = tokenizer(<br />        text,<br />        return_tensors=<span class="hljs-string">&quot;np&quot;</span>,<br />        truncation=<span class="hljs-literal">True</span>,<br />        max_length=<span class="hljs-number">512</span><br />    )<br /><br />    <span class="hljs-keyword">return</span> tokenized_inputs<br /><br /><span class="hljs-comment"># add pad token if none exists</span><br /><span class="hljs-keyword">if</span> tokenizer.pad_token <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br />    tokenizer.add_special_tokens({<span class="hljs-string">&#x27;pad_token&#x27;</span>: <span class="hljs-string">&#x27;[PAD]&#x27;</span>})<br />    model.resize_token_embeddings(<span class="hljs-built_in">len</span>(tokenizer))<br /><br /><span class="hljs-comment"># tokenize training and validation datasets</span><br />tokenized_dataset = dataset.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)<br />tokenized_dataset<br /><br /><span class="hljs-comment"># tokenized_dataset = </span><br /><span class="hljs-comment"># DatasetDict({</span><br /><span class="hljs-comment">#     train: Dataset({</span><br /><span class="hljs-comment">#        features: [&#x27;label&#x27;, &#x27;text&#x27;, &#x27;input_ids&#x27;, &#x27;attention_mask&#x27;],</span><br /><span class="hljs-comment">#         num_rows: 1000</span><br /><span class="hljs-comment">#     })</span><br /><span class="hljs-comment">#     validation: Dataset({</span><br /><span class="hljs-comment">#         features: [&#x27;label&#x27;, &#x27;text&#x27;, &#x27;input_ids&#x27;, &#x27;attention_mask&#x27;],</span><br /><span class="hljs-comment">#         num_rows: 1000</span><br /><span class="hljs-comment">#     })</span><br /><span class="hljs-comment"># })</span></span></pre><p name="c102" id="c102" class="graf graf--p graf-after--pre">At this point, we can also create a data collator, which will dynamically pad examples in each batch during training such that they all have the same length. This is computationally more efficient than padding all examples to be equal in length across the entire dataset.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="ini" name="1acb" id="1acb" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># create data collator</span><br /><span class="hljs-attr">data_collator</span> = DataCollatorWithPadding(tokenizer=tokenizer)</span></pre><h4 name="9716" id="9716" class="graf graf--h4 graf-after--pre">Evaluation metrics</h4><p name="0e0d" id="0e0d" class="graf graf--p graf-after--h4">We can define how we want to evaluate our fine-tuned model via a custom function. Here, we define the <em class="markup--em markup--p-em">compute_metrics() </em>function to compute the model’s accuracy.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="70ff" id="70ff" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># import accuracy evaluation metric</span><br />accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)<br /><br /><span class="hljs-comment"># define an evaluation function to pass into trainer later</span><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">p</span>):<br />    predictions, labels = p<br />    predictions = np.argmax(predictions, axis=<span class="hljs-number">1</span>)<br /><br />    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;accuracy&quot;</span>: accuracy.compute(predictions=predictions, <br />                                          references=labels)}</span></pre><h4 name="be42" id="be42" class="graf graf--h4 graf-after--pre">Untrained model performance</h4><p name="7064" id="7064" class="graf graf--p graf-after--h4">Before training our model, we can evaluate how the base model with a randomly initialized classification head performs on some example inputs.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="21b3" id="21b3" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># define list of examples</span><br />text_list = [<span class="hljs-string">&quot;It was good.&quot;</span>, <span class="hljs-string">&quot;Not a fan, don&#x27;t recommed.&quot;</span>, <br /><span class="hljs-string">&quot;Better than the first one.&quot;</span>, <span class="hljs-string">&quot;This is not worth watching even once.&quot;</span>, <br /><span class="hljs-string">&quot;This one is a pass.&quot;</span>]<br /><br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Untrained model predictions:&quot;</span>)<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;----------------------------&quot;</span>)<br /><span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> text_list:<br />    <span class="hljs-comment"># tokenize text</span><br />    inputs = tokenizer.encode(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br />    <span class="hljs-comment"># compute logits</span><br />    logits = model(inputs).logits<br />    <span class="hljs-comment"># convert logits to label</span><br />    predictions = torch.argmax(logits)<br /><br />    <span class="hljs-built_in">print</span>(text + <span class="hljs-string">&quot; - &quot;</span> + id2label[predictions.tolist()])<br /><br /><span class="hljs-comment"># Output:</span><br /><span class="hljs-comment"># Untrained model predictions:</span><br /><span class="hljs-comment"># ----------------------------</span><br /><span class="hljs-comment"># It was good. - Negative</span><br /><span class="hljs-comment"># Not a fan, don&#x27;t recommed. - Negative</span><br /><span class="hljs-comment"># Better than the first one. - Negative</span><br /><span class="hljs-comment"># This is not worth watching even once. - Negative</span><br /><span class="hljs-comment"># This one is a pass. - Negative</span></span></pre><p name="8f30" id="8f30" class="graf graf--p graf-after--pre">As expected, the model performance is equivalent to random guessing. Let’s see how we can improve this with fine-tuning.</p><h4 name="a213" id="a213" class="graf graf--h4 graf-after--p">Fine-tuning with LoRA</h4><p name="baa9" id="baa9" class="graf graf--p graf-after--h4">To use LoRA for fine-tuning, we first need a config file. This sets all the parameters for the LoRA algorithm. See comments in the code block for more details.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="a999" id="a999" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">peft_config = LoraConfig(task_type=<span class="hljs-string">&quot;SEQ_CLS&quot;</span>, <span class="hljs-comment"># sequence classification</span><br />                        r=<span class="hljs-number">4</span>, <span class="hljs-comment"># intrinsic rank of trainable weight matrix</span><br />                        lora_alpha=<span class="hljs-number">32</span>, <span class="hljs-comment"># this is like a learning rate</span><br />                        lora_dropout=<span class="hljs-number">0.01</span>, <span class="hljs-comment"># probablity of dropout</span><br />                        target_modules = [<span class="hljs-string">&#x27;q_lin&#x27;</span>]) <span class="hljs-comment"># we apply lora to query layer only</span></span></pre><p name="cf8f" id="cf8f" class="graf graf--p graf-after--pre">We can then create a new version of our model that can be trained via PEFT. Notice that the scale of trainable parameters was reduced by about 100x.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="dec2" id="dec2" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">model = get_peft_model(model, peft_config)<br />model.print_trainable_parameters()<br /><br /><span class="hljs-comment"># trainable params: 1,221,124 || all params: 67,584,004 || trainable%: 1.8068239934408148</span></span></pre><p name="f6e4" id="f6e4" class="graf graf--p graf-after--pre">Next, we define hyperparameters for model training.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="makefile" name="9131" id="9131" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># hyperparameters</span><br />lr = 1e-3 <span class="hljs-comment"># size of optimization step </span><br />batch_size = 4 <span class="hljs-comment"># number of examples processed per optimziation step</span><br />num_epochs = 10 <span class="hljs-comment"># number of times model runs through training data</span><br /><br /><span class="hljs-comment"># define training arguments</span><br />training_args = TrainingArguments(<br />    output_dir= model_checkpoint + <span class="hljs-string">&quot;-lora-text-classification&quot;</span>,<br />    learning_rate=lr,<br />    per_device_train_batch_size=batch_size, <br />    per_device_eval_batch_size=batch_size,<br />    num_train_epochs=num_epochs,<br />    weight_decay=0.01,<br />    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,<br />    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,<br />    load_best_model_at_end=True,<br />)</span></pre><p name="28e8" id="28e8" class="graf graf--p graf-after--pre">Finally, we create a trainer() object and fine-tune the model!</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="ed38" id="ed38" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># creater trainer object</span><br />trainer = Trainer(<br />    model=model, <span class="hljs-comment"># our peft model</span><br />    args=training_args, <span class="hljs-comment"># hyperparameters</span><br />    train_dataset=tokenized_dataset[<span class="hljs-string">&quot;train&quot;</span>], <span class="hljs-comment"># training data</span><br />    eval_dataset=tokenized_dataset[<span class="hljs-string">&quot;validation&quot;</span>], <span class="hljs-comment"># validation data</span><br />    tokenizer=tokenizer, <span class="hljs-comment"># define tokenizer</span><br />    data_collator=data_collator, <span class="hljs-comment"># this will dynamically pad examples in each batch to be equal length</span><br />    compute_metrics=compute_metrics, <span class="hljs-comment"># evaluates model using compute_metrics() function from before</span><br />)<br /><br /><span class="hljs-comment"># train model</span><br />trainer.train()</span></pre><p name="aecb" id="aecb" class="graf graf--p graf-after--pre">The above code will generate the following table of metrics during training.</p><figure name="abc4" id="abc4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4rd_YQmPs-fh1tltHduBjA.png" data-width="607" data-height="484" src="https://cdn-images-1.medium.com/max/800/1*4rd_YQmPs-fh1tltHduBjA.png"><figcaption class="imageCaption">Model training metrics. Image by author.</figcaption></figure><h4 name="db0f" id="db0f" class="graf graf--h4 graf-after--figure">Trained model performance</h4><p name="12af" id="12af" class="graf graf--p graf-after--h4">To see how the model performance has improved, let’s apply it to the same 5 examples from before.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="c528" id="c528" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">model.to(<span class="hljs-string">&#x27;mps&#x27;</span>) <span class="hljs-comment"># moving to mps for Mac (can alternatively do &#x27;cpu&#x27;)</span><br /><br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Trained model predictions:&quot;</span>)<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;--------------------------&quot;</span>)<br /><span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> text_list:<br />    inputs = tokenizer.encode(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(<span class="hljs-string">&quot;mps&quot;</span>) <span class="hljs-comment"># moving to mps for Mac (can alternatively do &#x27;cpu&#x27;)</span><br /><br />    logits = model(inputs).logits<br />    predictions = torch.<span class="hljs-built_in">max</span>(logits,<span class="hljs-number">1</span>).indices<br /><br />    <span class="hljs-built_in">print</span>(text + <span class="hljs-string">&quot; - &quot;</span> + id2label[predictions.tolist()[<span class="hljs-number">0</span>]])<br /><br /><span class="hljs-comment"># Output:</span><br /><span class="hljs-comment"># Trained model predictions:</span><br /><span class="hljs-comment"># ----------------------------</span><br /><span class="hljs-comment"># It was good. - Positive</span><br /><span class="hljs-comment"># Not a fan, don&#x27;t recommed. - Negative</span><br /><span class="hljs-comment"># Better than the first one. - Positive</span><br /><span class="hljs-comment"># This is not worth watching even once. - Negative</span><br /><span class="hljs-comment"># This one is a pass. - Positive # this one is tricky</span></span></pre><p name="2206" id="2206" class="graf graf--p graf-after--pre">The fine-tuned model improved significantly from its prior random guessing, correctly classifying all but one of the examples in the above code. This aligns with the ~90% accuracy metric we saw during training.</p><p name="cc64" id="cc64" class="graf graf--p graf-after--p">Links: <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Code Repo</a> | <a href="https://huggingface.co/shawhin/distilbert-base-uncased-lora-text-classification" data-href="https://huggingface.co/shawhin/distilbert-base-uncased-lora-text-classification" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Model</a> | <a href="https://huggingface.co/datasets/shawhin/imdb-truncated" data-href="https://huggingface.co/datasets/shawhin/imdb-truncated" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Dataset</a></p><h3 name="da6c" id="da6c" class="graf graf--h3 graf-after--p">Conclusions</h3><p name="4889" id="4889" class="graf graf--p graf-after--h3">While fine-tuning an existing model requires more computational resources and technical expertise than using one out-of-the-box, (smaller) fine-tuned models can outperform (larger) pre-trained base models for a particular use case, even when employing clever prompt engineering strategies. Furthermore, with all the open-source LLM resources available, it’s never been easier to fine-tune a model for a custom application.</p><p name="0b90" id="0b90" class="graf graf--p graf-after--p">The next article of this series will go one step beyond model fine-tuning and discuss how to train a language model from scratch.</p><p name="bca1" id="bca1" class="graf graf--p graf-after--p">👉 <strong class="markup--strong markup--p-strong">More on LLMs</strong>: <a href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" data-href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Introduction</a> | <a href="https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971" data-href="https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenAI API</a> | <a href="https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" data-href="https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Hugging Face Transformers</a> | <a href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" data-href="https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f" class="markup--anchor markup--p-anchor" target="_blank">Prompt Engineering</a> | <a href="https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9" data-href="https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Build an LLM</a> | <a href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" data-href="https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">QLoRA</a> | <a href="https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac" data-href="https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac" class="markup--anchor markup--p-anchor" target="_blank">RAG</a> | <a href="https://towardsdatascience.com/text-embeddings-classification-and-semantic-search-8291746220be" data-href="https://towardsdatascience.com/text-embeddings-classification-and-semantic-search-8291746220be" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Text Embeddings</a></p><div name="6d9c" id="6d9c" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/list/8e009ae3054c" data-href="https://shawhin.medium.com/list/8e009ae3054c" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/list/8e009ae3054c"><strong class="markup--strong markup--mixtapeEmbed-strong">Large Language Models (LLMs)</strong><br><em class="markup--em markup--mixtapeEmbed-em">A series on how to use LLMs in practice.</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/list/8e009ae3054c" class="js-mixtapeImage mixtapeImage mixtapeImage--mediumCatalog  u-ignoreBlock" data-media-id="e38ca2d0de6aa61d7c09af603d8f43e1" data-thumbnail-img-id="0*ae40ceaca4c0bdcbdf431a66c777e68bffc1e356.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/304/160/0*ae40ceaca4c0bdcbdf431a66c777e68bffc1e356.jpeg);"></a></div></div></div></section><section name="9c78" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="fbf4" id="fbf4" class="graf graf--h3 graf--leading">Resources</h3><p name="6be1" id="6be1" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Connect</strong>: <a href="https://shawhintalebi.com/" data-href="https://shawhintalebi.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">My website</a> | <a href="https://calendly.com/shawhintalebi" data-href="https://calendly.com/shawhintalebi" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Book a call</a> | <a href="https://shawhintalebi.com/contact/" data-href="https://shawhintalebi.com/contact/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Ask me anything</a></p><p name="0cb7" id="0cb7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Socials</strong>: <a href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" data-href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">YouTube 🎥</a> | <a href="https://www.linkedin.com/in/shawhintalebi/" data-href="https://www.linkedin.com/in/shawhintalebi/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">LinkedIn</a> | <a href="https://twitter.com/ShawhinT" data-href="https://twitter.com/ShawhinT" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Twitter</a></p><p name="2736" id="2736" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Support</strong>: <a href="https://www.buymeacoffee.com/shawhint" data-href="https://www.buymeacoffee.com/shawhint" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Buy me a coffee</a> ☕️</p><div name="b018" id="b018" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/subscribe" data-href="https://shawhin.medium.com/subscribe" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/subscribe"><strong class="markup--strong markup--mixtapeEmbed-strong">Get FREE access to every new story I write</strong><br><em class="markup--em markup--mixtapeEmbed-em">Get FREE access to every new story I write P.S. I do not share your email with anyone By signing up, you will create a…</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/subscribe" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8d6d6c2d301f2c367b618a850fa468b3" data-thumbnail-img-id="0*O69FxQ5FttNEA_r4" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*O69FxQ5FttNEA_r4);"></a></div></div></div></section><section name="eee3" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="eb3c" id="eb3c" class="graf graf--p graf--leading">[1] Deeplearning.ai Finetuning Large Langauge Models Short Course: <a href="https://www.deeplearning.ai/short-courses/finetuning-large-language-models/" data-href="https://www.deeplearning.ai/short-courses/finetuning-large-language-models/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://www.deeplearning.ai/short-courses/finetuning-large-language-models/</a></p><p name="32c9" id="32c9" class="graf graf--p graf-after--p">[2] <a href="https://arxiv.org/abs/2005.14165" data-href="https://arxiv.org/abs/2005.14165" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2005.14165</a><strong class="markup--strong markup--p-strong"> [cs.CL] (</strong>GPT-3 Paper)</p><p name="560a" id="560a" class="graf graf--p graf-after--p">[3] <a href="https://arxiv.org/abs/2303.18223" data-href="https://arxiv.org/abs/2303.18223" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2303.18223</a><strong class="markup--strong markup--p-strong"> [cs.CL] (</strong>Survey of LLMs)</p><p name="2fd5" id="2fd5" class="graf graf--p graf-after--p">[4] <a href="https://arxiv.org/abs/2203.02155" data-href="https://arxiv.org/abs/2203.02155" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2203.02155</a><strong class="markup--strong markup--p-strong"> [cs.CL] (</strong>InstructGPT paper)</p><p name="78c7" id="78c7" class="graf graf--p graf-after--p">[5] 🤗 PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware: <a href="https://huggingface.co/blog/peft" data-href="https://huggingface.co/blog/peft" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://huggingface.co/blog/peft</a></p><p name="6d6a" id="6d6a" class="graf graf--p graf-after--p">[6] <a href="https://arxiv.org/abs/2106.09685" data-href="https://arxiv.org/abs/2106.09685" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">arXiv:2106.09685</a><strong class="markup--strong markup--p-strong"> [cs.CL]</strong> (LoRA paper)</p><p name="3ddb" id="3ddb" class="graf graf--p graf-after--p graf--trailing">[7] Original dataset source — Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. <a href="https://aclanthology.org/P11-1015" data-href="https://aclanthology.org/P11-1015" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Learning Word Vectors for Sentiment Analysis</a>. In <em class="markup--em markup--p-em">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</em>, pages 142–150, Portland, Oregon, USA. Association for Computational Linguistics.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@shawhin" class="p-author h-card">Shaw Talebi</a> on <a href="https://medium.com/p/23473d763b91"><time class="dt-published" datetime="2023-09-11T18:06:49.925Z">September 11, 2023</time></a>.</p><p><a href="https://medium.com/@shawhin/fine-tuning-large-language-models-llms-23473d763b91" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 2, 2024.</p></footer></article></body></html>